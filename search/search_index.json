{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":"<p>Mainly you could copy the README.md here. However, you should be careful with:</p> <ul> <li>The banner section is different</li> <li>Link to assets (handling dark mode is different between GitHub and the documentation)</li> <li>Relative links</li> </ul> <p></p> <p> Libname is a Python toolkit dedicated to making people happy and fun.     Explore Libname docs \u00bb </p>"},{"location":"#table-of-contents","title":"\ud83d\udcda Table of contents","text":"<ul> <li>\ud83d\udcda Table of contents</li> <li>\ud83d\udd25 Tutorials</li> <li>\ud83d\ude80 Quick Start</li> <li>\ud83d\udce6 What's Included</li> <li>\ud83d\udc4d Contributing</li> <li>\ud83d\udc40 See Also</li> <li>\ud83d\ude4f Acknowledgments</li> <li>\ud83d\udc68\u200d\ud83c\udf93 Creator</li> <li>\ud83d\uddde\ufe0f Citation</li> <li>\ud83d\udcdd License</li> </ul>"},{"location":"#tutorials","title":"\ud83d\udd25 Tutorials","text":"<p>We propose some tutorials to get familiar with the library and its API:</p> <ul> <li>Getting started </li> </ul> <p>You do not necessarily need to register the notebooks on GitHub. Notebooks can be hosted on a specific drive.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Libname requires some stuff and several libraries including Numpy. Installation can be done using Pypi:</p> <pre><code>pip install libname\n</code></pre> <p>Now that Libname is installed, here are some basic examples of what you can do with the available modules.</p>"},{"location":"#print-hello-world","title":"Print Hello World","text":"<p>Let's start with a simple example:</p> <pre><code>from libname.fake import hello_world\n\nhello_world()\n</code></pre>"},{"location":"#make-addition","title":"Make addition","text":"<p>In order to add <code>a</code> to <code>b</code> you can use:</p> <pre><code>from libname.fake import addition\n\na = 1\nb = 2\nc = addition(a, b)\n</code></pre>"},{"location":"#whats-included","title":"\ud83d\udce6 What's Included","text":"<p>A list or table of methods available</p>"},{"location":"#contributing","title":"\ud83d\udc4d Contributing","text":"<p>Feel free to propose your ideas or come and contribute with us on the Libname toolbox! We have a specific document where we describe in a simple way how to make your first pull request: just here.</p>"},{"location":"#see-also","title":"\ud83d\udc40 See Also","text":"<p>This library is one approach of many...</p> <p>Other tools to explain your model include:</p> <ul> <li>Random</li> </ul> <p>More from the DEEL project:</p> <ul> <li>Xplique a Python library exclusively dedicated to explaining neural networks.</li> <li>deel-lip a Python library for training k-Lipschitz neural networks on TF.</li> <li>Influenciae Python toolkit dedicated to computing influence values for the discovery of potentially problematic samples in a dataset.</li> <li>deel-torchlip a Python library for training k-Lipschitz neural networks on PyTorch.</li> <li>DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of data quality, representativity and explainability for this purpose.</li> </ul>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<p>  This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the  DEEL  project.</p>"},{"location":"#creators","title":"\ud83d\udc68\u200d\ud83c\udf93 Creators","text":"<p>If you want to highlight the main contributors</p>"},{"location":"#citation","title":"\ud83d\uddde\ufe0f Citation","text":"<p>If you use Libname as part of your workflow in a scientific publication, please consider citing \ud83d\uddde\ufe0f our paper:</p> <pre><code>@article{rickroll,\n  title={Rickrolling},\n  author={Some Internet Trolls},\n  journal={Best Memes},\n  year={ND}\n}\n</code></pre>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>The package is released under  MIT license.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Thanks for taking the time to contribute!</p> <p>From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first. This way we can ensure that your precious work is not in vain.</p>"},{"location":"CONTRIBUTING/#setup-with-make","title":"Setup with make","text":"<ul> <li>Clone the repo <code>git clone https://github.com/deel-ai/dp-lipschitz.git</code>.</li> <li>Go to your freshly downloaded repo <code>cd lipdp</code></li> <li>Create a virtual environment and install the necessary dependencies for development:</li> </ul> <p><code>make prepare-dev &amp;&amp; source lipdp_dev_env/bin/activate</code>.</p> <p>Welcome to the team !</p>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>To run test <code>make test</code> This command activate your virtual environment and launch the <code>tox</code> command.</p> <p><code>tox</code> on the otherhand will do the following: - run pytest on the tests folder with python 3.6, python 3.7 and python 3.8</p> <p>Note: If you do not have those 3 interpreters the tests would be only performs with your current interpreter - run pylint on the deel-datasets main files, also with python 3.6, python 3.7 and python 3.8 Note: It is possible that pylint throw false-positive errors. If the linting test failed please check first pylint output to point out the reasons.</p> <p>Please, make sure you run all the tests at least once before opening a pull request.</p> <p>A word toward Pylint for those that don't know it:</p> <p>Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions.</p> <p>Basically, it will check that your code follow a certain number of convention. Any Pull Request will go through a Github workflow ensuring that your code respect the Pylint conventions (most of them at least).</p>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting Changes","text":"<p>After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly (see Governance policy).</p> <p>Something that will increase the chance that your pull request is accepted:</p> <ul> <li>Write tests and ensure that the existing ones pass.</li> <li>If <code>make test</code> is succesful, you have fair chances to pass the CI workflows (linting and test)</li> <li>Follow the existing coding style and run <code>make check_all</code> to check all files format.</li> <li>Write a good commit message (we follow a lowercase convention).</li> <li>For a major fix/feature make sure your PR has an issue and if it doesn't, please create one. This would help discussion with the community, and polishing ideas in case of a new feature.</li> </ul>"},{"location":"api/layers/","title":"deel.lipdp.layers module","text":""},{"location":"api/layers/#deel.lipdp.layers.AddBias","title":"<code>AddBias</code>","text":"<p>             Bases: <code>Layer</code></p> <p>Adds a bias to the input.</p> <p>Remark: the euclidean norm of the bias must be bounded in advance. Note that this is the euclidean norm of the whole bias vector, not the norm of each element of the bias vector.</p> <p>Warning: beware zero gradients outside the ball of norm norm_max. In the future, we might choose a smoother projection on the ball to ensure that the gradient remains non zero outside the ball.</p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>class AddBias(tf.keras.layers.Layer):\n    \"\"\"Adds a bias to the input.\n\n    Remark: the euclidean norm of the bias must be bounded in advance.\n    Note that this is the euclidean norm of the whole bias vector, not\n    the norm of each element of the bias vector.\n\n    Warning: beware zero gradients outside the ball of norm norm_max.\n    In the future, we might choose a smoother projection on the ball to ensure\n    that the gradient remains non zero outside the ball.\n    \"\"\"\n\n    def __init__(self, norm_max, **kwargs):\n        super().__init__(**kwargs)\n        self.norm_max = tf.convert_to_tensor(norm_max)\n\n    def build(self, input_shape):\n        self.bias = self.add_weight(\n            name=\"bias\",\n            shape=(input_shape[-1],),\n            initializer=\"zeros\",\n            trainable=True,\n        )\n\n    def call(self, inputs, **kwargs):\n        # parametrize the bias so it belongs to a ball of norm norm_max.\n        bias = tf.convert_to_tensor(\n            tf.clip_by_norm(self.bias, self.norm_max)\n        )  # 1-Lipschitz operation.\n        return inputs + bias\n</code></pre>"},{"location":"api/layers/#deel.lipdp.layers.DPLayer","title":"<code>DPLayer</code>","text":"<p>Wrapper for created differentially private layers, instanciates abstract methods use for computing the bounds of the gradient relatively to the parameters and to the input.</p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>class DPLayer:\n    \"\"\"\n    Wrapper for created differentially private layers, instanciates abstract methods\n    use for computing the bounds of the gradient relatively to the parameters and to the\n    input.\n    \"\"\"\n\n    @abstractmethod\n    def backpropagate_params(self, input_bound, gradient_bound):\n        \"\"\"Corresponds to the Lipschitz constant of the output wrt the parameters,\n            i.e. the norm of the Jacobian of the output wrt the parameters times the norm of the cotangeant vector.\n\n        Args:\n            input_bound: Maximum norm of input.\n            gradient_bound: Maximum norm of gradients (co-tangent vector)\n\n        Returns:\n            Maximum norm of tangent vector.\"\"\"\n        pass\n\n    @abstractmethod\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n        \"\"\"Applies the dilatation of the cotangeant vector norm (upstream gradient) by the Jacobian,\n            i.e. multiply by the Lipschitz constant of the output wrt input.\n\n        Args:\n            input_bound: Maximum norm of input.\n            gradient_bound: Maximum norm of gradients (co-tangent vector)\n\n        Returns:\n            Maximum norm of tangent vector.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def propagate_inputs(self, input_bound):\n        \"\"\"Maximum norm of output of element.\n\n        Remark: when the layer is linear, this coincides with its Lipschitz constant * input_bound.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def has_parameters(self):\n        pass\n</code></pre>"},{"location":"api/layers/#deel.lipdp.layers.DPLayer.backpropagate_inputs","title":"<code>backpropagate_inputs(input_bound, gradient_bound)</code>  <code>abstractmethod</code>","text":"<p>Applies the dilatation of the cotangeant vector norm (upstream gradient) by the Jacobian,     i.e. multiply by the Lipschitz constant of the output wrt input.</p> <p>Parameters:</p> Name Type Description Default <code>input_bound</code> <p>Maximum norm of input.</p> required <code>gradient_bound</code> <p>Maximum norm of gradients (co-tangent vector)</p> required <p>Returns:</p> Type Description <p>Maximum norm of tangent vector.</p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>@abstractmethod\ndef backpropagate_inputs(self, input_bound, gradient_bound):\n    \"\"\"Applies the dilatation of the cotangeant vector norm (upstream gradient) by the Jacobian,\n        i.e. multiply by the Lipschitz constant of the output wrt input.\n\n    Args:\n        input_bound: Maximum norm of input.\n        gradient_bound: Maximum norm of gradients (co-tangent vector)\n\n    Returns:\n        Maximum norm of tangent vector.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/layers/#deel.lipdp.layers.DPLayer.backpropagate_params","title":"<code>backpropagate_params(input_bound, gradient_bound)</code>  <code>abstractmethod</code>","text":"<p>Corresponds to the Lipschitz constant of the output wrt the parameters,     i.e. the norm of the Jacobian of the output wrt the parameters times the norm of the cotangeant vector.</p> <p>Parameters:</p> Name Type Description Default <code>input_bound</code> <p>Maximum norm of input.</p> required <code>gradient_bound</code> <p>Maximum norm of gradients (co-tangent vector)</p> required <p>Returns:</p> Type Description <p>Maximum norm of tangent vector.</p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>@abstractmethod\ndef backpropagate_params(self, input_bound, gradient_bound):\n    \"\"\"Corresponds to the Lipschitz constant of the output wrt the parameters,\n        i.e. the norm of the Jacobian of the output wrt the parameters times the norm of the cotangeant vector.\n\n    Args:\n        input_bound: Maximum norm of input.\n        gradient_bound: Maximum norm of gradients (co-tangent vector)\n\n    Returns:\n        Maximum norm of tangent vector.\"\"\"\n    pass\n</code></pre>"},{"location":"api/layers/#deel.lipdp.layers.DPLayer.propagate_inputs","title":"<code>propagate_inputs(input_bound)</code>  <code>abstractmethod</code>","text":"<p>Maximum norm of output of element.</p> <p>Remark: when the layer is linear, this coincides with its Lipschitz constant * input_bound.</p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>@abstractmethod\ndef propagate_inputs(self, input_bound):\n    \"\"\"Maximum norm of output of element.\n\n    Remark: when the layer is linear, this coincides with its Lipschitz constant * input_bound.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/layers/#deel.lipdp.layers.DP_AddBias","title":"<code>DP_AddBias</code>","text":"<p>             Bases: <code>AddBias</code>, <code>DPLayer</code></p> <p>Adds a bias to the input.</p> <p>The bias is projected on the ball of norm <code>norm_max</code> during training. The projection on the ball is a 1-Lipschitz function, since the ball is convex.</p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>class DP_AddBias(AddBias, DPLayer):\n    \"\"\"Adds a bias to the input.\n\n    The bias is projected on the ball of norm `norm_max` during training.\n    The projection on the ball is a 1-Lipschitz function, since the ball\n    is convex.\n    \"\"\"\n\n    def __init__(self, *args, nm_coef=1, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.nm_coef = nm_coef\n\n    def backpropagate_params(self, input_bound, gradient_bound):\n        return gradient_bound  # clipping is a 1-Lipschitz operation.\n\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n        return 1 * gradient_bound  # adding is a 1-Lipschitz operation.\n\n    def propagate_inputs(self, input_bound):\n        return input_bound + self.norm_max\n\n    def has_parameters(self):\n        return True\n</code></pre>"},{"location":"api/layers/#deel.lipdp.layers.DP_BoundedInput","title":"<code>DP_BoundedInput</code>","text":"<p>             Bases: <code>Layer</code>, <code>DPLayer</code></p> <p>Input layer that clips the input to a given norm.</p> <p>Remark: every pipeline should start with this layer.</p> <p>Attributes:</p> Name Type Description <code>upper_bound</code> <p>Maximum norm of the input.</p> <code>enforce_clipping</code> <p>If True (default), the input is clipped to the given norm.</p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>class DP_BoundedInput(tf.keras.layers.Layer, DPLayer):\n    \"\"\"Input layer that clips the input to a given norm.\n\n    Remark: every pipeline should start with this layer.\n\n    Attributes:\n        upper_bound: Maximum norm of the input.\n        enforce_clipping: If True (default), the input is clipped to the given norm.\n    \"\"\"\n\n    def __init__(self, *args, upper_bound, enforce_clipping=True, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.upper_bound = tf.convert_to_tensor(upper_bound)\n        self.enforce_clipping = enforce_clipping\n\n    def call(self, x, *args, **kwargs):\n        if self.enforce_clipping:\n            axes = list(range(1, len(x.shape)))\n            x = tf.clip_by_norm(x, self.upper_bound, axes=axes)\n        return x\n\n    def backpropagate_params(self, input_bound, gradient_bound):\n        raise ValueError(\"InputLayer doesn't have parameters\")\n\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n        return 1 * gradient_bound\n\n    def propagate_inputs(self, input_bound):\n        if input_bound is None:\n            return self.upper_bound\n        return tf.math.minimum(self.upper_bound, input_bound)\n\n    def has_parameters(self):\n        return False\n</code></pre>"},{"location":"api/layers/#deel.lipdp.layers.DP_ClipGradient","title":"<code>DP_ClipGradient</code>","text":"<p>             Bases: <code>Layer</code>, <code>DPLayer</code></p> <p>Clips the gradient during the backward pass.</p> <p>Behaves like identity function during the forward pass. The clipping is done automatically during the backward pass.</p> <p>Attributes:</p> Name Type Description <code>clip_value</code> <code>float</code> <pre><code>            The maximum norm of the gradient allowed. Only\n            declare this variable if you plan on using the \"fixed\" clipping mode.\n            Otherwise it will be updated automatically.\n</code></pre> <code>mode</code> <code>str</code> <p>The mode of clipping. Either \"fixed\" or \"dynamic\". Default is \"fixed\".</p> <p>Warning : The mode \"dynamic\" needs to be used along a callback that updates the clipping value.</p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>class DP_ClipGradient(tf.keras.layers.Layer, DPLayer):\n    \"\"\"Clips the gradient during the backward pass.\n\n    Behaves like identity function during the forward pass.\n    The clipping is done automatically during the backward pass.\n\n    Attributes:\n        clip_value (float, optional):\n                            The maximum norm of the gradient allowed. Only\n                            declare this variable if you plan on using the \"fixed\" clipping mode.\n                            Otherwise it will be updated automatically.\n        mode (str): The mode of clipping. Either \"fixed\" or \"dynamic\". Default is \"fixed\".\n\n    Warning : The mode \"dynamic\" needs to be used along a callback that updates the clipping value.\n    \"\"\"\n\n    def __init__(self, clip_value, mode=\"fixed\", *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self._dynamic_dp_dict = {}  # to be filled by the callback\n\n        assert mode in [\"fixed\", \"dynamic\"]\n        self.mode = mode\n\n        assert clip_value is None or clip_value &gt;= 0, \"clip_value must be positive\"\n        if mode == \"fixed\":\n            assert (\n                clip_value is not None\n            ), \"clip_value must be declared when using the fixed mode\"\n\n        if clip_value is None:\n            clip_value = (\n                0.0  # Change type back to float in case clip_value needs to be updated\n            )\n\n        self.clip_value = tf.Variable(clip_value, trainable=False, dtype=tf.float32)\n\n    def update_clipping_value(self, new_clip_value):\n        print(\"Update clipping value to : \", float(new_clip_value.numpy()))\n        self.clip_value.assign(new_clip_value)\n\n    def call(self, inputs, *args, **kwargs):\n        batch_size = tf.convert_to_tensor(tf.cast(tf.shape(inputs)[0], tf.float32))\n        # the clipping is done elementwise\n        # since REDUCTION=SUM_OVER_BATCH_SIZE, we need to divide by batch_size\n        # to get the correct norm.\n        # this makes the clipping independent of the batch size.\n        elementwise_clip_value = self.clip_value.value() / batch_size\n        return clip_gradient(inputs, elementwise_clip_value)\n\n    def backpropagate_params(self, input_bound, gradient_bound):\n        raise ValueError(\"ClipGradient doesn't have parameters\")\n\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n        return tf.math.minimum(gradient_bound, self.clip_value)\n\n    def propagate_inputs(self, input_bound):\n        return input_bound\n\n    def has_parameters(self):\n        return False\n</code></pre>"},{"location":"api/layers/#deel.lipdp.layers.DP_MaxPool2D","title":"<code>DP_MaxPool2D</code>","text":"<p>             Bases: <code>MaxPool2D</code>, <code>DPLayer</code></p> <p>Max pooling layer that preserves the gradient norm.</p> <p>Parameters:</p> Name Type Description Default <code>layer_cls</code> <p>Class of the layer to wrap.</p> required <p>Returns:</p> Type Description <p>A differentially private layer that doesn't have parameters.</p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>class DP_MaxPool2D(tf.keras.layers.MaxPool2D, DPLayer):\n    \"\"\"Max pooling layer that preserves the gradient norm.\n\n    Args:\n        layer_cls: Class of the layer to wrap.\n\n    Returns:\n        A differentially private layer that doesn't have parameters.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert (\n            self.strides is None or self.strides == self.pool_size\n        ), \"Ensure that strides == pool_size, otherwise it is not 1-Lipschitz.\"\n\n    def backpropagate_params(self, input_bound, gradient_bound):\n        raise ValueError(\"Layer doesn't have parameters\")\n\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n        return 1 * gradient_bound\n\n    def propagate_inputs(self, input_bound):\n        return input_bound\n\n    def has_parameters(self):\n        return False\n</code></pre>"},{"location":"api/layers/#deel.lipdp.layers.DP_ScaledL2NormPooling2D","title":"<code>DP_ScaledL2NormPooling2D</code>","text":"<p>             Bases: <code>ScaledL2NormPooling2D</code>, <code>DPLayer</code></p> <p>Max pooling layer that preserves the gradient norm.</p> <p>Parameters:</p> Name Type Description Default <code>layer_cls</code> <p>Class of the layer to wrap.</p> required <p>Returns:</p> Type Description <p>A differentially private layer that doesn't have parameters.</p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>class DP_ScaledL2NormPooling2D(deel.lip.layers.ScaledL2NormPooling2D, DPLayer):\n    \"\"\"Max pooling layer that preserves the gradient norm.\n\n    Args:\n        layer_cls: Class of the layer to wrap.\n\n    Returns:\n        A differentially private layer that doesn't have parameters.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert (\n            self.strides is None or self.strides == self.pool_size\n        ), \"Ensure that strides == pool_size, otherwise it is not 1-Lipschitz.\"\n\n    def backpropagate_params(self, input_bound, gradient_bound):\n        raise ValueError(\"Layer doesn't have parameters\")\n\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n        return 1 * gradient_bound\n\n    def propagate_inputs(self, input_bound):\n        return input_bound\n\n    def has_parameters(self):\n        return False\n</code></pre>"},{"location":"api/layers/#deel.lipdp.layers.DP_WrappedResidual","title":"<code>DP_WrappedResidual</code>","text":"<p>             Bases: <code>Layer</code>, <code>DPLayer</code></p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>class DP_WrappedResidual(tf.keras.layers.Layer, DPLayer):\n    def __init__(self, block, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.block = block\n\n    def call(self, inputs, *args, **kwargs):\n        assert len(inputs) == 2\n        i1, i2 = inputs\n        i2 = self.block(i2, *args, **kwargs)\n        return i1, i2\n\n    def backpropagate_params(self, input_bound, gradient_bound):\n        assert len(input_bound) == 2\n        assert len(gradient_bound) == 2\n        _, i2 = input_bound\n        _, g2 = gradient_bound\n        g2 = self.block.backpropagate_params(i2, g2)\n        return g2\n\n    def backpropagate_inputs(self, input_bound, gradient_bound):\n        assert len(input_bound) == 2\n        assert len(gradient_bound) == 2\n        _, i2 = input_bound\n        g1, g2 = gradient_bound\n        g2 = self.block.backpropagate_inputs(i2, g2)\n        return g1, g2\n\n    def propagate_inputs(self, input_bound):\n        assert len(input_bound) == 2\n        i1, i2 = input_bound\n        i2 = self.block.propagate_inputs(i2)\n        return i1, i2\n\n    def has_parameters(self):\n        return self.block.has_parameters()\n\n    @property\n    def nm_coef(self):\n        \"\"\"Returns the norm multiplier coefficient of the layer.\n\n        Remark: this is a property to mimic the behavior of an attribute.\n        \"\"\"\n        return self.block.nm_coef\n</code></pre>"},{"location":"api/layers/#deel.lipdp.layers.DP_WrappedResidual.nm_coef","title":"<code>nm_coef</code>  <code>property</code>","text":"<p>Returns the norm multiplier coefficient of the layer.</p> <p>Remark: this is a property to mimic the behavior of an attribute.</p>"},{"location":"api/layers/#deel.lipdp.layers.DP_GNP_Factory","title":"<code>DP_GNP_Factory(layer_cls)</code>","text":"<p>Factory for creating differentially private gradient norm preserving layers that don't have parameters.</p> <p>Remark: the layer is assumed to be GNP. This means that the gradient norm is preserved by the layer (i.e its Jacobian norm is 1). Please ensure that the layer is GNP before using this factory.</p> <p>Parameters:</p> Name Type Description Default <code>layer_cls</code> <p>Class of the layer to wrap.</p> required <p>Returns:</p> Type Description <p>A differentially private layer that doesn't have parameters.</p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>def DP_GNP_Factory(layer_cls):\n    \"\"\"Factory for creating differentially private gradient norm preserving layers that don't have parameters.\n\n    Remark: the layer is assumed to be GNP.\n    This means that the gradient norm is preserved by the layer (i.e its Jacobian norm is 1).\n    Please ensure that the layer is GNP before using this factory.\n\n    Args:\n        layer_cls: Class of the layer to wrap.\n\n    Returns:\n        A differentially private layer that doesn't have parameters.\n    \"\"\"\n\n    class DP_GNP(layer_cls, DPLayer):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n\n        def backpropagate_params(self, input_bound, gradient_bound):\n            raise ValueError(\"Layer doesn't have parameters\")\n\n        def backpropagate_inputs(self, input_bound, gradient_bound):\n            return 1 * gradient_bound\n\n        def propagate_inputs(self, input_bound):\n            return input_bound\n\n        def has_parameters(self):\n            return False\n\n    DP_GNP.__name__ = f\"DP_{layer_cls.__name__}\"\n    return DP_GNP\n</code></pre>"},{"location":"api/layers/#deel.lipdp.layers.clip_gradient","title":"<code>clip_gradient(x, clip_value)</code>","text":"<p>Clips the gradient during the backward pass.</p> <p>Behave like identity function during the forward pass.</p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>@tf.custom_gradient\ndef clip_gradient(x, clip_value):\n    \"\"\"Clips the gradient during the backward pass.\n\n    Behave like identity function during the forward pass.\n    \"\"\"\n\n    def grad_fn(dy):\n        # clip by norm each row\n        axes = list(range(1, len(dy.shape)))\n        clipped_dy = tf.clip_by_norm(dy, clip_value, axes=axes)\n        return clipped_dy, None  # No gradient for clip_value\n\n    return x, grad_fn\n</code></pre>"},{"location":"api/layers/#deel.lipdp.layers.make_residuals","title":"<code>make_residuals(merge_policy, wrapped_layers)</code>","text":"<p>Returns a list of layers that implement a residual block.</p> <p>Parameters:</p> Name Type Description Default <code>merge_policy</code> <p>either \"add\" or \"1-lip-add\".</p> required <code>wrapped_layers</code> <p>a list of layers that will be wrapped in residual blocks.</p> required <p>Returns:</p> Type Description <p>A list of layers that implement a residual block.</p> Source code in <code>deel/lipdp/layers.py</code> <pre><code>def make_residuals(merge_policy, wrapped_layers):\n    \"\"\"Returns a list of layers that implement a residual block.\n\n    Args:\n        merge_policy: either \"add\" or \"1-lip-add\".\n        wrapped_layers: a list of layers that will be wrapped in residual blocks.\n\n    Returns:\n        A list of layers that implement a residual block.\n    \"\"\"\n    layers = [DP_SplitResidual()]\n\n    for layer in wrapped_layers:\n        residual_block = DP_WrappedResidual(layer)\n        layers.append(residual_block)\n\n    layers.append(DP_MergeResidual(merge_policy))\n\n    return layers\n</code></pre>"},{"location":"api/losses/","title":"deel.lipdp.losses module","text":""},{"location":"api/losses/#deel.lipdp.losses.DP_KCosineSimilarity","title":"<code>DP_KCosineSimilarity</code>","text":"<p>             Bases: <code>Loss</code>, <code>DP_Loss</code></p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>class DP_KCosineSimilarity(Loss, DP_Loss):\n    def __init__(\n        self,\n        K=1.0,\n        axis=-1,\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name=\"cosine_similarity\",\n    ):\n        super().__init__(reduction=reduction, name=name)\n        # as the espilon is applied before the sqrt in tf.linalg.l2_normalize we\n        # apply square to it\n        self.K = K**2\n        self.axis = axis\n\n    @tf.function\n    def call(self, y_true, y_pred):\n        y_true = tf.linalg.l2_normalize(y_true, epsilon=self.K, axis=self.axis)\n        y_pred = tf.linalg.l2_normalize(y_pred, epsilon=self.K, axis=self.axis)\n        return -tf.reduce_sum(y_true * y_pred, axis=self.axis)\n\n    def get_L(self):\n        \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n        return 1 / float(self.K)\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_KCosineSimilarity.get_L","title":"<code>get_L()</code>","text":"<p>Lipschitz constant of the loss wrt the logits.</p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def get_L(self):\n    \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n    return 1 / float(self.K)\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_Loss","title":"<code>DP_Loss</code>","text":"Source code in <code>deel/lipdp/losses.py</code> <pre><code>class DP_Loss:\n    def get_L(self):\n        \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_Loss.get_L","title":"<code>get_L()</code>","text":"<p>Lipschitz constant of the loss wrt the logits.</p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def get_L(self):\n    \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_MeanAbsoluteError","title":"<code>DP_MeanAbsoluteError</code>","text":"<p>             Bases: <code>MeanAbsoluteError</code>, <code>DP_Loss</code></p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>class DP_MeanAbsoluteError(tf.keras.losses.MeanAbsoluteError, DP_Loss):\n    def __init__(\n        self,\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name=\"MulticlassKR\",\n    ):\n        r\"\"\"\n        Mean Absolute Error\n        \"\"\"\n        super(DP_MeanAbsoluteError, self).__init__(reduction=reduction, name=name)\n\n    def get_L(self):\n        \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n        return 1.0\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_MeanAbsoluteError.__init__","title":"<code>__init__(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='MulticlassKR')</code>","text":"<p>Mean Absolute Error</p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def __init__(\n    self,\n    reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    name=\"MulticlassKR\",\n):\n    r\"\"\"\n    Mean Absolute Error\n    \"\"\"\n    super(DP_MeanAbsoluteError, self).__init__(reduction=reduction, name=name)\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_MeanAbsoluteError.get_L","title":"<code>get_L()</code>","text":"<p>Lipschitz constant of the loss wrt the logits.</p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def get_L(self):\n    \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n    return 1.0\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_MulticlassHKR","title":"<code>DP_MulticlassHKR</code>","text":"<p>             Bases: <code>MulticlassHKR</code>, <code>DP_Loss</code></p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>class DP_MulticlassHKR(MulticlassHKR, DP_Loss):\n    def __init__(\n        self,\n        alpha=10.0,\n        min_margin=1.0,\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name=\"MulticlassHKR\",\n    ):\n        \"\"\"\n        The multiclass version of HKR. This is done by computing the HKR term over each\n        class and averaging the results.\n\n        Note that `y_true` should be one-hot encoded or pre-processed with the\n        `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n        Using a multi-GPU/TPU strategy requires to set `multi_gpu` to True and to\n        pre-process the labels `y_true` with the\n        `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n        Args:\n            alpha (float): regularization factor\n            min_margin (float): margin to enforce.\n            multi_gpu (bool): set to True when running on multi-GPU/TPU\n            reduction: passed to tf.keras.Loss constructor\n            name (str): passed to tf.keras.Loss constructor\n\n        \"\"\"\n        super(DP_MulticlassHKR, self).__init__(\n            alpha=alpha,\n            min_margin=min_margin,\n            multi_gpu=False,\n            reduction=reduction,\n            name=name,\n        )\n\n    def get_L(self):\n        \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n        return self.alpha + 1.0\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_MulticlassHKR.__init__","title":"<code>__init__(alpha=10.0, min_margin=1.0, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='MulticlassHKR')</code>","text":"<p>The multiclass version of HKR. This is done by computing the HKR term over each class and averaging the results.</p> <p>Note that <code>y_true</code> should be one-hot encoded or pre-processed with the <code>deel.lip.utils.process_labels_for_multi_gpu()</code> function.</p> <p>Using a multi-GPU/TPU strategy requires to set <code>multi_gpu</code> to True and to pre-process the labels <code>y_true</code> with the <code>deel.lip.utils.process_labels_for_multi_gpu()</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>regularization factor</p> <code>10.0</code> <code>min_margin</code> <code>float</code> <p>margin to enforce.</p> <code>1.0</code> <code>multi_gpu</code> <code>bool</code> <p>set to True when running on multi-GPU/TPU</p> required <code>reduction</code> <p>passed to tf.keras.Loss constructor</p> <code>SUM_OVER_BATCH_SIZE</code> <code>name</code> <code>str</code> <p>passed to tf.keras.Loss constructor</p> <code>'MulticlassHKR'</code> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def __init__(\n    self,\n    alpha=10.0,\n    min_margin=1.0,\n    reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    name=\"MulticlassHKR\",\n):\n    \"\"\"\n    The multiclass version of HKR. This is done by computing the HKR term over each\n    class and averaging the results.\n\n    Note that `y_true` should be one-hot encoded or pre-processed with the\n    `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n    Using a multi-GPU/TPU strategy requires to set `multi_gpu` to True and to\n    pre-process the labels `y_true` with the\n    `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n    Args:\n        alpha (float): regularization factor\n        min_margin (float): margin to enforce.\n        multi_gpu (bool): set to True when running on multi-GPU/TPU\n        reduction: passed to tf.keras.Loss constructor\n        name (str): passed to tf.keras.Loss constructor\n\n    \"\"\"\n    super(DP_MulticlassHKR, self).__init__(\n        alpha=alpha,\n        min_margin=min_margin,\n        multi_gpu=False,\n        reduction=reduction,\n        name=name,\n    )\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_MulticlassHKR.get_L","title":"<code>get_L()</code>","text":"<p>Lipschitz constant of the loss wrt the logits.</p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def get_L(self):\n    \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n    return self.alpha + 1.0\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_MulticlassHinge","title":"<code>DP_MulticlassHinge</code>","text":"<p>             Bases: <code>MulticlassHinge</code>, <code>DP_Loss</code></p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>class DP_MulticlassHinge(MulticlassHinge, DP_Loss):\n    def __init__(\n        self,\n        min_margin=1.0,\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name=\"MulticlassHinge\",\n    ):\n        \"\"\"\n        Loss to estimate the Hinge loss in a multiclass setup. It computes the\n        element-wise Hinge term. Note that this formulation differs from the one\n        commonly found in tensorflow/pytorch (which maximises the difference between\n        the two largest logits). This formulation is consistent with the binary\n        classification loss used in a multiclass fashion.\n\n        Note that `y_true` should be one-hot encoded or pre-processed with the\n        `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n        Args:\n            min_margin (float): margin to enforce.\n            reduction: passed to tf.keras.Loss constructor\n            name (str): passed to tf.keras.Loss constructor\n\n        \"\"\"\n        super(DP_MulticlassHinge, self).__init__(\n            min_margin=min_margin, reduction=reduction, name=name\n        )\n\n    def get_L(self):\n        \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n        return 1.0\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_MulticlassHinge.__init__","title":"<code>__init__(min_margin=1.0, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='MulticlassHinge')</code>","text":"<p>Loss to estimate the Hinge loss in a multiclass setup. It computes the element-wise Hinge term. Note that this formulation differs from the one commonly found in tensorflow/pytorch (which maximises the difference between the two largest logits). This formulation is consistent with the binary classification loss used in a multiclass fashion.</p> <p>Note that <code>y_true</code> should be one-hot encoded or pre-processed with the <code>deel.lip.utils.process_labels_for_multi_gpu()</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>min_margin</code> <code>float</code> <p>margin to enforce.</p> <code>1.0</code> <code>reduction</code> <p>passed to tf.keras.Loss constructor</p> <code>SUM_OVER_BATCH_SIZE</code> <code>name</code> <code>str</code> <p>passed to tf.keras.Loss constructor</p> <code>'MulticlassHinge'</code> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def __init__(\n    self,\n    min_margin=1.0,\n    reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    name=\"MulticlassHinge\",\n):\n    \"\"\"\n    Loss to estimate the Hinge loss in a multiclass setup. It computes the\n    element-wise Hinge term. Note that this formulation differs from the one\n    commonly found in tensorflow/pytorch (which maximises the difference between\n    the two largest logits). This formulation is consistent with the binary\n    classification loss used in a multiclass fashion.\n\n    Note that `y_true` should be one-hot encoded or pre-processed with the\n    `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n    Args:\n        min_margin (float): margin to enforce.\n        reduction: passed to tf.keras.Loss constructor\n        name (str): passed to tf.keras.Loss constructor\n\n    \"\"\"\n    super(DP_MulticlassHinge, self).__init__(\n        min_margin=min_margin, reduction=reduction, name=name\n    )\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_MulticlassHinge.get_L","title":"<code>get_L()</code>","text":"<p>Lipschitz constant of the loss wrt the logits.</p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def get_L(self):\n    \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n    return 1.0\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_MulticlassKR","title":"<code>DP_MulticlassKR</code>","text":"<p>             Bases: <code>MulticlassKR</code>, <code>DP_Loss</code></p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>class DP_MulticlassKR(MulticlassKR, DP_Loss):\n    def __init__(\n        self,\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name=\"MulticlassKR\",\n    ):\n        r\"\"\"\n        Loss to estimate average of Wasserstein-1 distance using Kantorovich-Rubinstein\n        duality over outputs. In this multiclass setup, the KR term is computed for each\n        class and then averaged.\n\n        Note that `y_true` should be one-hot encoded or pre-processed with the\n        `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n        Using a multi-GPU/TPU strategy requires to set `multi_gpu` to True and to\n        pre-process the labels `y_true` with the\n        `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n        Args:\n            multi_gpu (bool): set to True when running on multi-GPU/TPU\n            reduction: passed to tf.keras.Loss constructor\n            name (str): passed to tf.keras.Loss constructor\n\n        \"\"\"\n        super(DP_MulticlassKR, self).__init__(reduction=reduction, name=name)\n\n    def get_L(self):\n        \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n        return 1.0\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_MulticlassKR.__init__","title":"<code>__init__(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='MulticlassKR')</code>","text":"<p>Loss to estimate average of Wasserstein-1 distance using Kantorovich-Rubinstein duality over outputs. In this multiclass setup, the KR term is computed for each class and then averaged.</p> <p>Note that <code>y_true</code> should be one-hot encoded or pre-processed with the <code>deel.lip.utils.process_labels_for_multi_gpu()</code> function.</p> <p>Using a multi-GPU/TPU strategy requires to set <code>multi_gpu</code> to True and to pre-process the labels <code>y_true</code> with the <code>deel.lip.utils.process_labels_for_multi_gpu()</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>multi_gpu</code> <code>bool</code> <p>set to True when running on multi-GPU/TPU</p> required <code>reduction</code> <p>passed to tf.keras.Loss constructor</p> <code>SUM_OVER_BATCH_SIZE</code> <code>name</code> <code>str</code> <p>passed to tf.keras.Loss constructor</p> <code>'MulticlassKR'</code> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def __init__(\n    self,\n    reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    name=\"MulticlassKR\",\n):\n    r\"\"\"\n    Loss to estimate average of Wasserstein-1 distance using Kantorovich-Rubinstein\n    duality over outputs. In this multiclass setup, the KR term is computed for each\n    class and then averaged.\n\n    Note that `y_true` should be one-hot encoded or pre-processed with the\n    `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n    Using a multi-GPU/TPU strategy requires to set `multi_gpu` to True and to\n    pre-process the labels `y_true` with the\n    `deel.lip.utils.process_labels_for_multi_gpu()` function.\n\n    Args:\n        multi_gpu (bool): set to True when running on multi-GPU/TPU\n        reduction: passed to tf.keras.Loss constructor\n        name (str): passed to tf.keras.Loss constructor\n\n    \"\"\"\n    super(DP_MulticlassKR, self).__init__(reduction=reduction, name=name)\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_MulticlassKR.get_L","title":"<code>get_L()</code>","text":"<p>Lipschitz constant of the loss wrt the logits.</p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def get_L(self):\n    \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n    return 1.0\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_TauBCE","title":"<code>DP_TauBCE</code>","text":"<p>             Bases: <code>BinaryCrossentropy</code>, <code>DP_Loss</code></p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>class DP_TauBCE(tf.keras.losses.BinaryCrossentropy, DP_Loss):\n    def __init__(\n        self,\n        tau,\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name=\"TauBCE\",\n    ):\n        \"\"\"\n        Similar to original binary crossentropy, but with a settable temperature\n        parameter.\n\n        Args:\n            tau (float): temperature parameter.\n            reduction: reduction of the loss, must be SUM_OVER_BATCH_SIZE in order have a correct accounting.\n            name (str): name of the loss\n        \"\"\"\n        super().__init__(from_logits=True, reduction=reduction, name=name)\n        self.tau = tau\n\n    def call(self, y_true, y_pred):\n        y_pred = y_pred * self.tau\n        return super().call(y_true, y_pred) / self.tau\n\n    def get_L(self):\n        \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n        # as the implementation divide the loss by self.tau (and as it is used with \"from_logit=True\")\n        return 1.0\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_TauBCE.__init__","title":"<code>__init__(tau, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='TauBCE')</code>","text":"<p>Similar to original binary crossentropy, but with a settable temperature parameter.</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>temperature parameter.</p> required <code>reduction</code> <p>reduction of the loss, must be SUM_OVER_BATCH_SIZE in order have a correct accounting.</p> <code>SUM_OVER_BATCH_SIZE</code> <code>name</code> <code>str</code> <p>name of the loss</p> <code>'TauBCE'</code> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def __init__(\n    self,\n    tau,\n    reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    name=\"TauBCE\",\n):\n    \"\"\"\n    Similar to original binary crossentropy, but with a settable temperature\n    parameter.\n\n    Args:\n        tau (float): temperature parameter.\n        reduction: reduction of the loss, must be SUM_OVER_BATCH_SIZE in order have a correct accounting.\n        name (str): name of the loss\n    \"\"\"\n    super().__init__(from_logits=True, reduction=reduction, name=name)\n    self.tau = tau\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_TauBCE.get_L","title":"<code>get_L()</code>","text":"<p>Lipschitz constant of the loss wrt the logits.</p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def get_L(self):\n    \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n    # as the implementation divide the loss by self.tau (and as it is used with \"from_logit=True\")\n    return 1.0\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_TauCategoricalCrossentropy","title":"<code>DP_TauCategoricalCrossentropy</code>","text":"<p>             Bases: <code>TauCategoricalCrossentropy</code>, <code>DP_Loss</code></p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>class DP_TauCategoricalCrossentropy(TauCategoricalCrossentropy, DP_Loss):\n    def __init__(\n        self,\n        tau,\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name=\"TauCategoricalCrossentropy\",\n    ):\n        \"\"\"\n        Similar to original categorical crossentropy, but with a settable temperature\n        parameter.\n\n        Args:\n            tau (float): temperature parameter.\n            reduction: reduction of the loss, must be SUM_OVER_BATCH_SIZE in order have a correct accounting.\n            name (str): name of the loss\n        \"\"\"\n        super(DP_TauCategoricalCrossentropy, self).__init__(\n            tau=tau, reduction=reduction, name=name\n        )\n\n    def get_L(self):\n        \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n        # as the implementation divide the loss by self.tau (and as it is used with \"from_logit=True\")\n        return math.sqrt(2)\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_TauCategoricalCrossentropy.__init__","title":"<code>__init__(tau, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='TauCategoricalCrossentropy')</code>","text":"<p>Similar to original categorical crossentropy, but with a settable temperature parameter.</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>temperature parameter.</p> required <code>reduction</code> <p>reduction of the loss, must be SUM_OVER_BATCH_SIZE in order have a correct accounting.</p> <code>SUM_OVER_BATCH_SIZE</code> <code>name</code> <code>str</code> <p>name of the loss</p> <code>'TauCategoricalCrossentropy'</code> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def __init__(\n    self,\n    tau,\n    reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    name=\"TauCategoricalCrossentropy\",\n):\n    \"\"\"\n    Similar to original categorical crossentropy, but with a settable temperature\n    parameter.\n\n    Args:\n        tau (float): temperature parameter.\n        reduction: reduction of the loss, must be SUM_OVER_BATCH_SIZE in order have a correct accounting.\n        name (str): name of the loss\n    \"\"\"\n    super(DP_TauCategoricalCrossentropy, self).__init__(\n        tau=tau, reduction=reduction, name=name\n    )\n</code></pre>"},{"location":"api/losses/#deel.lipdp.losses.DP_TauCategoricalCrossentropy.get_L","title":"<code>get_L()</code>","text":"<p>Lipschitz constant of the loss wrt the logits.</p> Source code in <code>deel/lipdp/losses.py</code> <pre><code>def get_L(self):\n    \"\"\"Lipschitz constant of the loss wrt the logits.\"\"\"\n    # as the implementation divide the loss by self.tau (and as it is used with \"from_logit=True\")\n    return math.sqrt(2)\n</code></pre>"},{"location":"api/model/","title":"deel.lipdp.model module","text":""},{"location":"api/model/#deel.lipdp.model.DPParameters","title":"<code>DPParameters</code>  <code>dataclass</code>","text":"<p>Parameters used to set the dp training.</p> <p>Attributes:</p> Name Type Description <code>noisify_strategy</code> <code>str</code> <p>either \"per-layer\" or \"global\".</p> <code>noise_multiplier</code> <code>float</code> <p>noise multiplier.</p> <code>delta</code> <code>float</code> <p>delta parameter for DP.</p> Source code in <code>deel/lipdp/model.py</code> <pre><code>@dataclass\nclass DPParameters:\n    \"\"\"Parameters used to set the dp training.\n\n    Attributes:\n        noisify_strategy (str): either \"per-layer\" or \"global\".\n        noise_multiplier (float): noise multiplier.\n        delta (float): delta parameter for DP.\n    \"\"\"\n\n    noisify_strategy: str\n    noise_multiplier: float\n    delta: float\n</code></pre>"},{"location":"api/model/#deel.lipdp.model.DP_Accountant","title":"<code>DP_Accountant</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback to compute the DP guarantees at the end of each epoch.</p> <p>Note: wandb is not a strict requirement for this callback to work, logging is also supported.</p> <p>Attributes:</p> Name Type Description <code>log_fn</code> <p>log function to use. Takes a dictionary of (key, value) pairs as input.     if 'wandb', use wandb.log.     if 'logging', use logging.info.     if 'all', use both wandb.log and logging.info.</p> Source code in <code>deel/lipdp/model.py</code> <pre><code>class DP_Accountant(keras.callbacks.Callback):\n    \"\"\"Callback to compute the DP guarantees at the end of each epoch.\n\n    Note: wandb is not a strict requirement for this callback to work, logging is also supported.\n\n    Attributes:\n        log_fn: log function to use. Takes a dictionary of (key, value) pairs as input.\n                if 'wandb', use wandb.log.\n                if 'logging', use logging.info.\n                if 'all', use both wandb.log and logging.info.\n    \"\"\"\n\n    def __init__(self, log_fn=\"all\"):\n        super().__init__()\n        if log_fn == \"wandb\":\n            import wandb\n\n            log_fn = wandb.log\n        elif log_fn == \"logging\":\n            import logging\n\n            log_fn = logging.info\n        elif log_fn == \"all\":\n            import wandb\n            import logging\n\n            log_fn = lambda x: [wandb.log(x), logging.info(x)]\n        self.log_fn = log_fn\n\n    def on_epoch_end(self, epoch, logs=None):\n        epsilon, delta = get_eps_delta(model=self.model, epochs=epoch + 1)\n        print(f\"\\n {(epsilon,delta)}-DP guarantees for epoch {epoch+1} \\n\")\n\n        # plot epoch at the same time as epsilon and delta to ease comparison of plots in wandb API.\n        to_log = {\n            \"epsilon\": epsilon,\n            \"delta\": delta,\n            \"epoch\": epoch + 1,\n        }\n\n        last_layer = self.model.layers_backward_order()[0]\n        if isinstance(last_layer, deel.lipdp.layers.DP_ClipGradient):\n            clipping_value = float(last_layer.clip_value.numpy())\n            to_log[\"clipping_value\"] = clipping_value\n\n        self.log_fn(to_log)\n</code></pre>"},{"location":"api/model/#deel.lipdp.model.DP_Model","title":"<code>DP_Model</code>","text":"<p>             Bases: <code>Model</code></p> Source code in <code>deel/lipdp/model.py</code> <pre><code>class DP_Model(deel.lip.model.Model):\n    def __init__(\n        self,\n        dp_layers,\n        *args,\n        dp_parameters: DPParameters,\n        dataset_metadata: DatasetMetadata,\n        debug: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Model Class based on the DEEL Sequential model. Only layer from the lipdp.layers module are allowed since\n        the framework assume 1 lipschitz layers.\n\n        Args:\n            dp_layers: the list of layers to use ( as done in sequential ) but here we can leverage\n                the fact that layers may have multiple inputs/outputs.\n            dp_parameters (DPParameters): parameters used to set the dp procedure.\n            dataset_metadata (DatasetMetadata): information about the dataset. Must contain: the input shape, number\n                of training samples, the input bound, number of batches in the dataset and the batch size.\n            debug (bool, optional): when true print additionnal debug informations (must be in eager mode). Defaults to False.\n\n        Note:\n            The model is then calibrated to verify (epsilon,delta)-DP guarantees by noisying the values of the gradients during the training step.\n            DP accounting is done with the associated Callback.\n\n        Raises:\n            TypeError: when the dp_parameters.noisify_strategy is not one of \"per-layer\" or \"global\"\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.dp_layers = dp_layers\n        self.dp_parameters = dp_parameters\n        self.dataset_metadata = dataset_metadata\n        self.debug = debug\n        if self.dp_parameters.noisify_strategy == \"global\":\n            self.noisify_fun = global_noisify\n        elif self.dp_parameters.noisify_strategy == \"per-layer\":\n            self.noisify_fun = local_noisify\n        else:\n            raise TypeError(\n                \"Incorrect noisify_strategy argument during model initialisation.\"\n            )\n\n    def layers_forward_order(self):\n        return self.dp_layers\n\n    def layers_backward_order(self):\n        return self.dp_layers[::-1]\n\n    def call(self, inputs, *args, **kwargs):\n        x = inputs\n        for layer in self.layers_forward_order():\n            x = layer(x, *args, **kwargs)\n        return x\n\n    def signal_to_noise_elementwise(self, data):\n        \"\"\"Compute the signal to noise ratio of the model.\n\n        Args:\n            data: a tuple (x,y) of a batch of data.\n\n        Returns:\n            ratios: dictionary of signal to noise ratios. Keys are trainable variables names.\n            norms: dictionary of gradient norms. Keys are trainable variables names.\n            bounds: dictionary of gradient norm bounds. Keys are trainable variables names.\n        \"\"\"\n        import tqdm\n\n        examples, labels = data\n\n        trainable_vars = self.trainable_variables\n        names = [v.name for v in trainable_vars]\n\n        bounds = compute_gradient_bounds(model=self)\n        batch_size = self.dataset_metadata.batch_size\n        bounds = {name: bound * batch_size for name, bound in bounds.items()}\n\n        norms = {name: [] for name in names}\n        ratios = {name: [] for name in names}\n        total = len(examples)\n        for x, y in tqdm.tqdm(zip(examples, labels), total=total):\n            with tf.GradientTape() as tape:\n                x = tf.expand_dims(x, axis=0)\n                y = tf.expand_dims(y, axis=0)\n                y_pred = self(x, training=True)\n                loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n            gradient_element = tape.gradient(loss, self.trainable_variables)\n            norms_element = [tf.linalg.norm(g, axis=None) for g in gradient_element]\n            norms_element = {name: norm for name, norm in zip(names, norms_element)}\n            for name in names:\n                norms[name].append(norms_element[name].numpy())\n\n            ratios_element = {}\n            for name in names:\n                ratios_element[name] = norms_element[name] / bounds[name]\n            for name in names:\n                ratios[name].append(ratios_element[name])\n\n        ratios = {name: np.stack(ratios[name]) for name in names}\n        norms = {name: np.stack(norms[name]) for name in names}\n\n        return ratios, norms, bounds\n\n    def signal_to_noise_average(self, data):\n        \"\"\"Compute the signal to noise ratio of the model.\n\n        Args:\n            data: a tuple (x,y) of a batch of data. The batch size must be equal to the one of the dataset.\n\n        Returns:\n            ratios: dictionary of signal to noise ratios. Keys are trainable variables names.\n            norms: dictionary of gradient norms. Keys are trainable variables names.\n            bounds: dictionary of gradient norm bounds. Keys are trainable variables names.\n        \"\"\"\n        x, y = data\n\n        assert (\n            x.shape[0] == self.dataset_metadata.batch_size\n        ), \"Batch size must be equal to the one of the dataset\"\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # tf.cast(y_pred,dtype=y.dtype)\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # gradient norms\n        norms = [tf.linalg.norm(g, axis=None) for g in gradients]\n        names = [v.name for v in trainable_vars]\n        norms = {name: norm for name, norm in zip(names, norms)}\n\n        # Get gradient bounds\n        bounds = compute_gradient_bounds(model=self)\n        batch_size = self.dataset_metadata.batch_size\n        bounds = {name: (bound * batch_size) for name, bound in bounds.items()}\n\n        ratios = {}\n        for name in names:\n            ratios[name] = norms[name] / bounds[name]\n        return ratios, norms, bounds\n\n    # Define the differentially private training step\n    def train_step(self, data):\n        \"\"\"Train step of the model with DP guarantees.\n\n        Args:\n            data: a tuple (x,y) of a batch of data.\n\n        Returns:\n            metrics: dictionary of metrics.\n        \"\"\"\n        # Unpack data\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # tf.cast(y_pred,dtype=y.dtype)\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        # Get gradient bounds\n        gradient_bounds = compute_gradient_bounds(model=self)\n        noisy_gradients = self.noisify_fun(\n            self, gradient_bounds, trainable_vars, gradients\n        )\n        # Each optimizer is a postprocessing of private gradients\n        self.optimizer.apply_gradients(zip(noisy_gradients, trainable_vars))\n\n        # Update Metrics\n        self.compiled_metrics.update_state(y, y_pred)\n\n        # Condense to ensure Lipschitz constraints |W|_2 = 1\n        self.condense()\n        return {m.name: m.result() for m in self.metrics}\n</code></pre>"},{"location":"api/model/#deel.lipdp.model.DP_Model.__init__","title":"<code>__init__(dp_layers, *args, dp_parameters, dataset_metadata, debug=False, **kwargs)</code>","text":"<p>Model Class based on the DEEL Sequential model. Only layer from the lipdp.layers module are allowed since the framework assume 1 lipschitz layers.</p> <p>Parameters:</p> Name Type Description Default <code>dp_layers</code> <p>the list of layers to use ( as done in sequential ) but here we can leverage the fact that layers may have multiple inputs/outputs.</p> required <code>dp_parameters</code> <code>DPParameters</code> <p>parameters used to set the dp procedure.</p> required <code>dataset_metadata</code> <code>DatasetMetadata</code> <p>information about the dataset. Must contain: the input shape, number of training samples, the input bound, number of batches in the dataset and the batch size.</p> required <code>debug</code> <code>bool</code> <p>when true print additionnal debug informations (must be in eager mode). Defaults to False.</p> <code>False</code> Note <p>The model is then calibrated to verify (epsilon,delta)-DP guarantees by noisying the values of the gradients during the training step. DP accounting is done with the associated Callback.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>when the dp_parameters.noisify_strategy is not one of \"per-layer\" or \"global\"</p> Source code in <code>deel/lipdp/model.py</code> <pre><code>def __init__(\n    self,\n    dp_layers,\n    *args,\n    dp_parameters: DPParameters,\n    dataset_metadata: DatasetMetadata,\n    debug: bool = False,\n    **kwargs,\n):\n    \"\"\"Model Class based on the DEEL Sequential model. Only layer from the lipdp.layers module are allowed since\n    the framework assume 1 lipschitz layers.\n\n    Args:\n        dp_layers: the list of layers to use ( as done in sequential ) but here we can leverage\n            the fact that layers may have multiple inputs/outputs.\n        dp_parameters (DPParameters): parameters used to set the dp procedure.\n        dataset_metadata (DatasetMetadata): information about the dataset. Must contain: the input shape, number\n            of training samples, the input bound, number of batches in the dataset and the batch size.\n        debug (bool, optional): when true print additionnal debug informations (must be in eager mode). Defaults to False.\n\n    Note:\n        The model is then calibrated to verify (epsilon,delta)-DP guarantees by noisying the values of the gradients during the training step.\n        DP accounting is done with the associated Callback.\n\n    Raises:\n        TypeError: when the dp_parameters.noisify_strategy is not one of \"per-layer\" or \"global\"\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.dp_layers = dp_layers\n    self.dp_parameters = dp_parameters\n    self.dataset_metadata = dataset_metadata\n    self.debug = debug\n    if self.dp_parameters.noisify_strategy == \"global\":\n        self.noisify_fun = global_noisify\n    elif self.dp_parameters.noisify_strategy == \"per-layer\":\n        self.noisify_fun = local_noisify\n    else:\n        raise TypeError(\n            \"Incorrect noisify_strategy argument during model initialisation.\"\n        )\n</code></pre>"},{"location":"api/model/#deel.lipdp.model.DP_Model.signal_to_noise_average","title":"<code>signal_to_noise_average(data)</code>","text":"<p>Compute the signal to noise ratio of the model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>a tuple (x,y) of a batch of data. The batch size must be equal to the one of the dataset.</p> required <p>Returns:</p> Name Type Description <code>ratios</code> <p>dictionary of signal to noise ratios. Keys are trainable variables names.</p> <code>norms</code> <p>dictionary of gradient norms. Keys are trainable variables names.</p> <code>bounds</code> <p>dictionary of gradient norm bounds. Keys are trainable variables names.</p> Source code in <code>deel/lipdp/model.py</code> <pre><code>def signal_to_noise_average(self, data):\n    \"\"\"Compute the signal to noise ratio of the model.\n\n    Args:\n        data: a tuple (x,y) of a batch of data. The batch size must be equal to the one of the dataset.\n\n    Returns:\n        ratios: dictionary of signal to noise ratios. Keys are trainable variables names.\n        norms: dictionary of gradient norms. Keys are trainable variables names.\n        bounds: dictionary of gradient norm bounds. Keys are trainable variables names.\n    \"\"\"\n    x, y = data\n\n    assert (\n        x.shape[0] == self.dataset_metadata.batch_size\n    ), \"Batch size must be equal to the one of the dataset\"\n\n    with tf.GradientTape() as tape:\n        y_pred = self(x, training=True)  # Forward pass\n        # tf.cast(y_pred,dtype=y.dtype)\n        # Compute the loss value\n        # (the loss function is configured in `compile()`)\n        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n    # Compute gradients\n    trainable_vars = self.trainable_variables\n    gradients = tape.gradient(loss, trainable_vars)\n\n    # gradient norms\n    norms = [tf.linalg.norm(g, axis=None) for g in gradients]\n    names = [v.name for v in trainable_vars]\n    norms = {name: norm for name, norm in zip(names, norms)}\n\n    # Get gradient bounds\n    bounds = compute_gradient_bounds(model=self)\n    batch_size = self.dataset_metadata.batch_size\n    bounds = {name: (bound * batch_size) for name, bound in bounds.items()}\n\n    ratios = {}\n    for name in names:\n        ratios[name] = norms[name] / bounds[name]\n    return ratios, norms, bounds\n</code></pre>"},{"location":"api/model/#deel.lipdp.model.DP_Model.signal_to_noise_elementwise","title":"<code>signal_to_noise_elementwise(data)</code>","text":"<p>Compute the signal to noise ratio of the model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>a tuple (x,y) of a batch of data.</p> required <p>Returns:</p> Name Type Description <code>ratios</code> <p>dictionary of signal to noise ratios. Keys are trainable variables names.</p> <code>norms</code> <p>dictionary of gradient norms. Keys are trainable variables names.</p> <code>bounds</code> <p>dictionary of gradient norm bounds. Keys are trainable variables names.</p> Source code in <code>deel/lipdp/model.py</code> <pre><code>def signal_to_noise_elementwise(self, data):\n    \"\"\"Compute the signal to noise ratio of the model.\n\n    Args:\n        data: a tuple (x,y) of a batch of data.\n\n    Returns:\n        ratios: dictionary of signal to noise ratios. Keys are trainable variables names.\n        norms: dictionary of gradient norms. Keys are trainable variables names.\n        bounds: dictionary of gradient norm bounds. Keys are trainable variables names.\n    \"\"\"\n    import tqdm\n\n    examples, labels = data\n\n    trainable_vars = self.trainable_variables\n    names = [v.name for v in trainable_vars]\n\n    bounds = compute_gradient_bounds(model=self)\n    batch_size = self.dataset_metadata.batch_size\n    bounds = {name: bound * batch_size for name, bound in bounds.items()}\n\n    norms = {name: [] for name in names}\n    ratios = {name: [] for name in names}\n    total = len(examples)\n    for x, y in tqdm.tqdm(zip(examples, labels), total=total):\n        with tf.GradientTape() as tape:\n            x = tf.expand_dims(x, axis=0)\n            y = tf.expand_dims(y, axis=0)\n            y_pred = self(x, training=True)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n        gradient_element = tape.gradient(loss, self.trainable_variables)\n        norms_element = [tf.linalg.norm(g, axis=None) for g in gradient_element]\n        norms_element = {name: norm for name, norm in zip(names, norms_element)}\n        for name in names:\n            norms[name].append(norms_element[name].numpy())\n\n        ratios_element = {}\n        for name in names:\n            ratios_element[name] = norms_element[name] / bounds[name]\n        for name in names:\n            ratios[name].append(ratios_element[name])\n\n    ratios = {name: np.stack(ratios[name]) for name in names}\n    norms = {name: np.stack(norms[name]) for name in names}\n\n    return ratios, norms, bounds\n</code></pre>"},{"location":"api/model/#deel.lipdp.model.DP_Model.train_step","title":"<code>train_step(data)</code>","text":"<p>Train step of the model with DP guarantees.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>a tuple (x,y) of a batch of data.</p> required <p>Returns:</p> Name Type Description <code>metrics</code> <p>dictionary of metrics.</p> Source code in <code>deel/lipdp/model.py</code> <pre><code>def train_step(self, data):\n    \"\"\"Train step of the model with DP guarantees.\n\n    Args:\n        data: a tuple (x,y) of a batch of data.\n\n    Returns:\n        metrics: dictionary of metrics.\n    \"\"\"\n    # Unpack data\n    x, y = data\n\n    with tf.GradientTape() as tape:\n        y_pred = self(x, training=True)  # Forward pass\n        # tf.cast(y_pred,dtype=y.dtype)\n        # Compute the loss value\n        # (the loss function is configured in `compile()`)\n        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n    # Compute gradients\n    trainable_vars = self.trainable_variables\n    gradients = tape.gradient(loss, trainable_vars)\n    # Get gradient bounds\n    gradient_bounds = compute_gradient_bounds(model=self)\n    noisy_gradients = self.noisify_fun(\n        self, gradient_bounds, trainable_vars, gradients\n    )\n    # Each optimizer is a postprocessing of private gradients\n    self.optimizer.apply_gradients(zip(noisy_gradients, trainable_vars))\n\n    # Update Metrics\n    self.compiled_metrics.update_state(y, y_pred)\n\n    # Condense to ensure Lipschitz constraints |W|_2 = 1\n    self.condense()\n    return {m.name: m.result() for m in self.metrics}\n</code></pre>"},{"location":"api/model/#deel.lipdp.model.DP_Sequential","title":"<code>DP_Sequential</code>","text":"<p>             Bases: <code>Sequential</code></p> Source code in <code>deel/lipdp/model.py</code> <pre><code>class DP_Sequential(deel.lip.model.Sequential):\n    def __init__(\n        self,\n        *args,\n        dp_parameters: DPParameters,\n        dataset_metadata: DatasetMetadata,\n        debug: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Model Class based on the DEEL Sequential model. Only layer from the lipdp.layers module are allowed since\n        the framework assume 1 lipschitz layers.\n\n        Args:\n            dp_parameters (DPParameters): parameters used to set the dp procedure.\n            dataset_metadata (DatasetMetadata): information about the dataset. Must contain: the input shape, number\n                of training samples, the input bound, number of batches in the dataset and the batch size.\n            debug (bool, optional): when true print additionnal debug informations (must be in eager mode). Defaults to False.\n\n        Note:\n            The model is then calibrated to verify (epsilon,delta)-DP guarantees by noisying the values of the gradients during the training step.\n            DP accounting is done with the associated Callback.\n\n        Raises:\n            TypeError: when the dp_parameters.noisify_strategy is not one of \"per-layer\" or \"global\"\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.dp_parameters = dp_parameters\n        self.dataset_metadata = dataset_metadata\n        self.debug = debug\n        if self.dp_parameters.noisify_strategy == \"global\":\n            self.noisify_fun = global_noisify\n        elif self.dp_parameters.noisify_strategy == \"per-layer\":\n            self.noisify_fun = local_noisify\n        else:\n            raise TypeError(\n                \"Incorrect noisify_strategy argument during model initialisation.\"\n            )\n\n    def layers_forward_order(self):\n        return self.layers\n\n    def layers_backward_order(self):\n        return self.layers[::-1]\n\n    # Define the differentially private training step\n    def train_step(self, data):\n        # Unpack data\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # tf.cast(y_pred,dtype=y.dtype)\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        # Get gradient bounds\n        gradient_bounds = compute_gradient_bounds(model=self)\n        noisy_gradients = self.noisify_fun(\n            self, gradient_bounds, trainable_vars, gradients\n        )\n        # Each optimizer is a postprocessing of the already (epsilon,delta)-DP gradients\n        self.optimizer.apply_gradients(zip(noisy_gradients, trainable_vars))\n        # self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        # Update Metrics\n        self.compiled_metrics.update_state(y, y_pred)\n        # Condense to verify |W|_2 = 1\n        self.condense()\n        return {m.name: m.result() for m in self.metrics}\n</code></pre>"},{"location":"api/model/#deel.lipdp.model.DP_Sequential.__init__","title":"<code>__init__(*args, dp_parameters, dataset_metadata, debug=False, **kwargs)</code>","text":"<p>Model Class based on the DEEL Sequential model. Only layer from the lipdp.layers module are allowed since the framework assume 1 lipschitz layers.</p> <p>Parameters:</p> Name Type Description Default <code>dp_parameters</code> <code>DPParameters</code> <p>parameters used to set the dp procedure.</p> required <code>dataset_metadata</code> <code>DatasetMetadata</code> <p>information about the dataset. Must contain: the input shape, number of training samples, the input bound, number of batches in the dataset and the batch size.</p> required <code>debug</code> <code>bool</code> <p>when true print additionnal debug informations (must be in eager mode). Defaults to False.</p> <code>False</code> Note <p>The model is then calibrated to verify (epsilon,delta)-DP guarantees by noisying the values of the gradients during the training step. DP accounting is done with the associated Callback.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>when the dp_parameters.noisify_strategy is not one of \"per-layer\" or \"global\"</p> Source code in <code>deel/lipdp/model.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    dp_parameters: DPParameters,\n    dataset_metadata: DatasetMetadata,\n    debug: bool = False,\n    **kwargs,\n):\n    \"\"\"Model Class based on the DEEL Sequential model. Only layer from the lipdp.layers module are allowed since\n    the framework assume 1 lipschitz layers.\n\n    Args:\n        dp_parameters (DPParameters): parameters used to set the dp procedure.\n        dataset_metadata (DatasetMetadata): information about the dataset. Must contain: the input shape, number\n            of training samples, the input bound, number of batches in the dataset and the batch size.\n        debug (bool, optional): when true print additionnal debug informations (must be in eager mode). Defaults to False.\n\n    Note:\n        The model is then calibrated to verify (epsilon,delta)-DP guarantees by noisying the values of the gradients during the training step.\n        DP accounting is done with the associated Callback.\n\n    Raises:\n        TypeError: when the dp_parameters.noisify_strategy is not one of \"per-layer\" or \"global\"\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.dp_parameters = dp_parameters\n    self.dataset_metadata = dataset_metadata\n    self.debug = debug\n    if self.dp_parameters.noisify_strategy == \"global\":\n        self.noisify_fun = global_noisify\n    elif self.dp_parameters.noisify_strategy == \"per-layer\":\n        self.noisify_fun = local_noisify\n    else:\n        raise TypeError(\n            \"Incorrect noisify_strategy argument during model initialisation.\"\n        )\n</code></pre>"},{"location":"api/model/#deel.lipdp.model.compute_gradient_bounds","title":"<code>compute_gradient_bounds(model)</code>","text":"<p>Compute the gradient norm bounds of the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>model to train.</p> required <p>Returns:</p> Name Type Description <code>gradient_bounds</code> <p>dictionary of gradient norm bounds with (key, value) pairs (layer_name, gradient_bound).              The order of the bounds is the same as              the order of the layers returned by model.layers_backward_order().</p> Source code in <code>deel/lipdp/model.py</code> <pre><code>def compute_gradient_bounds(model):\n    \"\"\"Compute the gradient norm bounds of the model.\n\n    Args:\n        model: model to train.\n\n    Returns:\n        gradient_bounds: dictionary of gradient norm bounds with (key, value) pairs (layer_name, gradient_bound).\n                         The order of the bounds is the same as\n                         the order of the layers returned by model.layers_backward_order().\n    \"\"\"\n    # Initialisation, get lipschitz constant of loss\n    input_bounds = {}\n    gradient_bounds = {}\n    input_bound = None  # Unknown at the start.\n\n    # Forward pass to assess maximum activation norms\n    for layer in model.layers_forward_order():\n        assert isinstance(layer, DPLayer)\n        if model.debug:\n            print(f\"Layer {layer.name} input bound: {input_bound}\")\n        input_bounds[layer.name] = input_bound\n        input_bound = layer.propagate_inputs(input_bound)\n\n    if model.debug:\n        print(f\"Layer {layer.name} input bound: {input_bound}\")\n\n    # since we aggregate using SUM_OVER_BATCH\n    gradient_bound = tf.convert_to_tensor(model.loss.get_L()) / tf.convert_to_tensor(\n        model.dataset_metadata.batch_size, dtype=tf.float32\n    )\n\n    # Backward pass to compute gradient norm bounds and accumulate Lip constant\n    for layer in model.layers_backward_order():\n        assert isinstance(layer, DPLayer)\n        layer_input_bound = input_bounds[layer.name]\n        if layer.has_parameters():\n            assert len(layer.trainable_variables) == 1\n            var_name = layer.trainable_variables[0].name\n            gradient_bounds[var_name] = layer.backpropagate_params(\n                layer_input_bound, gradient_bound\n            )\n        gradient_bound = layer.backpropagate_inputs(layer_input_bound, gradient_bound)\n\n    # Return gradient bounds\n    return gradient_bounds\n</code></pre>"},{"location":"api/model/#deel.lipdp.model.get_eps_delta","title":"<code>get_eps_delta(model, epochs)</code>","text":"<p>Compute the (epsilon, delta)-DP guarantees of the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>model to train.</p> required <code>epochs</code> <p>number of epochs elapsed.</p> required <p>Returns:</p> Name Type Description <code>epsilon</code> <p>epsilon parameter of the (epsilon, delta)-DP guarantee.</p> <code>delta</code> <p>delta parameter of the (epsilon, delta)-DP guarantee.</p> Source code in <code>deel/lipdp/model.py</code> <pre><code>def get_eps_delta(model, epochs):\n    \"\"\"Compute the (epsilon, delta)-DP guarantees of the model.\n\n    Args:\n        model: model to train.\n        epochs: number of epochs elapsed.\n\n    Returns:\n        epsilon: epsilon parameter of the (epsilon, delta)-DP guarantee.\n        delta: delta parameter of the (epsilon, delta)-DP guarantee.\n    \"\"\"\n    num_grad_steps = epochs * model.dataset_metadata.nb_steps_per_epochs\n\n    prob = model.dataset_metadata.batch_size / model.dataset_metadata.nb_samples_train\n\n    # Dynamic clipping might be used.\n    last_layer = model.layers_backward_order()[0]\n    dynamic_clipping = {\"mode\": \"fixed\"}\n    if isinstance(last_layer, deel.lipdp.layers.DP_ClipGradient):\n        dynamic_clipping.update(last_layer._dynamic_dp_dict)  # copy dict\n        if \"patience\" in dynamic_clipping:\n            dynamic_clipping[\"num_updates\"] = epochs // dynamic_clipping[\"patience\"]\n\n    if model.dp_parameters.noisify_strategy == \"per-layer\":\n        nm_coefs = get_noise_multiplier_coefs(model)\n        noise_multipliers = [\n            model.dp_parameters.noise_multiplier * coef for coef in nm_coefs.values()\n        ]\n        mode = \"per-layer\"\n    elif model.dp_parameters.noisify_strategy == \"global\":\n        noise_multipliers = model.dp_parameters.noise_multiplier\n        mode = \"global\"\n\n    mech = DPGD_Mechanism(\n        mode=mode,\n        prob=prob,\n        noise_multipliers=noise_multipliers,\n        num_grad_steps=num_grad_steps,\n        delta=model.dp_parameters.delta,\n        dynamic_clipping=dynamic_clipping,\n    )\n\n    return mech.epsilon, mech.delta\n</code></pre>"},{"location":"api/model/#deel.lipdp.model.get_noise_multiplier_coefs","title":"<code>get_noise_multiplier_coefs(model)</code>","text":"<p>Get the noise multiplier coefficients of the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>model to train.</p> required <p>Returns:</p> Name Type Description <code>dict_coefs</code> <p>dictionary of noise multiplier coefficients.         The order of the coefficients is the same as         the order of the layers returned by model.layers_forward_order().</p> Source code in <code>deel/lipdp/model.py</code> <pre><code>def get_noise_multiplier_coefs(model):\n    \"\"\"Get the noise multiplier coefficients of the model.\n\n    Args:\n        model: model to train.\n\n    Returns:\n        dict_coefs: dictionary of noise multiplier coefficients.\n                    The order of the coefficients is the same as\n                    the order of the layers returned by model.layers_forward_order().\n    \"\"\"\n    dict_coefs = {}\n    for (\n        layer\n    ) in (\n        model.layers_forward_order()\n    ):  # remark: insertion order is preserved in Python 3.7+\n        assert isinstance(layer, DPLayer)\n        if layer.has_parameters():\n            assert len(layer.trainable_variables) == 1\n            dict_coefs[layer.trainable_variables[0].name] = layer.nm_coef\n    return dict_coefs\n</code></pre>"},{"location":"api/model/#deel.lipdp.model.global_noisify","title":"<code>global_noisify(model, gradient_bounds, trainable_vars, gradients)</code>","text":"<p>Add noise to gradients.</p> a single global noise is added to all gradients, based on the global sensitivity. <p>This is the default behaviour of the original DPGD algorithm. This may yield looser privacy bounds than local noisify.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>model to train.</p> required <code>gradient_bounds</code> <p>dictionary of gradient norm upper bounds. The keys are the names of the trainable variables.</p> required <code>trainable_vars</code> <p>list of trainable variables. The list is in the same order as gradients.</p> required <code>gradients</code> <p>list of gradients to add noise to. The list is in the same order as trainable_vars.</p> required <p>Returns:</p> Name Type Description <code>noisy_grads</code> <p>list of noisy gradients. The list is in the same order as trainable_vars.</p> Source code in <code>deel/lipdp/model.py</code> <pre><code>def global_noisify(model, gradient_bounds, trainable_vars, gradients):\n    \"\"\"Add noise to gradients.\n\n    Remark: a single global noise is added to all gradients, based on the global sensitivity.\n            This is the default behaviour of the original DPGD algorithm.\n            This may yield looser privacy bounds than local noisify.\n\n    Args:\n        model: model to train.\n        gradient_bounds: dictionary of gradient norm upper bounds. The keys are the names of the trainable variables.\n        trainable_vars: list of trainable variables. The list is in the same order as gradients.\n        gradients: list of gradients to add noise to. The list is in the same order as trainable_vars.\n\n    Returns:\n        noisy_grads: list of noisy gradients. The list is in the same order as trainable_vars.\n    \"\"\"\n    global_sensitivity = tf.math.sqrt(\n        tf.math.reduce_sum([bound**2 for bound in gradient_bounds.values()])\n    )\n    # no factor-2 : use add_remove definition of DP.\n    stddev = model.dp_parameters.noise_multiplier * global_sensitivity\n    noises = [tf.random.normal(shape=tf.shape(g), stddev=stddev) for g in gradients]\n    if model.debug:\n        for grad, var in zip(gradients, trainable_vars):\n            upperboundgrad = (\n                gradient_bounds[var.name] * model.dataset_metadata.batch_size\n            )\n            noise_msg = (\n                f\"Adding noise of stddev : {stddev}\"\n                f\" to variable {var.name}\"\n                f\" of gradient norm upper bound {upperboundgrad}\"\n                f\" and effective norm {tf.norm(grad)}\"\n            )\n            print(noise_msg)\n    noisy_grads = [g + n for g, n in zip(gradients, noises)]\n    return noisy_grads\n</code></pre>"},{"location":"api/model/#deel.lipdp.model.local_noisify","title":"<code>local_noisify(model, gradient_bounds, trainable_vars, gradients)</code>","text":"<p>Add noise to gradients of trainable variables.</p> <p>Remark: this yields tighter bounds than global_noisify.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>model to train.</p> required <code>gradient_bounds</code> <p>dictionary of gradient norm upper bounds. Keys are trainable variables names.</p> required <code>trainable_vars</code> <p>list of trainable variables. Same order as gradients.</p> required <code>gradients</code> <p>list of gradients. Same order as trainable_vars.</p> required <p>Returns:</p> Type Description <p>list of noisy gradients. Same order as trainable_vars.</p> Source code in <code>deel/lipdp/model.py</code> <pre><code>def local_noisify(model, gradient_bounds, trainable_vars, gradients):\n    \"\"\"Add noise to gradients of trainable variables.\n\n    Remark: this yields tighter bounds than global_noisify.\n\n    Args:\n        model: model to train.\n        gradient_bounds: dictionary of gradient norm upper bounds. Keys are trainable variables names.\n        trainable_vars: list of trainable variables. Same order as gradients.\n        gradients: list of gradients. Same order as trainable_vars.\n\n    Returns:\n        list of noisy gradients. Same order as trainable_vars.\n    \"\"\"\n    nm_coefs = get_noise_multiplier_coefs(model)\n    noises = []\n    for grad, var in zip(gradients, trainable_vars):\n        if var.name in gradient_bounds.keys():\n            # no factor-2 : use add_remove definition of DP\n            stddev = (\n                model.dp_parameters.noise_multiplier\n                * gradient_bounds[var.name]\n                * nm_coefs[var.name]\n            )\n            noises.append(tf.random.normal(shape=tf.shape(grad), stddev=stddev))\n            if model.debug:\n                upperboundgrad = (\n                    gradient_bounds[var.name] * model.dataset_metadata.batch_size\n                )\n                noise_msg = (\n                    f\"Adding noise of stddev : {stddev}\"\n                    f\" to variable {var.name}\"\n                    f\" of gradient norm upper bound {upperboundgrad}\"\n                    f\" and effective norm {tf.norm(grad)}\"\n                )\n                print(noise_msg)\n        else:\n            raise ValueError(f\"Variable {var.name} not in gradient bounds.\")\n\n    noisy_grads = [g + n for g, n in zip(gradients, noises)]\n    return noisy_grads\n</code></pre>"},{"location":"api/pipeline/","title":"deel.lipdp.pipeline module","text":""},{"location":"api/pipeline/#deel.lipdp.pipeline.AugmultConfig","title":"<code>AugmultConfig</code>  <code>dataclass</code>","text":"<p>Preprocessing options for images at training time.</p> <p>Copied from https://github.com/google-deepmind/jax_privacy that was released under license Apache-2.0.</p> <p>Attributes:</p> Name Type Description <code>augmult</code> <code>int</code> <p>Number of augmentation multiplicities to use. <code>augmult=0</code> corresponds to no augmentation at all, <code>augmult=1</code> to standard data augmentation (one augmented view per mini-batch) and <code>augmult&gt;1</code> to having several augmented view of each sample within the mini-batch.</p> <code>random_crop</code> <code>bool</code> <p>Whether to use random crops for data augmentation.</p> <code>random_flip</code> <code>bool</code> <p>Whether to use random horizontal flips for data augmentation.</p> <code>random_color</code> <code>bool</code> <p>Whether to use random color jittering for data augmentation.</p> <code>pad</code> <code>Union[int, None]</code> <p>Optional padding before the image is cropped for data augmentation.</p> Source code in <code>deel/lipdp/pipeline.py</code> <pre><code>@dataclass\nclass AugmultConfig:\n    \"\"\"Preprocessing options for images at training time.\n\n    Copied from https://github.com/google-deepmind/jax_privacy that was released\n    under license Apache-2.0.\n\n    Attributes:\n      augmult: Number of augmentation multiplicities to use. `augmult=0`\n        corresponds to no augmentation at all, `augmult=1` to standard data\n        augmentation (one augmented view per mini-batch) and `augmult&gt;1` to\n        having several augmented view of each sample within the mini-batch.\n      random_crop: Whether to use random crops for data augmentation.\n      random_flip: Whether to use random horizontal flips for data augmentation.\n      random_color: Whether to use random color jittering for data augmentation.\n      pad: Optional padding before the image is cropped for data augmentation.\n    \"\"\"\n\n    augmult: int\n    random_crop: bool\n    random_flip: bool\n    random_color: bool\n    pad: Union[int, None] = 4\n\n    def apply(\n        self,\n        image: tf.Tensor,\n        label: tf.Tensor,\n        *,\n        crop_size: Sequence[int],\n    ) -&gt; tuple[tf.Tensor, tf.Tensor]:\n        return apply_augmult(\n            image,\n            label,\n            augmult=self.augmult,\n            random_flip=self.random_flip,\n            random_crop=self.random_crop,\n            random_color=self.random_color,\n            pad=self.pad,\n            crop_size=crop_size,\n        )\n</code></pre>"},{"location":"api/pipeline/#deel.lipdp.pipeline.DatasetMetadata","title":"<code>DatasetMetadata</code>  <code>dataclass</code>","text":"<p>class that handle dataset metadata that will be used to compute privacy guarantees</p> Source code in <code>deel/lipdp/pipeline.py</code> <pre><code>@dataclass\nclass DatasetMetadata:\n    \"\"\"\n    class that handle dataset metadata that will be used\n    to compute privacy guarantees\n    \"\"\"\n\n    input_shape: Tuple[int, int, int]\n    nb_classes: int\n    nb_samples_train: int\n    nb_samples_test: int\n    class_names: List[str]\n    nb_steps_per_epochs: int\n    batch_size: int\n    max_norm: float\n</code></pre>"},{"location":"api/pipeline/#deel.lipdp.pipeline.apply_augmult","title":"<code>apply_augmult(image, label, *, augmult, random_flip, random_crop, random_color, crop_size, pad)</code>","text":"<p>Augmult data augmentation (Hoffer et al., 2019; Fort et al., 2021).</p> <p>Copied from https://github.com/google-deepmind/jax_privacy that was released under license Apache-2.0.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>(single) image to augment.</p> required <code>label</code> <code>Tensor</code> <p>label corresponding to the image (not modified by this function).</p> required <code>augmult</code> <code>int</code> <p>number of augmentation multiplicities to use. This number should be non-negative (this function will fail if it is not).</p> required <code>random_flip</code> <code>bool</code> <p>whether to use random horizontal flips for data augmentation.</p> required <code>random_crop</code> <code>bool</code> <p>whether to use random crops for data augmentation.</p> required <code>random_color</code> <code>bool</code> <p>whether to use random color jittering for data augmentation.</p> required <code>crop_size</code> <code>Sequence[int]</code> <p>size of the crop for random crops.</p> required <code>pad</code> <code>Union[int, None]</code> <p>optional padding before the image is cropped.</p> required <p>Returns:   images: augmented images with a new prepended dimension of size <code>augmult</code>.   labels: repeated labels with a new prepended dimension of size <code>augmult</code>.</p> Source code in <code>deel/lipdp/pipeline.py</code> <pre><code>def apply_augmult(\n    image: tf.Tensor,\n    label: tf.Tensor,\n    *,\n    augmult: int,\n    random_flip: bool,\n    random_crop: bool,\n    random_color: bool,\n    crop_size: Sequence[int],\n    pad: Union[int, None],\n) -&gt; tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Augmult data augmentation (Hoffer et al., 2019; Fort et al., 2021).\n\n    Copied from https://github.com/google-deepmind/jax_privacy that was released\n    under license Apache-2.0.\n\n    Args:\n      image: (single) image to augment.\n      label: label corresponding to the image (not modified by this function).\n      augmult: number of augmentation multiplicities to use. This number\n        should be non-negative (this function will fail if it is not).\n      random_flip: whether to use random horizontal flips for data augmentation.\n      random_crop: whether to use random crops for data augmentation.\n      random_color: whether to use random color jittering for data augmentation.\n      crop_size: size of the crop for random crops.\n      pad: optional padding before the image is cropped.\n    Returns:\n      images: augmented images with a new prepended dimension of size `augmult`.\n      labels: repeated labels with a new prepended dimension of size `augmult`.\n    \"\"\"\n    if augmult == 0:\n        # No augmentations; return original images and labels with a new dimension.\n        images = tf.expand_dims(image, axis=0)\n        labels = tf.expand_dims(label, axis=0)\n    elif augmult &gt; 0:\n        # Perform one or more augmentations.\n        raw_image = tf.identity(image)\n        augmented_images = []\n\n        for _ in range(augmult):\n            image_now = raw_image\n\n            if random_crop:\n                if pad:\n                    image_now = padding_input(image_now, pad=pad)\n                image_now = tf.image.random_crop(image_now, size=crop_size)\n            if random_flip:\n                image_now = tf.image.random_flip_left_right(image_now)\n            if random_color:\n                # values copied/adjusted from a color jittering tutorial\n                # https://www.wouterbulten.nl/blog/tech/data-augmentation-using-tensorflow-data-dataset/\n                image_now = tf.image.random_hue(image_now, 0.1)\n                image_now = tf.image.random_saturation(image_now, 0.6, 1.6)\n                image_now = tf.image.random_brightness(image_now, 0.15)\n                image_now = tf.image.random_contrast(image_now, 0.7, 1.3)\n\n            augmented_images.append(image_now)\n\n        images = tf.stack(augmented_images, axis=0)\n        labels = tf.stack([label] * augmult, axis=0)\n    else:\n        raise ValueError(\"Augmult should be non-negative.\")\n\n    return images, labels\n</code></pre>"},{"location":"api/pipeline/#deel.lipdp.pipeline.default_delta_value","title":"<code>default_delta_value(dataset_metadata)</code>","text":"<p>Default policy to set delta value.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_metadata</code> <code>DatasetMetadata</code> <p>metadata of the dataset.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>default delta value.</p> Source code in <code>deel/lipdp/pipeline.py</code> <pre><code>def default_delta_value(dataset_metadata) -&gt; float:\n    \"\"\"Default policy to set delta value.\n\n    Args:\n        dataset_metadata (DatasetMetadata): metadata of the dataset.\n\n    Returns:\n        float: default delta value.\n    \"\"\"\n    n = dataset_metadata.nb_samples_train\n    smallest_power10_bigger = 10 ** np.ceil(np.log10(n))\n    delta = float(1 / smallest_power10_bigger)\n    print(f\"Default delta value: {delta}\")\n    return delta\n</code></pre>"},{"location":"api/pipeline/#deel.lipdp.pipeline.load_adbench_data","title":"<code>load_adbench_data(dataset_name, dataset_dir, standardize=True, redownload=False)</code>","text":"<p>Load a dataset from the adbench package.</p> Source code in <code>deel/lipdp/pipeline.py</code> <pre><code>def load_adbench_data(\n    dataset_name: str,\n    dataset_dir: str,\n    standardize: bool = True,\n    redownload: bool = False,\n):\n    \"\"\"Load a dataset from the adbench package.\"\"\"\n    if redownload:\n        download_adbench_datasets(dataset_dir)\n\n    data = np.load(\n        f\"{dataset_dir}/datasets/Classical/{dataset_name}.npz\", allow_pickle=True\n    )\n    x_data, y_data = data[\"X\"], data[\"y\"]\n\n    if standardize:\n        x_data = (x_data - x_data.mean()) / x_data.std()\n\n    return x_data, y_data\n</code></pre>"},{"location":"api/pipeline/#deel.lipdp.pipeline.load_and_prepare_images_data","title":"<code>load_and_prepare_images_data(dataset_name='mnist', batch_size=256, colorspace='RGB', bound_fct=None, drop_remainder=True, multiplicity=0)</code>","text":"<p>Load dataset_name image dataset using tensorflow datasets.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>name of the dataset to load.</p> <code>'mnist'</code> <code>batch_size</code> <code>int</code> <p>batch size</p> <code>256</code> <code>colorspace</code> <code>str</code> <p>one of RGB, HSV, YIQ, YUV</p> <code>'RGB'</code> <code>drop_remainder</code> <code>bool</code> <p>when true drop the last batch if it has less than batch_size elements. Defaults to True.</p> <code>True</code> <code>multiplicity</code> <code>int</code> <p>multiplicity of data-augmentation. 0 means no augmentation, 1 means standard augmentation, &gt;1 means multiple.</p> <code>0</code> <code>bound_fct</code> <code>callable</code> <p>function that is responsible of bounding the inputs. Can be None, bound_normalize or bound_clip_value. None means that no clipping is performed, and max theoretical value is reported ( sqrt(whc) ). bound_normalize means that each input is normalized setting the bound to 1. bound_clip_value will clip norm to defined value.</p> <code>None</code> <p>Returns:</p> Type Description <p>ds_train, ds_test, metadata: two dataset, with data preparation, augmentation, shuffling and batching. Also return an DatasetMetadata object with infos about the dataset.</p> Source code in <code>deel/lipdp/pipeline.py</code> <pre><code>def load_and_prepare_images_data(\n    dataset_name: str = \"mnist\",\n    batch_size: int = 256,\n    colorspace: str = \"RGB\",\n    bound_fct: bool = None,\n    drop_remainder: bool = True,\n    multiplicity: int = 0,\n):\n    \"\"\"\n    Load dataset_name image dataset using tensorflow datasets.\n\n    Args:\n        dataset_name (str): name of the dataset to load.\n        batch_size (int): batch size\n        colorspace (str): one of RGB, HSV, YIQ, YUV\n        drop_remainder (bool, optional): when true drop the last batch if it\n            has less than batch_size elements. Defaults to True.\n        multiplicity (int): multiplicity of data-augmentation. 0 means no\n            augmentation, 1 means standard augmentation, &gt;1 means multiple.\n        bound_fct (callable, optional): function that is responsible of\n            bounding the inputs. Can be None, bound_normalize or bound_clip_value.\n            None means that no clipping is performed, and max theoretical value is\n            reported ( sqrt(w*h*c) ). bound_normalize means that each input is\n            normalized setting the bound to 1. bound_clip_value will clip norm to\n            defined value.\n\n    Returns:\n        ds_train, ds_test, metadata: two dataset, with data preparation,\n            augmentation, shuffling and batching. Also return an\n            DatasetMetadata object with infos about the dataset.\n    \"\"\"\n    # load data\n    (ds_train, ds_test), ds_info = tfds.load(\n        dataset_name,\n        split=[\"train\", \"test\"],\n        shuffle_files=True,\n        as_supervised=True,\n        with_info=True,\n    )\n\n    # None bound yield default trivial bound\n    nb_classes = ds_info.features[\"label\"].num_classes\n    input_shape = ds_info.features[\"image\"].shape\n    if bound_fct is None:\n        # TODO: consider throwing an error here to avoid unexpected behavior.\n        print(\n            \"No bound function provided, using default bound sqrt(w*h*c) for the input.\"\n        )\n        bound_fct = (\n            lambda x, y: (x, y),\n            float(input_shape[-3] * input_shape[-2] * input_shape[-1]),\n        )\n    bound_callable, bound_val = bound_fct\n\n    to_float = lambda x, y: (tf.cast(x, tf.float32) / 255.0, tf.one_hot(y, nb_classes))\n\n    if input_shape[-1] == 1:\n        assert (\n            colorspace == \"grayscale\"\n        ), \"grayscale is the only valid colorspace for grayscale images\"\n        colorspace = None\n    color_space_fun = get_colorspace_function(colorspace)\n\n    ############################\n    ####### Train pipeline #####\n    ############################\n\n    # train pipeline\n    ds_train = ds_train.map(  # map to 0,1 and one hot encode\n        to_float,\n        num_parallel_calls=tf.data.AUTOTUNE,\n    )\n    ds_train = ds_train.shuffle(  # shuffle\n        min(batch_size * 10, max(batch_size, ds_train.cardinality())),\n        reshuffle_each_iteration=True,\n    )\n\n    if multiplicity &gt;= 1:\n        augmult_config = default_augmult_config(multiplicity)\n        crop_size = ds_info.features[\"image\"].shape\n        ds_train = ds_train.map(\n            lambda x, y: augmult_config.apply(x, y, crop_size=crop_size)\n        )\n        ds_train = ds_train.unbatch()\n    else:\n        multiplicity = 1\n\n    ds_train = ds_train.map(  # map colorspace\n        color_space_fun,\n        num_parallel_calls=tf.data.AUTOTUNE,\n    )\n    ds_train = ds_train.map(\n        bound_callable, num_parallel_calls=tf.data.AUTOTUNE\n    )  # apply bound\n    ds_train = ds_train.batch(\n        batch_size * multiplicity, drop_remainder=drop_remainder\n    )  # batch\n    ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n\n    ############################\n    ####### Test pipeline ######\n    ############################\n\n    ds_test = (\n        ds_test.map(\n            to_float,\n            num_parallel_calls=tf.data.AUTOTUNE,\n        )\n        .map(\n            color_space_fun,\n            num_parallel_calls=tf.data.AUTOTUNE,\n        )\n        .map(bound_callable, num_parallel_calls=tf.data.AUTOTUNE)  # apply bound\n        .shuffle(\n            min(batch_size * 10, max(batch_size, ds_test.cardinality())),\n            reshuffle_each_iteration=True,\n        )\n        .batch(batch_size, drop_remainder=False)\n        .prefetch(tf.data.AUTOTUNE)\n    )\n    # get dataset metadata\n    metadata = DatasetMetadata(\n        input_shape=ds_info.features[\"image\"].shape,\n        nb_classes=ds_info.features[\"label\"].num_classes,\n        nb_samples_train=ds_info.splits[\"train\"].num_examples,\n        nb_samples_test=ds_info.splits[\"test\"].num_examples,\n        class_names=ds_info.features[\"label\"].names,\n        nb_steps_per_epochs=ds_train.cardinality().numpy()\n        if ds_train.cardinality() &gt; 0  # handle case cardinality return -1 (unknown)\n        else ds_info.splits[\"train\"].num_examples / batch_size,\n        batch_size=batch_size,\n        max_norm=bound_val,\n    )\n\n    return ds_train, ds_test, metadata\n</code></pre>"},{"location":"api/pipeline/#deel.lipdp.pipeline.padding_input","title":"<code>padding_input(x, pad)</code>","text":"<p>Pad input image through 'mirroring' on the four edges.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>image to pad.</p> required <code>pad</code> <code>int</code> <p>number of padding pixels.</p> required <p>Returns:   Padded image.</p> Source code in <code>deel/lipdp/pipeline.py</code> <pre><code>def padding_input(x: tf.Tensor, pad: int):\n    \"\"\"Pad input image through 'mirroring' on the four edges.\n\n    Args:\n      x: image to pad.\n      pad: number of padding pixels.\n    Returns:\n      Padded image.\n    \"\"\"\n    x = tf.concat([x[:pad, :, :][::-1], x, x[-pad:, :, :][::-1]], axis=0)\n    x = tf.concat([x[:, :pad, :][:, ::-1], x, x[:, -pad:, :][:, ::-1]], axis=1)\n    return x\n</code></pre>"},{"location":"api/pipeline/#deel.lipdp.pipeline.prepare_tabular_data","title":"<code>prepare_tabular_data(x_train, x_test, y_train, y_test, batch_size, bound_fct=None, drop_remainder=True)</code>","text":"<p>Convert Numpy dataset into tensorflow datasets.</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>array</code> <p>input data, of shape (N, F) with floats.</p> required <code>x_test</code> <code>array</code> <p>input data, of shape (N, F) with floats.</p> required <code>y_train</code> <code>array</code> <p>labels in one hot encoding, of shape (N, C) with floats.</p> required <code>y_test</code> <code>array</code> <p>labels in one hot encoding, of shape (N, C) with floats.</p> required <code>batch_size</code> <code>int</code> <p>logical batch size</p> required <code>bound_fct</code> <code>callable</code> <p>function that is responsible of bounding the inputs. Can be None, bound_normalize or bound_clip_value. None means that no clipping is performed, and max theoretical value is reported ( sqrt(whc) ). bound_normalize means that each input is normalized setting the bound to 1. bound_clip_value will clip norm to defined value.</p> <code>None</code> <code>drop_remainder</code> <code>bool</code> <p>when true drop the last batch if it has less than batch_size elements. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <p>ds_train, ds_test, metadata: two dataset, with data preparation, augmentation, shuffling and batching. Also return an DatasetMetadata object with infos about the dataset.</p> Source code in <code>deel/lipdp/pipeline.py</code> <pre><code>def prepare_tabular_data(\n    x_train: np.array,\n    x_test: np.array,\n    y_train: np.array,\n    y_test: np.array,\n    batch_size: int,\n    bound_fct: Callable = None,\n    drop_remainder: bool = True,\n):\n    \"\"\"Convert Numpy dataset into tensorflow datasets.\n\n    Args:\n        x_train (np.array): input data, of shape (N, F) with floats.\n        x_test (np.array): input data, of shape (N, F) with floats.\n        y_train (np.array): labels in one hot encoding, of shape (N, C) with floats.\n        y_test (np.array): labels in one hot encoding, of shape (N, C) with floats.\n        batch_size (int): logical batch size\n        bound_fct (callable, optional): function that is responsible of\n            bounding the inputs. Can be None, bound_normalize or bound_clip_value.\n            None means that no clipping is performed, and max theoretical value is\n            reported ( sqrt(w*h*c) ). bound_normalize means that each input is\n            normalized setting the bound to 1. bound_clip_value will clip norm to\n            defined value.\n        drop_remainder (bool, optional): when true drop the last batch if it\n            has less than batch_size elements. Defaults to True.\n\n\n    Returns:\n        ds_train, ds_test, metadata: two dataset, with data preparation,\n            augmentation, shuffling and batching. Also return an\n            DatasetMetadata object with infos about the dataset.\n    \"\"\"\n    # None bound yield default trivial bound\n    nb_classes = np.unique(y_train).shape[0]\n    input_shape = x_train.shape[1:]\n    bound_callable, bound_val = bound_fct\n\n    ############################\n    ####### Train pipeline #####\n    ############################\n\n    to_float = lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.float32))\n\n    ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    ds_train = ds_train.map(to_float, num_parallel_calls=tf.data.AUTOTUNE)\n    ds_train = ds_train.shuffle(  # shuffle\n        min(batch_size * 10, max(batch_size, ds_train.cardinality())),\n        reshuffle_each_iteration=True,\n    )\n\n    ds_train = ds_train.map(\n        bound_callable, num_parallel_calls=tf.data.AUTOTUNE\n    )  # apply bound\n    ds_train = ds_train.batch(batch_size, drop_remainder=drop_remainder)  # batch\n    ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n\n    ############################\n    ####### Test pipeline ######\n    ############################\n\n    ds_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n    ds_test = ds_test.map(to_float, num_parallel_calls=tf.data.AUTOTUNE)\n    ds_test = (\n        ds_test.map(bound_callable, num_parallel_calls=tf.data.AUTOTUNE)  # apply bound\n        .shuffle(\n            min(batch_size * 10, max(batch_size, ds_test.cardinality())),\n            reshuffle_each_iteration=True,\n        )\n        .batch(batch_size, drop_remainder=False)\n        .prefetch(tf.data.AUTOTUNE)\n    )\n    # get dataset metadata\n    metadata = DatasetMetadata(\n        input_shape=input_shape,\n        nb_classes=nb_classes,\n        nb_samples_train=x_train.shape[0],\n        nb_samples_test=x_test.shape[0],\n        class_names=[str(i) for i in range(nb_classes)],\n        nb_steps_per_epochs=ds_train.cardinality().numpy()\n        if ds_train.cardinality() &gt; 0  # handle case cardinality return -1 (unknown)\n        else x_train.shape[0] / batch_size,\n        batch_size=batch_size,\n        max_norm=bound_val,\n    )\n\n    return ds_train, ds_test, metadata\n</code></pre>"},{"location":"api/pipeline/#deel.lipdp.pipeline.standardize_CIFAR","title":"<code>standardize_CIFAR(image)</code>","text":"<p>Standardize the image with the CIFAR10 mean and std dev.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>image to standardize of shape (H,W,C) of type tf.float32.</p> required Source code in <code>deel/lipdp/pipeline.py</code> <pre><code>def standardize_CIFAR(image: tf.Tensor):\n    \"\"\"Standardize the image with the CIFAR10 mean and std dev.\n\n    Args:\n        image (tf.Tensor): image to standardize of shape (H,W,C) of type tf.float32.\n    \"\"\"\n    CIFAR10_MEAN = tf.constant([[[0.4914, 0.4822, 0.4465]]], dtype=tf.float32)\n    CIFAR10_STD_DEV = tf.constant([[[0.2023, 0.1994, 0.2010]]], dtype=tf.float32)\n    return (image - CIFAR10_MEAN) / CIFAR10_STD_DEV\n</code></pre>"},{"location":"api/sensitivity/","title":"deel.lipdp.sensitivity module","text":""},{"location":"api/sensitivity/#deel.lipdp.sensitivity.get_max_epochs","title":"<code>get_max_epochs(epsilon_max, model, epochs_max=1024, safe=True, atol=0.01)</code>","text":"<p>Return the maximum number of epochs to reach a given epsilon_max value.</p> <p>The computation of (epsilon, delta) is slow since it involves solving a minimization problem (mandatory to go from RDP accountant to DP results). Hence each call takes typically around 1s. This function is used to avoid calling get_eps_delta too many times be leveraging the fact that epsilon is a non-decreasing function of the number of epochs: we unlocks the dichotomy search.</p> <p>Hence the number of calls is typically log2(epochs_max) + 1. The maximum of epochs is set to 1024 by default to avoid too long computation times, even in high privacy regimes.</p> <p>Parameters:</p> Name Type Description Default <code>epsilon_max</code> <p>The maximum value of epsilon we want to reach.</p> required <code>model</code> <p>The model used to compute the values of epsilon.</p> required <code>epochs_max</code> <p>The maximum number of epochs to reach epsilon_max. Defaults to 1024.         If None, the dichotomy search is used to find the upper bound.</p> <code>1024</code> <code>safe</code> <p>If True, the dichotomy search returns the largest number of epochs such that epsilon &lt;= epsilon_max.   Otherwise, it returns the smallest number of epochs such that epsilon &gt;= epsilon_max.</p> <code>True</code> <code>atol</code> <p>The absolute tolerance to panic on numerical inaccuracy. Defaults to 1e-2.</p> <code>0.01</code> <p>Returns:</p> Type Description <p>The maximum number of epochs to reach epsilon_max. It may be zero if epsilon_max is too small.</p> Source code in <code>deel/lipdp/sensitivity.py</code> <pre><code>def get_max_epochs(epsilon_max, model, epochs_max=1024, safe=True, atol=1e-2):\n    \"\"\"Return the maximum number of epochs to reach a given epsilon_max value.\n\n    The computation of (epsilon, delta) is slow since it involves solving a minimization problem\n    (mandatory to go from RDP accountant to DP results). Hence each call takes typically around 1s.\n    This function is used to avoid calling get_eps_delta too many times be leveraging the fact that\n    epsilon is a non-decreasing function of the number of epochs: we unlocks the dichotomy search.\n\n    Hence the number of calls is typically log2(epochs_max) + 1.\n    The maximum of epochs is set to 1024 by default to avoid too long computation times, even in high\n    privacy regimes.\n\n    Args:\n        epsilon_max: The maximum value of epsilon we want to reach.\n        model: The model used to compute the values of epsilon.\n        epochs_max: The maximum number of epochs to reach epsilon_max. Defaults to 1024.\n                    If None, the dichotomy search is used to find the upper bound.\n        safe: If True, the dichotomy search returns the largest number of epochs such that epsilon &lt;= epsilon_max.\n              Otherwise, it returns the smallest number of epochs such that epsilon &gt;= epsilon_max.\n        atol: The absolute tolerance to panic on numerical inaccuracy. Defaults to 1e-2.\n\n    Returns:\n        The maximum number of epochs to reach epsilon_max. It may be zero if epsilon_max is too small.\n    \"\"\"\n    steps_per_epoch = model.dataset_metadata.nb_steps_per_epochs\n\n    def fun(epoch):\n        if epoch == 0:\n            epsilon = 0\n        else:\n            epsilon, _ = get_eps_delta(model, epoch)\n        return epsilon\n\n    # dichotomy search on the number of epochs.\n    if epochs_max is None:\n        epochs_max = 512\n        epsilon = 0\n        while epsilon &lt; epsilon_max:\n            epochs_max *= 2\n            epsilon = fun(epochs_max)\n            print(f\"epochs_max = {epochs_max} at epsilon = {epsilon}\")\n\n    epochs_min = 0\n\n    while epochs_max - epochs_min &gt; 1:\n        epoch = (epochs_max + epochs_min) // 2\n        epsilon = fun(epoch)\n        if epsilon &lt; epsilon_max:\n            epochs_min = epoch\n        else:\n            epochs_max = epoch\n        print(\n            f\"epoch bounds = {epochs_min, epochs_max} and epsilon = {epsilon} at epoch {epoch}\"\n        )\n\n    if safe:\n        last_epsilon = fun(epochs_min)\n        error = last_epsilon - epsilon_max\n        if error &lt;= 0:\n            return epochs_min\n        elif error &lt; atol:\n            # This branch should never be taken if fun is a non-decreasing function of the number of epochs.\n            # fun is mathematcally non-decreasing, but numerical inaccuracy can lead to this case.\n            print(f\"Numerical inaccuracy with error {error:.7f} in the dichotomy search: using a conservative value.\")\n            return epochs_min - 1\n        else:\n            assert False, f\"Numerical inaccuracy with error {error:.7f}&gt;{atol:.3f} in the dichotomy search.\"\n\n    return epochs_max\n</code></pre>"},{"location":"api/sensitivity/#deel.lipdp.sensitivity.gradient_norm_check","title":"<code>gradient_norm_check(upper_bounds, model, examples)</code>","text":"<p>Verifies that the values of per-sample gradients on a layer never exceede a value determined by the theoretical work.</p> Args <p>upper_bounds: maximum gradient bounds for each layer (dictionnary of 'layers name ': 'bounds' pairs). model: The model containing the layers we are interested in. Layers must only have one trainable variable. examples: a batch of examples to test on.  </p> <p>Returns :     Boolean value. True corresponds to upper bound has been validated.</p> Source code in <code>deel/lipdp/sensitivity.py</code> <pre><code>def gradient_norm_check(upper_bounds, model, examples):\n    \"\"\"Verifies that the values of per-sample gradients on a layer never exceede a value\n    determined by the theoretical work.\n\n    Args :\n        upper_bounds: maximum gradient bounds for each layer (dictionnary of 'layers name ': 'bounds' pairs).\n        model: The model containing the layers we are interested in. Layers must only have one trainable variable.\n        examples: a batch of examples to test on.  \n    Returns :\n        Boolean value. True corresponds to upper bound has been validated.\n    \"\"\"\n    activations = examples\n    var_seen = set()\n    for layer in model.layers:\n        post_activations = layer(activations, training=True)\n        assert len(layer.trainable_variables) &lt; 2\n        if len(layer.trainable_variables) == 1:\n            assert len(layer.trainable_variables) == 1\n            train_var = layer.trainable_variables[0]\n            var_name = layer.trainable_variables[0].name\n            var_seen.add(var_name)\n            bound = upper_bounds[var_name]\n            check_layer_gradient_norm(bound, layer, activations)\n        activations = post_activations\n    for var_name in upper_bounds:\n        assert var_name in var_seen\n</code></pre>"},{"location":"notebooks/advanced_cifar10/","title":"Demo 2: advanced use on CIFAR10","text":"<p>The library is based on tensorflow.</p> <pre><code>import tensorflow as tf\n</code></pre> <pre><code>from deel.lipdp import losses\nfrom deel.lipdp.model import DP_Model\nfrom deel.lipdp.model import DPParameters\n</code></pre> <p>The <code>DP_Accountant</code> callback keeps track of \\((\\epsilon,\\delta)\\)-DP values epoch after epoch. In practice we may be interested in reaching the maximum val_accuracy under privacy constraint \\(\\epsilon\\): the convenience function <code>get_max_epochs</code> exactly does that by performing a dichotomy search over the number of epochs.</p> <pre><code>from deel.lipdp.model import DP_Accountant\nfrom deel.lipdp.sensitivity import get_max_epochs\n</code></pre> <p>The framework requires a control of the maximum norm of inputs. This can be ensured with input clipping for example: <code>bound_clip_value</code>.</p> <pre><code>from deel.lipdp.pipeline import bound_clip_value\nfrom deel.lipdp.pipeline import load_and_prepare_data\n</code></pre> <pre><code>import warnings\nwarnings.filterwarnings(\"ignore\")\n\ndp_parameters = DPParameters(\n    noisify_strategy=\"global\",\n    noise_multiplier=4.0,\n    delta=1e-5,\n)\n\nepsilon_max = 10.0\n</code></pre> <p>With many parameters, it can be interesting to use <code>local</code> strategy over <code>global</code>, since the effective noise growths as \\(\\mathcal{O}(\\sqrt{(D)})\\) in <code>global</code> strategy. Since the privacy leakge is more important is <code>local</code> strategy, we compensate with high <code>noise_multiplier</code>.</p> <p></p> <pre><code>def augmentation_fct(image, label):\n    image = tf.image.random_flip_left_right(image)\n    return image, label\n\ninput_upper_bound = 30.0\nds_train, ds_test, dataset_metadata = load_and_prepare_data(\n    \"cifar10\",\n    colorspace=\"HSV\",\n    batch_size=10_000,\n    drop_remainder=True,  # accounting assumes fixed batch size\n    augmentation_fct=augmentation_fct,\n    bound_fct=bound_clip_value(  # other strategies are possible, like normalization.\n        input_upper_bound\n    ),  # clipping preprocessing allows to control input bound\n)\n</code></pre> <pre>\n<code>2023-05-24 17:27:24.335576: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-24 17:27:24.905888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 47066 MB memory:  -&gt; device: 0, name: Quadro RTX 8000, pci bus id: 0000:03:00.0, compute capability: 7.5\n</code>\n</pre> <pre><code>from deel.lipdp.layers import DP_AddBias\nfrom deel.lipdp.layers import DP_BoundedInput\nfrom deel.lipdp.layers import DP_ClipGradient\nfrom deel.lipdp.layers import DP_Flatten\nfrom deel.lipdp.layers import DP_GroupSort\nfrom deel.lipdp.layers import DP_Lambda\nfrom deel.lipdp.layers import DP_LayerCentering\nfrom deel.lipdp.layers import DP_Permute\nfrom deel.lipdp.layers import DP_QuickSpectralDense\nfrom deel.lipdp.layers import DP_Reshape\nfrom deel.lipdp.layers import DP_ScaledGlobalL2NormPooling2D\nfrom deel.lipdp.layers import DP_ScaledL2NormPooling2D\nfrom deel.lipdp.layers import DP_QuickSpectralConv2D\n</code></pre> <p>The MLP Mixer uses residual connections. Residuals connections are handled with the utility function <code>make_residuals</code> that wraps the layers inside a block that handles bounds propagation.</p> <p></p> <pre><code>from deel.lipdp.layers import make_residuals\n</code></pre> <p>Now, we proceed with the creation of the environnement.</p> <pre><code>skip_connections = False  # use skip connections, like in original MLP Mixer architecture.\nclip_loss_gradient = 2**0.5  # elementwise gradient is clipped to value sqrt(2) - which is the maximum for CCE loss.\nadd_biases = False  # Add biases after linear transformations.\nbiases_norm_max = 0.05\nhidden_size = 64\nmlp_seq_dim = 64\nmlp_channel_dim = 128\nnum_mixer_layers = 2  # Two MLP Mixer blocks.\nlayer_centering = False  # Centering operation (like LayerNormalization without the reducing operation). Linear 1-Lipschitz.\npatch_size = 4  # Number of pixels in each patch.\n\ndef create_MLP_Mixer(dp_parameters, dataset_metadata, upper_bound):\n    input_shape = (32, 32, 3)\n    layers = [DP_BoundedInput(input_shape=input_shape, upper_bound=upper_bound)]\n\n    layers.append(\n        DP_Lambda(\n            tf.image.extract_patches,\n            arguments=dict(\n                sizes=[1, patch_size, patch_size, 1],\n                strides=[1, patch_size, patch_size, 1],\n                rates=[1, 1, 1, 1],\n                padding=\"VALID\",\n            ),\n        )\n    )\n\n    seq_len = (input_shape[0] // patch_size) * (input_shape[1] // patch_size)\n\n    layers.append(DP_Reshape((seq_len, (patch_size ** 2) * input_shape[-1])))\n    layers.append(\n        DP_QuickSpectralDense(\n            units=hidden_size, use_bias=False, kernel_initializer=\"identity\"\n        )\n    )\n\n    for _ in range(num_mixer_layers):\n        to_add = [\n            DP_Permute((2, 1)),\n            DP_QuickSpectralDense(\n                units=mlp_seq_dim, use_bias=False, kernel_initializer=\"identity\"\n            ),\n        ]\n        if add_biases:\n            to_add.append(DP_AddBias(biases_norm_max))\n        to_add.append(DP_GroupSort(2))\n        if layer_centering:\n            to_add.append(DP_LayerCentering())\n        to_add += [\n            DP_QuickSpectralDense(\n                units=seq_len, use_bias=False, kernel_initializer=\"identity\"\n            ),\n            DP_Permute((2, 1)),\n        ]\n\n        if skip_connections:\n            layers += make_residuals(\"1-lip-add\", to_add)\n        else:\n            layers += to_add\n\n        to_add = [\n            DP_QuickSpectralDense(\n                units=mlp_channel_dim, use_bias=False, kernel_initializer=\"identity\"\n            ),\n        ]\n        if add_biases:\n            to_add.append(DP_AddBias(biases_norm_max))\n        to_add.append(DP_GroupSort(2))\n        if layer_centering:\n            to_add.append(DP_LayerCentering())\n        to_add.append(\n            DP_QuickSpectralDense(\n                units=hidden_size, use_bias=False, kernel_initializer=\"identity\"\n            )\n        )\n\n        if skip_connections:\n            layers += make_residuals(\"1-lip-add\", to_add)\n        else:\n            layers += to_add\n\n    layers.append(DP_Flatten())\n    layers.append(\n        DP_QuickSpectralDense(units=10, use_bias=False, kernel_initializer=\"identity\")\n    )\n\n    layers.append(DP_ClipGradient(clip_loss_gradient))\n\n    model = DP_Model(\n        layers,\n        dp_parameters=dp_parameters,\n        dataset_metadata=dataset_metadata,\n        name=\"mlp_mixer\",\n    )\n\n    model.build(input_shape=(None, *input_shape))\n\n    return model\n</code></pre> <p>We compile the model with: * any first order optimizer (e.g Adam). No adaptation is needed. * a loss with known Lipschitz constant, e.g Categorical Cross-entropy with temperature.</p> <pre><code>model = create_MLP_Mixer(dp_parameters, dataset_metadata, input_upper_bound)\nmodel.compile(\n    # Compile model using DP loss\n    loss=losses.DP_TauCategoricalCrossentropy(256.0),\n    # this method is compatible with any first order optimizer\n    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4),\n    metrics=[\"accuracy\"],\n)\nmodel.summary()\n</code></pre> <pre>\n<code>Model: \"mlp_mixer\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dp__bounded_input (DP_Bound  multiple                 0         \n edInput)                                                        \n\n dp__lambda (DP_Lambda)      multiple                  0         \n\n dp__reshape (DP_Reshape)    multiple                  0         \n\n dp__quick_spectral_dense (D  multiple                 3072      \n P_QuickSpectralDense)                                           \n\n dp__permute (DP_Permute)    multiple                  0         \n\n dp__quick_spectral_dense_1   multiple                 4096      \n (DP_QuickSpectralDense)                                         \n\n dp__group_sort (DP_GroupSor  multiple                 0         \n t)                                                              \n\n dp__quick_spectral_dense_2   multiple                 4096      \n (DP_QuickSpectralDense)                                         \n\n dp__permute_1 (DP_Permute)  multiple                  0         \n\n dp__quick_spectral_dense_3   multiple                 8192      \n (DP_QuickSpectralDense)                                         \n\n dp__group_sort_1 (DP_GroupS  multiple                 0         \n ort)                                                            \n\n dp__quick_spectral_dense_4   multiple                 8192      \n (DP_QuickSpectralDense)                                         \n\n dp__permute_2 (DP_Permute)  multiple                  0         \n\n dp__quick_spectral_dense_5   multiple                 4096      \n (DP_QuickSpectralDense)                                         \n\n dp__group_sort_2 (DP_GroupS  multiple                 0         \n ort)                                                            \n\n dp__quick_spectral_dense_6   multiple                 4096      \n (DP_QuickSpectralDense)                                         \n\n dp__permute_3 (DP_Permute)  multiple                  0         \n\n dp__quick_spectral_dense_7   multiple                 8192      \n (DP_QuickSpectralDense)                                         \n\n dp__group_sort_3 (DP_GroupS  multiple                 0         \n ort)                                                            \n\n dp__quick_spectral_dense_8   multiple                 8192      \n (DP_QuickSpectralDense)                                         \n\n dp__flatten (DP_Flatten)    multiple                  0         \n\n dp__quick_spectral_dense_9   multiple                 40960     \n (DP_QuickSpectralDense)                                         \n\n dp__clip_gradient (DP_ClipG  multiple                 0         \n radient)                                                        \n\n=================================================================\nTotal params: 93,184\nTrainable params: 93,184\nNon-trainable params: 0\n_________________________________________________________________\n</code>\n</pre> <p>Observe that the model contains only 246K parmaeters. This is an advantage of MLP Mixer architectures: the number of parameters is small. However the number of FLOPS can be quite high. Without gradient clipping, huge batch sizes can be used, which benefits to privacy/utility ratio. </p> <p>In order to control epsilon, we compute the adequate number of epochs.</p> <pre><code>num_epochs = get_max_epochs(epsilon_max, model)\n</code></pre> <pre>\n<code>epoch bounds = (0, 512.0) and epsilon = 14.81894855578722 at epoch 512.0\nepoch bounds = (256.0, 512.0) and epsilon = 9.820083418023108 at epoch 256.0\nepoch bounds = (256.0, 384.0) and epsilon = 12.31951600358698 at epoch 384.0\nepoch bounds = (256.0, 320.0) and epsilon = 11.069799714608529 at epoch 320.0\nepoch bounds = (256.0, 288.0) and epsilon = 10.44494156631582 at epoch 288.0\nepoch bounds = (256.0, 272.0) and epsilon = 10.132512492169463 at epoch 272.0\nepoch bounds = (264.0, 272.0) and epsilon = 9.976297955096285 at epoch 264.0\nepoch bounds = (264.0, 268.0) and epsilon = 10.054405223632873 at epoch 268.0\nepoch bounds = (264.0, 266.0) and epsilon = 10.015351589364581 at epoch 266.0\nepoch bounds = (265.0, 266.0) and epsilon = 9.995824772230431 at epoch 265.0\n</code>\n</pre> <pre><code>hist = model.fit(\n    ds_train,\n    epochs=num_epochs,\n    validation_data=ds_test,\n    callbacks=[\n        # accounting is done thanks to a callback\n        DP_Accountant(log_fn=\"logging\"),  # wandb.log also available.\n    ],\n)\n</code></pre> <pre>\n<code>Epoch 1/265\n5/5 [==============================] - ETA: 0s - loss: 0.1751 - accuracy: 0.1077\n (0.5205893807331654, 1e-05)-DP guarantees for epoch 1 \n\n5/5 [==============================] - 8s 547ms/step - loss: 0.1751 - accuracy: 0.1077 - val_loss: 0.1409 - val_accuracy: 0.1045\nEpoch 2/265\n5/5 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.1061\n (0.7169615437758403, 1e-05)-DP guarantees for epoch 2 \n\n5/5 [==============================] - 3s 451ms/step - loss: 0.1243 - accuracy: 0.1061 - val_loss: 0.1145 - val_accuracy: 0.1055\nEpoch 3/265\n5/5 [==============================] - ETA: 0s - loss: 0.1124 - accuracy: 0.1170\n (0.8714581783028138, 1e-05)-DP guarantees for epoch 3 \n\n5/5 [==============================] - 3s 386ms/step - loss: 0.1124 - accuracy: 0.1170 - val_loss: 0.1095 - val_accuracy: 0.1124\nEpoch 4/265\n5/5 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.1178\n (1.0041033056975341, 1e-05)-DP guarantees for epoch 4 \n\n5/5 [==============================] - 3s 416ms/step - loss: 0.1051 - accuracy: 0.1178 - val_loss: 0.1019 - val_accuracy: 0.1173\nEpoch 5/265\n5/5 [==============================] - ETA: 0s - loss: 0.0994 - accuracy: 0.1219\n (1.121902451763874, 1e-05)-DP guarantees for epoch 5 \n\n5/5 [==============================] - 3s 404ms/step - loss: 0.0994 - accuracy: 0.1219 - val_loss: 0.0973 - val_accuracy: 0.1199\nEpoch 6/265\n5/5 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.1287\n (1.2297900098052366, 1e-05)-DP guarantees for epoch 6 \n\n5/5 [==============================] - 3s 372ms/step - loss: 0.0950 - accuracy: 0.1287 - val_loss: 0.0952 - val_accuracy: 0.1274\nEpoch 7/265\n5/5 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.1332\n (1.3301791512711914, 1e-05)-DP guarantees for epoch 7 \n\n5/5 [==============================] - 2s 355ms/step - loss: 0.0927 - accuracy: 0.1332 - val_loss: 0.0917 - val_accuracy: 0.1319\nEpoch 8/265\n5/5 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.1396\n (1.425115891691246, 1e-05)-DP guarantees for epoch 8 \n\n5/5 [==============================] - 3s 360ms/step - loss: 0.0896 - accuracy: 0.1396 - val_loss: 0.0898 - val_accuracy: 0.1348\nEpoch 9/265\n5/5 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.1423\n (1.512644960027369, 1e-05)-DP guarantees for epoch 9 \n\n5/5 [==============================] - 2s 367ms/step - loss: 0.0878 - accuracy: 0.1423 - val_loss: 0.0876 - val_accuracy: 0.1386\nEpoch 10/265\n5/5 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.1461\n (1.599192443478913, 1e-05)-DP guarantees for epoch 10 \n\n5/5 [==============================] - 3s 359ms/step - loss: 0.0857 - accuracy: 0.1461 - val_loss: 0.0859 - val_accuracy: 0.1469\nEpoch 11/265\n5/5 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.1543\n (1.6782666312983627, 1e-05)-DP guarantees for epoch 11 \n\n5/5 [==============================] - 3s 353ms/step - loss: 0.0840 - accuracy: 0.1543 - val_loss: 0.0844 - val_accuracy: 0.1497\nEpoch 12/265\n5/5 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.1556\n (1.7566369758486253, 1e-05)-DP guarantees for epoch 12 \n\n5/5 [==============================] - 3s 358ms/step - loss: 0.0829 - accuracy: 0.1556 - val_loss: 0.0829 - val_accuracy: 0.1516\nEpoch 13/265\n5/5 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.1578\n (1.833150779023074, 1e-05)-DP guarantees for epoch 13 \n\n5/5 [==============================] - 3s 367ms/step - loss: 0.0816 - accuracy: 0.1578 - val_loss: 0.0819 - val_accuracy: 0.1565\nEpoch 14/265\n5/5 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.1618\n (1.903546174784228, 1e-05)-DP guarantees for epoch 14 \n\n5/5 [==============================] - 3s 370ms/step - loss: 0.0806 - accuracy: 0.1618 - val_loss: 0.0809 - val_accuracy: 0.1592\nEpoch 15/265\n5/5 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.1657\n (1.9739415712927695, 1e-05)-DP guarantees for epoch 15 \n\n5/5 [==============================] - 3s 353ms/step - loss: 0.0794 - accuracy: 0.1657 - val_loss: 0.0799 - val_accuracy: 0.1614\nEpoch 16/265\n5/5 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.1654\n (2.044336966003477, 1e-05)-DP guarantees for epoch 16 \n\n5/5 [==============================] - 2s 358ms/step - loss: 0.0788 - accuracy: 0.1654 - val_loss: 0.0791 - val_accuracy: 0.1642\nEpoch 17/265\n5/5 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.1696\n (2.111107170532668, 1e-05)-DP guarantees for epoch 17 \n\n5/5 [==============================] - 3s 373ms/step - loss: 0.0778 - accuracy: 0.1696 - val_loss: 0.0783 - val_accuracy: 0.1667\nEpoch 18/265\n5/5 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.1720\n (2.173720558035018, 1e-05)-DP guarantees for epoch 18 \n\n5/5 [==============================] - 3s 355ms/step - loss: 0.0773 - accuracy: 0.1720 - val_loss: 0.0775 - val_accuracy: 0.1713\nEpoch 19/265\n5/5 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.1745\n (2.236333946199693, 1e-05)-DP guarantees for epoch 19 \n\n5/5 [==============================] - 3s 357ms/step - loss: 0.0765 - accuracy: 0.1745 - val_loss: 0.0768 - val_accuracy: 0.1718\nEpoch 20/265\n5/5 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.1785\n (2.298947335447459, 1e-05)-DP guarantees for epoch 20 \n\n5/5 [==============================] - 3s 351ms/step - loss: 0.0755 - accuracy: 0.1785 - val_loss: 0.0761 - val_accuracy: 0.1749\nEpoch 21/265\n5/5 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.1809\n (2.3615607218535017, 1e-05)-DP guarantees for epoch 21 \n\n5/5 [==============================] - 2s 370ms/step - loss: 0.0751 - accuracy: 0.1809 - val_loss: 0.0755 - val_accuracy: 0.1779\nEpoch 22/265\n5/5 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.1807\n (2.424031214499055, 1e-05)-DP guarantees for epoch 22 \n\n5/5 [==============================] - 3s 359ms/step - loss: 0.0744 - accuracy: 0.1807 - val_loss: 0.0749 - val_accuracy: 0.1782\nEpoch 23/265\n5/5 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.1829\n (2.4794700865598074, 1e-05)-DP guarantees for epoch 23 \n\n5/5 [==============================] - 2s 353ms/step - loss: 0.0737 - accuracy: 0.1829 - val_loss: 0.0744 - val_accuracy: 0.1796\nEpoch 24/265\n5/5 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.1836\n (2.5344857802909178, 1e-05)-DP guarantees for epoch 24 \n\n5/5 [==============================] - 2s 353ms/step - loss: 0.0735 - accuracy: 0.1836 - val_loss: 0.0738 - val_accuracy: 0.1815\nEpoch 25/265\n5/5 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.1853\n (2.589501472054093, 1e-05)-DP guarantees for epoch 25 \n\n5/5 [==============================] - 3s 371ms/step - loss: 0.0730 - accuracy: 0.1853 - val_loss: 0.0733 - val_accuracy: 0.1836\nEpoch 26/265\n5/5 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.1884\n (2.6445171621630954, 1e-05)-DP guarantees for epoch 26 \n\n5/5 [==============================] - 3s 356ms/step - loss: 0.0726 - accuracy: 0.1884 - val_loss: 0.0729 - val_accuracy: 0.1857\nEpoch 27/265\n5/5 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.1881\n (2.699532854747239, 1e-05)-DP guarantees for epoch 27 \n\n5/5 [==============================] - 2s 349ms/step - loss: 0.0722 - accuracy: 0.1881 - val_loss: 0.0723 - val_accuracy: 0.1882\nEpoch 28/265\n5/5 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.1901\n (2.754548546420506, 1e-05)-DP guarantees for epoch 28 \n\n5/5 [==============================] - 3s 371ms/step - loss: 0.0715 - accuracy: 0.1901 - val_loss: 0.0718 - val_accuracy: 0.1879\nEpoch 29/265\n5/5 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.1928\n (2.809564239271509, 1e-05)-DP guarantees for epoch 29 \n\n5/5 [==============================] - 3s 360ms/step - loss: 0.0711 - accuracy: 0.1928 - val_loss: 0.0715 - val_accuracy: 0.1915\nEpoch 30/265\n5/5 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.1933\n (2.8645799306976425, 1e-05)-DP guarantees for epoch 30 \n\n5/5 [==============================] - 2s 362ms/step - loss: 0.0710 - accuracy: 0.1933 - val_loss: 0.0710 - val_accuracy: 0.1922\nEpoch 31/265\n5/5 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.1993\n (2.915773408283026, 1e-05)-DP guarantees for epoch 31 \n\n5/5 [==============================] - 2s 352ms/step - loss: 0.0701 - accuracy: 0.1993 - val_loss: 0.0706 - val_accuracy: 0.1940\nEpoch 32/265\n5/5 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.1996\n (2.9633676512735834, 1e-05)-DP guarantees for epoch 32 \n\n5/5 [==============================] - 2s 355ms/step - loss: 0.0698 - accuracy: 0.1996 - val_loss: 0.0702 - val_accuracy: 0.1964\nEpoch 33/265\n5/5 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.2004\n (3.010961895901816, 1e-05)-DP guarantees for epoch 33 \n\n5/5 [==============================] - 3s 375ms/step - loss: 0.0695 - accuracy: 0.2004 - val_loss: 0.0699 - val_accuracy: 0.1984\nEpoch 34/265\n5/5 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.1995\n (3.0585561401091397, 1e-05)-DP guarantees for epoch 34 \n\n5/5 [==============================] - 3s 352ms/step - loss: 0.0692 - accuracy: 0.1995 - val_loss: 0.0696 - val_accuracy: 0.1975\nEpoch 35/265\n5/5 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.2045\n (3.1061503817189315, 1e-05)-DP guarantees for epoch 35 \n\n5/5 [==============================] - 3s 349ms/step - loss: 0.0685 - accuracy: 0.2045 - val_loss: 0.0692 - val_accuracy: 0.2009\nEpoch 36/265\n5/5 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.2045\n (3.1537446235861095, 1e-05)-DP guarantees for epoch 36 \n\n5/5 [==============================] - 3s 364ms/step - loss: 0.0686 - accuracy: 0.2045 - val_loss: 0.0689 - val_accuracy: 0.2032\nEpoch 37/265\n5/5 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.2033\n (3.2013388677062005, 1e-05)-DP guarantees for epoch 37 \n\n5/5 [==============================] - 2s 349ms/step - loss: 0.0684 - accuracy: 0.2033 - val_loss: 0.0686 - val_accuracy: 0.2033\nEpoch 38/265\n5/5 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.2024\n (3.2489331117939875, 1e-05)-DP guarantees for epoch 38 \n\n5/5 [==============================] - 3s 352ms/step - loss: 0.0684 - accuracy: 0.2024 - val_loss: 0.0683 - val_accuracy: 0.2046\nEpoch 39/265\n5/5 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.2064\n (3.296527354122463, 1e-05)-DP guarantees for epoch 39 \n\n5/5 [==============================] - 3s 390ms/step - loss: 0.0675 - accuracy: 0.2064 - val_loss: 0.0681 - val_accuracy: 0.2055\nEpoch 40/265\n5/5 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.2071\n (3.3441215974412257, 1e-05)-DP guarantees for epoch 40 \n\n5/5 [==============================] - 2s 343ms/step - loss: 0.0678 - accuracy: 0.2071 - val_loss: 0.0679 - val_accuracy: 0.2061\nEpoch 41/265\n5/5 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.2076\n (3.391715841019588, 1e-05)-DP guarantees for epoch 41 \n\n5/5 [==============================] - 2s 348ms/step - loss: 0.0670 - accuracy: 0.2076 - val_loss: 0.0676 - val_accuracy: 0.2047\nEpoch 42/265\n5/5 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.2074\n (3.4393100820764655, 1e-05)-DP guarantees for epoch 42 \n\n5/5 [==============================] - 3s 362ms/step - loss: 0.0670 - accuracy: 0.2074 - val_loss: 0.0673 - val_accuracy: 0.2077\nEpoch 43/265\n5/5 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.2091\n (3.4869043257012042, 1e-05)-DP guarantees for epoch 43 \n\n5/5 [==============================] - 3s 365ms/step - loss: 0.0668 - accuracy: 0.2091 - val_loss: 0.0671 - val_accuracy: 0.2098\nEpoch 44/265\n5/5 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.2133\n (3.5344943006583662, 1e-05)-DP guarantees for epoch 44 \n\n5/5 [==============================] - 2s 353ms/step - loss: 0.0664 - accuracy: 0.2133 - val_loss: 0.0668 - val_accuracy: 0.2111\nEpoch 45/265\n5/5 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.2116\n (3.577278802435221, 1e-05)-DP guarantees for epoch 45 \n\n5/5 [==============================] - 3s 368ms/step - loss: 0.0662 - accuracy: 0.2116 - val_loss: 0.0666 - val_accuracy: 0.2110\nEpoch 46/265\n5/5 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.2144\n (3.6176202954309518, 1e-05)-DP guarantees for epoch 46 \n\n5/5 [==============================] - 3s 363ms/step - loss: 0.0658 - accuracy: 0.2144 - val_loss: 0.0663 - val_accuracy: 0.2136\nEpoch 47/265\n5/5 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.2136\n (3.6579617884266824, 1e-05)-DP guarantees for epoch 47 \n\n5/5 [==============================] - 3s 361ms/step - loss: 0.0660 - accuracy: 0.2136 - val_loss: 0.0662 - val_accuracy: 0.2103\nEpoch 48/265\n5/5 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.2124\n (3.698303280878773, 1e-05)-DP guarantees for epoch 48 \n\n5/5 [==============================] - 3s 378ms/step - loss: 0.0658 - accuracy: 0.2124 - val_loss: 0.0660 - val_accuracy: 0.2126\nEpoch 49/265\n5/5 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.2170\n (3.7386447748463074, 1e-05)-DP guarantees for epoch 49 \n\n5/5 [==============================] - 3s 356ms/step - loss: 0.0651 - accuracy: 0.2170 - val_loss: 0.0658 - val_accuracy: 0.2141\nEpoch 50/265\n5/5 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.2147\n (3.778986264959221, 1e-05)-DP guarantees for epoch 50 \n\n5/5 [==============================] - 2s 359ms/step - loss: 0.0650 - accuracy: 0.2147 - val_loss: 0.0657 - val_accuracy: 0.2139\nEpoch 51/265\n5/5 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.2157\n (3.819327759198358, 1e-05)-DP guarantees for epoch 51 \n\n5/5 [==============================] - 3s 362ms/step - loss: 0.0649 - accuracy: 0.2157 - val_loss: 0.0654 - val_accuracy: 0.2154\nEpoch 52/265\n5/5 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.2177\n (3.859669252353283, 1e-05)-DP guarantees for epoch 52 \n\n5/5 [==============================] - 3s 374ms/step - loss: 0.0646 - accuracy: 0.2177 - val_loss: 0.0652 - val_accuracy: 0.2159\nEpoch 53/265\n5/5 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.2164\n (3.900010744909916, 1e-05)-DP guarantees for epoch 53 \n\n5/5 [==============================] - 3s 398ms/step - loss: 0.0647 - accuracy: 0.2164 - val_loss: 0.0651 - val_accuracy: 0.2139\nEpoch 54/265\n5/5 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.2180\n (3.9403522382284417, 1e-05)-DP guarantees for epoch 54 \n\n5/5 [==============================] - 2s 356ms/step - loss: 0.0642 - accuracy: 0.2180 - val_loss: 0.0649 - val_accuracy: 0.2165\nEpoch 55/265\n5/5 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.2178\n (3.9806937272852823, 1e-05)-DP guarantees for epoch 55 \n\n5/5 [==============================] - 3s 385ms/step - loss: 0.0643 - accuracy: 0.2178 - val_loss: 0.0648 - val_accuracy: 0.2190\nEpoch 56/265\n5/5 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.2194\n (4.021035219696142, 1e-05)-DP guarantees for epoch 56 \n\n5/5 [==============================] - 3s 358ms/step - loss: 0.0642 - accuracy: 0.2194 - val_loss: 0.0646 - val_accuracy: 0.2190\nEpoch 57/265\n5/5 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.2193\n (4.061376713362479, 1e-05)-DP guarantees for epoch 57 \n\n5/5 [==============================] - 3s 357ms/step - loss: 0.0641 - accuracy: 0.2193 - val_loss: 0.0644 - val_accuracy: 0.2188\nEpoch 58/265\n5/5 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.2209\n (4.101718205195644, 1e-05)-DP guarantees for epoch 58 \n\n5/5 [==============================] - 3s 389ms/step - loss: 0.0637 - accuracy: 0.2209 - val_loss: 0.0643 - val_accuracy: 0.2203\nEpoch 59/265\n5/5 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.2207\n (4.142059698567775, 1e-05)-DP guarantees for epoch 59 \n\n5/5 [==============================] - 2s 350ms/step - loss: 0.0636 - accuracy: 0.2207 - val_loss: 0.0641 - val_accuracy: 0.2217\nEpoch 60/265\n5/5 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.2238\n (4.182401188996273, 1e-05)-DP guarantees for epoch 60 \n\n5/5 [==============================] - 2s 350ms/step - loss: 0.0631 - accuracy: 0.2238 - val_loss: 0.0639 - val_accuracy: 0.2218\nEpoch 61/265\n5/5 [==============================] - ETA: 0s - loss: 0.0635 - accuracy: 0.2223\n (4.222742681534986, 1e-05)-DP guarantees for epoch 61 \n\n5/5 [==============================] - 3s 357ms/step - loss: 0.0635 - accuracy: 0.2223 - val_loss: 0.0638 - val_accuracy: 0.2214\nEpoch 62/265\n5/5 [==============================] - ETA: 0s - loss: 0.0628 - accuracy: 0.2212\n (4.263084178169554, 1e-05)-DP guarantees for epoch 62 \n\n5/5 [==============================] - 3s 358ms/step - loss: 0.0628 - accuracy: 0.2212 - val_loss: 0.0637 - val_accuracy: 0.2214\nEpoch 63/265\n5/5 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.2236\n (4.303425669322495, 1e-05)-DP guarantees for epoch 63 \n\n5/5 [==============================] - 3s 357ms/step - loss: 0.0629 - accuracy: 0.2236 - val_loss: 0.0635 - val_accuracy: 0.2238\nEpoch 64/265\n5/5 [==============================] - ETA: 0s - loss: 0.0628 - accuracy: 0.2244\n (4.343767159305043, 1e-05)-DP guarantees for epoch 64 \n\n5/5 [==============================] - 2s 357ms/step - loss: 0.0628 - accuracy: 0.2244 - val_loss: 0.0633 - val_accuracy: 0.2229\nEpoch 65/265\n5/5 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.2242\n (4.384108652677016, 1e-05)-DP guarantees for epoch 65 \n\n5/5 [==============================] - 3s 375ms/step - loss: 0.0627 - accuracy: 0.2242 - val_loss: 0.0632 - val_accuracy: 0.2232\nEpoch 66/265\n5/5 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.2260\n (4.42445014497077, 1e-05)-DP guarantees for epoch 66 \n\n5/5 [==============================] - 2s 344ms/step - loss: 0.0625 - accuracy: 0.2260 - val_loss: 0.0630 - val_accuracy: 0.2248\nEpoch 67/265\n5/5 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.2271\n (4.4647916365799585, 1e-05)-DP guarantees for epoch 67 \n\n5/5 [==============================] - 3s 368ms/step - loss: 0.0625 - accuracy: 0.2271 - val_loss: 0.0628 - val_accuracy: 0.2265\nEpoch 68/265\n5/5 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.2292\n (4.505133128586104, 1e-05)-DP guarantees for epoch 68 \n\n5/5 [==============================] - 3s 365ms/step - loss: 0.0622 - accuracy: 0.2292 - val_loss: 0.0626 - val_accuracy: 0.2242\nEpoch 69/265\n5/5 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.2276\n (4.544958472325187, 1e-05)-DP guarantees for epoch 69 \n\n5/5 [==============================] - 2s 359ms/step - loss: 0.0623 - accuracy: 0.2276 - val_loss: 0.0626 - val_accuracy: 0.2254\nEpoch 70/265\n5/5 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.2288\n (4.580253889044595, 1e-05)-DP guarantees for epoch 70 \n\n5/5 [==============================] - 2s 362ms/step - loss: 0.0619 - accuracy: 0.2288 - val_loss: 0.0624 - val_accuracy: 0.2272\nEpoch 71/265\n5/5 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.2288\n (4.613504255128257, 1e-05)-DP guarantees for epoch 71 \n\n5/5 [==============================] - 2s 356ms/step - loss: 0.0619 - accuracy: 0.2288 - val_loss: 0.0623 - val_accuracy: 0.2258\nEpoch 72/265\n5/5 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.2283\n (4.646754619793705, 1e-05)-DP guarantees for epoch 72 \n\n5/5 [==============================] - 3s 379ms/step - loss: 0.0617 - accuracy: 0.2283 - val_loss: 0.0622 - val_accuracy: 0.2262\nEpoch 73/265\n5/5 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.2309\n (4.680004986868141, 1e-05)-DP guarantees for epoch 73 \n\n5/5 [==============================] - 3s 363ms/step - loss: 0.0615 - accuracy: 0.2309 - val_loss: 0.0621 - val_accuracy: 0.2292\nEpoch 74/265\n5/5 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.2298\n (4.713255352027643, 1e-05)-DP guarantees for epoch 74 \n\n5/5 [==============================] - 3s 392ms/step - loss: 0.0614 - accuracy: 0.2298 - val_loss: 0.0619 - val_accuracy: 0.2273\nEpoch 75/265\n5/5 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.2288\n (4.746505714565027, 1e-05)-DP guarantees for epoch 75 \n\n5/5 [==============================] - 2s 346ms/step - loss: 0.0616 - accuracy: 0.2288 - val_loss: 0.0618 - val_accuracy: 0.2283\nEpoch 76/265\n5/5 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.2314\n (4.779756080992392, 1e-05)-DP guarantees for epoch 76 \n\n5/5 [==============================] - 3s 375ms/step - loss: 0.0613 - accuracy: 0.2314 - val_loss: 0.0617 - val_accuracy: 0.2285\nEpoch 77/265\n5/5 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.2321\n (4.813006446042454, 1e-05)-DP guarantees for epoch 77 \n\n5/5 [==============================] - 3s 368ms/step - loss: 0.0611 - accuracy: 0.2321 - val_loss: 0.0615 - val_accuracy: 0.2279\nEpoch 78/265\n5/5 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.2321\n (4.84625681135709, 1e-05)-DP guarantees for epoch 78 \n\n5/5 [==============================] - 2s 366ms/step - loss: 0.0609 - accuracy: 0.2321 - val_loss: 0.0614 - val_accuracy: 0.2309\nEpoch 79/265\n5/5 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.2326\n (4.879507178851574, 1e-05)-DP guarantees for epoch 79 \n\n5/5 [==============================] - 3s 359ms/step - loss: 0.0608 - accuracy: 0.2326 - val_loss: 0.0613 - val_accuracy: 0.2316\nEpoch 80/265\n5/5 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.2311\n (4.912757545677179, 1e-05)-DP guarantees for epoch 80 \n\n5/5 [==============================] - 2s 352ms/step - loss: 0.0608 - accuracy: 0.2311 - val_loss: 0.0612 - val_accuracy: 0.2311\nEpoch 81/265\n5/5 [==============================] - ETA: 0s - loss: 0.0607 - accuracy: 0.2333\n (4.9460079085624, 1e-05)-DP guarantees for epoch 81 \n\n5/5 [==============================] - 2s 344ms/step - loss: 0.0607 - accuracy: 0.2333 - val_loss: 0.0611 - val_accuracy: 0.2317\nEpoch 82/265\n5/5 [==============================] - ETA: 0s - loss: 0.0607 - accuracy: 0.2341\n (4.979258270989774, 1e-05)-DP guarantees for epoch 82 \n\n5/5 [==============================] - 2s 339ms/step - loss: 0.0607 - accuracy: 0.2341 - val_loss: 0.0610 - val_accuracy: 0.2338\nEpoch 83/265\n5/5 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.2339\n (5.012508634818511, 1e-05)-DP guarantees for epoch 83 \n\n5/5 [==============================] - 2s 358ms/step - loss: 0.0604 - accuracy: 0.2339 - val_loss: 0.0609 - val_accuracy: 0.2318\nEpoch 84/265\n5/5 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.2348\n (5.045759003430268, 1e-05)-DP guarantees for epoch 84 \n\n5/5 [==============================] - 3s 360ms/step - loss: 0.0605 - accuracy: 0.2348 - val_loss: 0.0608 - val_accuracy: 0.2312\nEpoch 85/265\n5/5 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.2332\n (5.0790093680054635, 1e-05)-DP guarantees for epoch 85 \n\n5/5 [==============================] - 3s 348ms/step - loss: 0.0603 - accuracy: 0.2332 - val_loss: 0.0607 - val_accuracy: 0.2326\nEpoch 86/265\n5/5 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.2355\n (5.112259736439092, 1e-05)-DP guarantees for epoch 86 \n\n5/5 [==============================] - 3s 364ms/step - loss: 0.0600 - accuracy: 0.2355 - val_loss: 0.0606 - val_accuracy: 0.2333\nEpoch 87/265\n5/5 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.2357\n (5.14551009793596, 1e-05)-DP guarantees for epoch 87 \n\n5/5 [==============================] - 2s 351ms/step - loss: 0.0600 - accuracy: 0.2357 - val_loss: 0.0604 - val_accuracy: 0.2335\nEpoch 88/265\n5/5 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.2397\n (5.178760460033292, 1e-05)-DP guarantees for epoch 88 \n\n5/5 [==============================] - 2s 348ms/step - loss: 0.0598 - accuracy: 0.2397 - val_loss: 0.0603 - val_accuracy: 0.2327\nEpoch 89/265\n5/5 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.2377\n (5.212010824793953, 1e-05)-DP guarantees for epoch 89 \n\n5/5 [==============================] - 2s 345ms/step - loss: 0.0596 - accuracy: 0.2377 - val_loss: 0.0602 - val_accuracy: 0.2333\nEpoch 90/265\n5/5 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.2372\n (5.24526119058743, 1e-05)-DP guarantees for epoch 90 \n\n5/5 [==============================] - 2s 356ms/step - loss: 0.0597 - accuracy: 0.2372 - val_loss: 0.0601 - val_accuracy: 0.2336\nEpoch 91/265\n5/5 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.2367\n (5.278511560314511, 1e-05)-DP guarantees for epoch 91 \n\n5/5 [==============================] - 2s 361ms/step - loss: 0.0595 - accuracy: 0.2367 - val_loss: 0.0600 - val_accuracy: 0.2331\nEpoch 92/265\n5/5 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.2373\n (5.311761920262455, 1e-05)-DP guarantees for epoch 92 \n\n5/5 [==============================] - 3s 355ms/step - loss: 0.0598 - accuracy: 0.2373 - val_loss: 0.0599 - val_accuracy: 0.2358\nEpoch 93/265\n5/5 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.2368\n (5.3450122912656255, 1e-05)-DP guarantees for epoch 93 \n\n5/5 [==============================] - 3s 364ms/step - loss: 0.0594 - accuracy: 0.2368 - val_loss: 0.0598 - val_accuracy: 0.2346\nEpoch 94/265\n5/5 [==============================] - ETA: 0s - loss: 0.0592 - accuracy: 0.2380\n (5.37826264973137, 1e-05)-DP guarantees for epoch 94 \n\n5/5 [==============================] - 2s 351ms/step - loss: 0.0592 - accuracy: 0.2380 - val_loss: 0.0597 - val_accuracy: 0.2347\nEpoch 95/265\n5/5 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.2357\n (5.4115130208687106, 1e-05)-DP guarantees for epoch 95 \n\n5/5 [==============================] - 2s 360ms/step - loss: 0.0593 - accuracy: 0.2357 - val_loss: 0.0596 - val_accuracy: 0.2348\nEpoch 96/265\n5/5 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.2376\n (5.444763387799843, 1e-05)-DP guarantees for epoch 96 \n\n5/5 [==============================] - 2s 349ms/step - loss: 0.0594 - accuracy: 0.2376 - val_loss: 0.0595 - val_accuracy: 0.2362\nEpoch 97/265\n5/5 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.2411\n (5.47801375480832, 1e-05)-DP guarantees for epoch 97 \n\n5/5 [==============================] - 2s 363ms/step - loss: 0.0589 - accuracy: 0.2411 - val_loss: 0.0594 - val_accuracy: 0.2375\nEpoch 98/265\n5/5 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.2404\n (5.511264111964721, 1e-05)-DP guarantees for epoch 98 \n\n5/5 [==============================] - 2s 350ms/step - loss: 0.0590 - accuracy: 0.2404 - val_loss: 0.0593 - val_accuracy: 0.2377\nEpoch 99/265\n5/5 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.2406\n (5.544514479570887, 1e-05)-DP guarantees for epoch 99 \n\n5/5 [==============================] - 2s 347ms/step - loss: 0.0586 - accuracy: 0.2406 - val_loss: 0.0593 - val_accuracy: 0.2389\nEpoch 100/265\n5/5 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.2436\n (5.5777648468507035, 1e-05)-DP guarantees for epoch 100 \n\n5/5 [==============================] - 3s 356ms/step - loss: 0.0587 - accuracy: 0.2436 - val_loss: 0.0592 - val_accuracy: 0.2383\nEpoch 101/265\n5/5 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.2405\n (5.611015209476669, 1e-05)-DP guarantees for epoch 101 \n\n5/5 [==============================] - 3s 362ms/step - loss: 0.0586 - accuracy: 0.2405 - val_loss: 0.0590 - val_accuracy: 0.2382\nEpoch 102/265\n5/5 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.2409\n (5.644265572603777, 1e-05)-DP guarantees for epoch 102 \n\n5/5 [==============================] - 3s 359ms/step - loss: 0.0586 - accuracy: 0.2409 - val_loss: 0.0589 - val_accuracy: 0.2376\nEpoch 103/265\n5/5 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.2425\n (5.67751593629532, 1e-05)-DP guarantees for epoch 103 \n\n5/5 [==============================] - 3s 366ms/step - loss: 0.0584 - accuracy: 0.2425 - val_loss: 0.0588 - val_accuracy: 0.2397\nEpoch 104/265\n5/5 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.2422\n (5.710766303023046, 1e-05)-DP guarantees for epoch 104 \n\n5/5 [==============================] - 3s 370ms/step - loss: 0.0583 - accuracy: 0.2422 - val_loss: 0.0587 - val_accuracy: 0.2384\nEpoch 105/265\n5/5 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.2425\n (5.7440166690784755, 1e-05)-DP guarantees for epoch 105 \n\n5/5 [==============================] - 3s 364ms/step - loss: 0.0582 - accuracy: 0.2425 - val_loss: 0.0586 - val_accuracy: 0.2383\nEpoch 106/265\n5/5 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.2411\n (5.777267031618594, 1e-05)-DP guarantees for epoch 106 \n\n5/5 [==============================] - 2s 345ms/step - loss: 0.0583 - accuracy: 0.2411 - val_loss: 0.0586 - val_accuracy: 0.2387\nEpoch 107/265\n5/5 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.2438\n (5.8105173958576675, 1e-05)-DP guarantees for epoch 107 \n\n5/5 [==============================] - 2s 343ms/step - loss: 0.0578 - accuracy: 0.2438 - val_loss: 0.0585 - val_accuracy: 0.2409\nEpoch 108/265\n5/5 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.2442\n (5.843767765269359, 1e-05)-DP guarantees for epoch 108 \n\n5/5 [==============================] - 2s 359ms/step - loss: 0.0582 - accuracy: 0.2442 - val_loss: 0.0584 - val_accuracy: 0.2440\nEpoch 109/265\n5/5 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.2456\n (5.877018127929281, 1e-05)-DP guarantees for epoch 109 \n\n5/5 [==============================] - 2s 355ms/step - loss: 0.0578 - accuracy: 0.2456 - val_loss: 0.0584 - val_accuracy: 0.2419\nEpoch 110/265\n5/5 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.2440\n (5.910268490844311, 1e-05)-DP guarantees for epoch 110 \n\n5/5 [==============================] - 2s 362ms/step - loss: 0.0580 - accuracy: 0.2440 - val_loss: 0.0583 - val_accuracy: 0.2429\nEpoch 111/265\n5/5 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.2473\n (5.943518855328065, 1e-05)-DP guarantees for epoch 111 \n\n5/5 [==============================] - 2s 350ms/step - loss: 0.0578 - accuracy: 0.2473 - val_loss: 0.0583 - val_accuracy: 0.2448\nEpoch 112/265\n5/5 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.2469\n (5.9767692222925275, 1e-05)-DP guarantees for epoch 112 \n\n5/5 [==============================] - 3s 348ms/step - loss: 0.0577 - accuracy: 0.2469 - val_loss: 0.0582 - val_accuracy: 0.2447\nEpoch 113/265\n5/5 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.2479\n (6.0100195891034165, 1e-05)-DP guarantees for epoch 113 \n\n5/5 [==============================] - 2s 348ms/step - loss: 0.0579 - accuracy: 0.2479 - val_loss: 0.0581 - val_accuracy: 0.2453\nEpoch 114/265\n5/5 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.2468\n (6.043269950764723, 1e-05)-DP guarantees for epoch 114 \n\n5/5 [==============================] - 3s 361ms/step - loss: 0.0576 - accuracy: 0.2468 - val_loss: 0.0580 - val_accuracy: 0.2432\nEpoch 115/265\n5/5 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.2472\n (6.076520315246205, 1e-05)-DP guarantees for epoch 115 \n\n5/5 [==============================] - 2s 357ms/step - loss: 0.0574 - accuracy: 0.2472 - val_loss: 0.0579 - val_accuracy: 0.2441\nEpoch 116/265\n5/5 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.2476\n (6.109770681686705, 1e-05)-DP guarantees for epoch 116 \n\n5/5 [==============================] - 2s 363ms/step - loss: 0.0573 - accuracy: 0.2476 - val_loss: 0.0579 - val_accuracy: 0.2440\nEpoch 117/265\n5/5 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.2470\n (6.143021045607053, 1e-05)-DP guarantees for epoch 117 \n\n5/5 [==============================] - 3s 357ms/step - loss: 0.0575 - accuracy: 0.2470 - val_loss: 0.0578 - val_accuracy: 0.2479\nEpoch 118/265\n5/5 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.2481\n (6.1762714106501475, 1e-05)-DP guarantees for epoch 118 \n\n5/5 [==============================] - 3s 360ms/step - loss: 0.0572 - accuracy: 0.2481 - val_loss: 0.0576 - val_accuracy: 0.2450\nEpoch 119/265\n5/5 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.2500\n (6.209521499901805, 1e-05)-DP guarantees for epoch 119 \n\n5/5 [==============================] - 3s 367ms/step - loss: 0.0572 - accuracy: 0.2500 - val_loss: 0.0576 - val_accuracy: 0.2446\nEpoch 120/265\n5/5 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.2497\n (6.241605627485653, 1e-05)-DP guarantees for epoch 120 \n\n5/5 [==============================] - 2s 355ms/step - loss: 0.0569 - accuracy: 0.2497 - val_loss: 0.0575 - val_accuracy: 0.2451\nEpoch 121/265\n5/5 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.2510\n (6.271221812058615, 1e-05)-DP guarantees for epoch 121 \n\n5/5 [==============================] - 2s 351ms/step - loss: 0.0569 - accuracy: 0.2510 - val_loss: 0.0574 - val_accuracy: 0.2445\nEpoch 122/265\n5/5 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.2481\n (6.298196491974402, 1e-05)-DP guarantees for epoch 122 \n\n5/5 [==============================] - 2s 359ms/step - loss: 0.0571 - accuracy: 0.2481 - val_loss: 0.0574 - val_accuracy: 0.2447\nEpoch 123/265\n5/5 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.2517\n (6.324510712314491, 1e-05)-DP guarantees for epoch 123 \n\n5/5 [==============================] - 2s 345ms/step - loss: 0.0568 - accuracy: 0.2517 - val_loss: 0.0573 - val_accuracy: 0.2481\nEpoch 124/265\n5/5 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.2505\n (6.350824932887864, 1e-05)-DP guarantees for epoch 124 \n\n5/5 [==============================] - 3s 359ms/step - loss: 0.0570 - accuracy: 0.2505 - val_loss: 0.0573 - val_accuracy: 0.2449\nEpoch 125/265\n5/5 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.2489\n (6.377139153079873, 1e-05)-DP guarantees for epoch 125 \n\n5/5 [==============================] - 2s 368ms/step - loss: 0.0567 - accuracy: 0.2489 - val_loss: 0.0572 - val_accuracy: 0.2450\nEpoch 126/265\n5/5 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.2488\n (6.403453374888347, 1e-05)-DP guarantees for epoch 126 \n\n5/5 [==============================] - 3s 349ms/step - loss: 0.0570 - accuracy: 0.2488 - val_loss: 0.0572 - val_accuracy: 0.2485\nEpoch 127/265\n5/5 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.2539\n (6.429767596763488, 1e-05)-DP guarantees for epoch 127 \n\n5/5 [==============================] - 3s 391ms/step - loss: 0.0566 - accuracy: 0.2539 - val_loss: 0.0571 - val_accuracy: 0.2452\nEpoch 128/265\n5/5 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.2505\n (6.4560818158974875, 1e-05)-DP guarantees for epoch 128 \n\n5/5 [==============================] - 3s 367ms/step - loss: 0.0565 - accuracy: 0.2505 - val_loss: 0.0570 - val_accuracy: 0.2466\nEpoch 129/265\n5/5 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.2522\n (6.482396036898421, 1e-05)-DP guarantees for epoch 129 \n\n5/5 [==============================] - 2s 343ms/step - loss: 0.0566 - accuracy: 0.2522 - val_loss: 0.0570 - val_accuracy: 0.2461\nEpoch 130/265\n5/5 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.2521\n (6.5087102545452, 1e-05)-DP guarantees for epoch 130 \n\n5/5 [==============================] - 2s 353ms/step - loss: 0.0561 - accuracy: 0.2521 - val_loss: 0.0569 - val_accuracy: 0.2468\nEpoch 131/265\n5/5 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.2534\n (6.53502447810436, 1e-05)-DP guarantees for epoch 131 \n\n5/5 [==============================] - 2s 374ms/step - loss: 0.0562 - accuracy: 0.2534 - val_loss: 0.0569 - val_accuracy: 0.2470\nEpoch 132/265\n5/5 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.2530\n (6.5613386977335715, 1e-05)-DP guarantees for epoch 132 \n\n5/5 [==============================] - 2s 350ms/step - loss: 0.0563 - accuracy: 0.2530 - val_loss: 0.0568 - val_accuracy: 0.2501\nEpoch 133/265\n5/5 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.2564\n (6.587652915827986, 1e-05)-DP guarantees for epoch 133 \n\n5/5 [==============================] - 3s 368ms/step - loss: 0.0561 - accuracy: 0.2564 - val_loss: 0.0569 - val_accuracy: 0.2470\nEpoch 134/265\n5/5 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.2555\n (6.613967135260202, 1e-05)-DP guarantees for epoch 134 \n\n5/5 [==============================] - 3s 402ms/step - loss: 0.0561 - accuracy: 0.2555 - val_loss: 0.0568 - val_accuracy: 0.2492\nEpoch 135/265\n5/5 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.2535\n (6.6402813578423405, 1e-05)-DP guarantees for epoch 135 \n\n5/5 [==============================] - 2s 347ms/step - loss: 0.0564 - accuracy: 0.2535 - val_loss: 0.0567 - val_accuracy: 0.2499\nEpoch 136/265\n5/5 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.2552\n (6.666595582737012, 1e-05)-DP guarantees for epoch 136 \n\n5/5 [==============================] - 2s 360ms/step - loss: 0.0559 - accuracy: 0.2552 - val_loss: 0.0567 - val_accuracy: 0.2506\nEpoch 137/265\n5/5 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.2562\n (6.692909796982604, 1e-05)-DP guarantees for epoch 137 \n\n5/5 [==============================] - 3s 364ms/step - loss: 0.0560 - accuracy: 0.2562 - val_loss: 0.0566 - val_accuracy: 0.2484\nEpoch 138/265\n5/5 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.2538\n (6.719224016310403, 1e-05)-DP guarantees for epoch 138 \n\n5/5 [==============================] - 2s 349ms/step - loss: 0.0560 - accuracy: 0.2538 - val_loss: 0.0565 - val_accuracy: 0.2471\nEpoch 139/265\n5/5 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.2526\n (6.74553823900151, 1e-05)-DP guarantees for epoch 139 \n\n5/5 [==============================] - 3s 399ms/step - loss: 0.0560 - accuracy: 0.2526 - val_loss: 0.0565 - val_accuracy: 0.2509\nEpoch 140/265\n5/5 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.2536\n (6.771852459824933, 1e-05)-DP guarantees for epoch 140 \n\n5/5 [==============================] - 3s 493ms/step - loss: 0.0560 - accuracy: 0.2536 - val_loss: 0.0564 - val_accuracy: 0.2493\nEpoch 141/265\n5/5 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.2555\n (6.798166680154963, 1e-05)-DP guarantees for epoch 141 \n\n5/5 [==============================] - 3s 391ms/step - loss: 0.0557 - accuracy: 0.2555 - val_loss: 0.0563 - val_accuracy: 0.2511\nEpoch 142/265\n5/5 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.2541\n (6.824480898392123, 1e-05)-DP guarantees for epoch 142 \n\n5/5 [==============================] - 3s 443ms/step - loss: 0.0559 - accuracy: 0.2541 - val_loss: 0.0563 - val_accuracy: 0.2484\nEpoch 143/265\n5/5 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.2547\n (6.850795124433479, 1e-05)-DP guarantees for epoch 143 \n\n5/5 [==============================] - 3s 368ms/step - loss: 0.0560 - accuracy: 0.2547 - val_loss: 0.0563 - val_accuracy: 0.2487\nEpoch 144/265\n5/5 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.2545\n (6.877109344205954, 1e-05)-DP guarantees for epoch 144 \n\n5/5 [==============================] - 3s 374ms/step - loss: 0.0556 - accuracy: 0.2545 - val_loss: 0.0562 - val_accuracy: 0.2487\nEpoch 145/265\n5/5 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.2569\n (6.903423558068683, 1e-05)-DP guarantees for epoch 145 \n\n5/5 [==============================] - 3s 378ms/step - loss: 0.0555 - accuracy: 0.2569 - val_loss: 0.0562 - val_accuracy: 0.2508\nEpoch 146/265\n5/5 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.2560\n (6.929737777126363, 1e-05)-DP guarantees for epoch 146 \n\n5/5 [==============================] - 3s 387ms/step - loss: 0.0558 - accuracy: 0.2560 - val_loss: 0.0561 - val_accuracy: 0.2504\nEpoch 147/265\n5/5 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.2556\n (6.956052008535497, 1e-05)-DP guarantees for epoch 147 \n\n5/5 [==============================] - 3s 372ms/step - loss: 0.0557 - accuracy: 0.2556 - val_loss: 0.0561 - val_accuracy: 0.2509\nEpoch 148/265\n5/5 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.2538\n (6.982366223228706, 1e-05)-DP guarantees for epoch 148 \n\n5/5 [==============================] - 3s 381ms/step - loss: 0.0557 - accuracy: 0.2538 - val_loss: 0.0561 - val_accuracy: 0.2528\nEpoch 149/265\n5/5 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.2580\n (7.0086804403647855, 1e-05)-DP guarantees for epoch 149 \n\n5/5 [==============================] - 3s 357ms/step - loss: 0.0553 - accuracy: 0.2580 - val_loss: 0.0560 - val_accuracy: 0.2530\nEpoch 150/265\n5/5 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.2595\n (7.034994664689931, 1e-05)-DP guarantees for epoch 150 \n\n5/5 [==============================] - 3s 361ms/step - loss: 0.0549 - accuracy: 0.2595 - val_loss: 0.0560 - val_accuracy: 0.2519\nEpoch 151/265\n5/5 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.2585\n (7.061308885525292, 1e-05)-DP guarantees for epoch 151 \n\n5/5 [==============================] - 2s 355ms/step - loss: 0.0554 - accuracy: 0.2585 - val_loss: 0.0559 - val_accuracy: 0.2531\nEpoch 152/265\n5/5 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.2580\n (7.087623106633284, 1e-05)-DP guarantees for epoch 152 \n\n5/5 [==============================] - 2s 355ms/step - loss: 0.0554 - accuracy: 0.2580 - val_loss: 0.0558 - val_accuracy: 0.2543\nEpoch 153/265\n5/5 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.2585\n (7.113937323136563, 1e-05)-DP guarantees for epoch 153 \n\n5/5 [==============================] - 3s 365ms/step - loss: 0.0553 - accuracy: 0.2585 - val_loss: 0.0558 - val_accuracy: 0.2537\nEpoch 154/265\n5/5 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.2595\n (7.140251544398778, 1e-05)-DP guarantees for epoch 154 \n\n5/5 [==============================] - 3s 359ms/step - loss: 0.0551 - accuracy: 0.2595 - val_loss: 0.0558 - val_accuracy: 0.2551\nEpoch 155/265\n5/5 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.2600\n (7.166565767658498, 1e-05)-DP guarantees for epoch 155 \n\n5/5 [==============================] - 3s 355ms/step - loss: 0.0550 - accuracy: 0.2600 - val_loss: 0.0557 - val_accuracy: 0.2569\nEpoch 156/265\n5/5 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.2561\n (7.192879981310637, 1e-05)-DP guarantees for epoch 156 \n\n5/5 [==============================] - 2s 353ms/step - loss: 0.0553 - accuracy: 0.2561 - val_loss: 0.0556 - val_accuracy: 0.2545\nEpoch 157/265\n5/5 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.2581\n (7.2191942080187195, 1e-05)-DP guarantees for epoch 157 \n\n5/5 [==============================] - 3s 356ms/step - loss: 0.0550 - accuracy: 0.2581 - val_loss: 0.0556 - val_accuracy: 0.2566\nEpoch 158/265\n5/5 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.2601\n (7.245508431022666, 1e-05)-DP guarantees for epoch 158 \n\n5/5 [==============================] - 2s 353ms/step - loss: 0.0550 - accuracy: 0.2601 - val_loss: 0.0556 - val_accuracy: 0.2574\nEpoch 159/265\n5/5 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.2599\n (7.27182264840541, 1e-05)-DP guarantees for epoch 159 \n\n5/5 [==============================] - 2s 343ms/step - loss: 0.0548 - accuracy: 0.2599 - val_loss: 0.0555 - val_accuracy: 0.2567\nEpoch 160/265\n5/5 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.2616\n (7.298136867745498, 1e-05)-DP guarantees for epoch 160 \n\n5/5 [==============================] - 2s 367ms/step - loss: 0.0548 - accuracy: 0.2616 - val_loss: 0.0554 - val_accuracy: 0.2560\nEpoch 161/265\n5/5 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.2595\n (7.324451088022072, 1e-05)-DP guarantees for epoch 161 \n\n5/5 [==============================] - 3s 349ms/step - loss: 0.0551 - accuracy: 0.2595 - val_loss: 0.0554 - val_accuracy: 0.2577\nEpoch 162/265\n5/5 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.2606\n (7.350765305854425, 1e-05)-DP guarantees for epoch 162 \n\n5/5 [==============================] - 2s 352ms/step - loss: 0.0548 - accuracy: 0.2606 - val_loss: 0.0554 - val_accuracy: 0.2580\nEpoch 163/265\n5/5 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.2588\n (7.37707952170881, 1e-05)-DP guarantees for epoch 163 \n\n5/5 [==============================] - 2s 351ms/step - loss: 0.0547 - accuracy: 0.2588 - val_loss: 0.0553 - val_accuracy: 0.2549\nEpoch 164/265\n5/5 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.2585\n (7.403393741099066, 1e-05)-DP guarantees for epoch 164 \n\n5/5 [==============================] - 3s 379ms/step - loss: 0.0546 - accuracy: 0.2585 - val_loss: 0.0553 - val_accuracy: 0.2591\nEpoch 165/265\n5/5 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.2607\n (7.429707969366283, 1e-05)-DP guarantees for epoch 165 \n\n5/5 [==============================] - 3s 368ms/step - loss: 0.0546 - accuracy: 0.2607 - val_loss: 0.0552 - val_accuracy: 0.2574\nEpoch 166/265\n5/5 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.2598\n (7.456022189620042, 1e-05)-DP guarantees for epoch 166 \n\n5/5 [==============================] - 2s 356ms/step - loss: 0.0547 - accuracy: 0.2598 - val_loss: 0.0551 - val_accuracy: 0.2544\nEpoch 167/265\n5/5 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.2589\n (7.4823364015791975, 1e-05)-DP guarantees for epoch 167 \n\n5/5 [==============================] - 2s 343ms/step - loss: 0.0544 - accuracy: 0.2589 - val_loss: 0.0552 - val_accuracy: 0.2570\nEpoch 168/265\n5/5 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.2620\n (7.508650622437409, 1e-05)-DP guarantees for epoch 168 \n\n5/5 [==============================] - 2s 343ms/step - loss: 0.0546 - accuracy: 0.2620 - val_loss: 0.0551 - val_accuracy: 0.2585\nEpoch 169/265\n5/5 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.2609\n (7.5349648424170645, 1e-05)-DP guarantees for epoch 169 \n\n5/5 [==============================] - 3s 371ms/step - loss: 0.0544 - accuracy: 0.2609 - val_loss: 0.0550 - val_accuracy: 0.2591\nEpoch 170/265\n5/5 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.2618\n (7.561279065737033, 1e-05)-DP guarantees for epoch 170 \n\n5/5 [==============================] - 3s 369ms/step - loss: 0.0545 - accuracy: 0.2618 - val_loss: 0.0551 - val_accuracy: 0.2582\nEpoch 171/265\n5/5 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.2642\n (7.587593290867159, 1e-05)-DP guarantees for epoch 171 \n\n5/5 [==============================] - 3s 372ms/step - loss: 0.0542 - accuracy: 0.2642 - val_loss: 0.0551 - val_accuracy: 0.2598\nEpoch 172/265\n5/5 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.2640\n (7.613907506714526, 1e-05)-DP guarantees for epoch 172 \n\n5/5 [==============================] - 3s 369ms/step - loss: 0.0543 - accuracy: 0.2640 - val_loss: 0.0550 - val_accuracy: 0.2604\nEpoch 173/265\n5/5 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.2642\n (7.640221723584304, 1e-05)-DP guarantees for epoch 173 \n\n5/5 [==============================] - 3s 359ms/step - loss: 0.0543 - accuracy: 0.2642 - val_loss: 0.0549 - val_accuracy: 0.2604\nEpoch 174/265\n5/5 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.2635\n (7.666535950048996, 1e-05)-DP guarantees for epoch 174 \n\n5/5 [==============================] - 2s 344ms/step - loss: 0.0542 - accuracy: 0.2635 - val_loss: 0.0549 - val_accuracy: 0.2628\nEpoch 175/265\n5/5 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.2648\n (7.692850164248792, 1e-05)-DP guarantees for epoch 175 \n\n5/5 [==============================] - 2s 358ms/step - loss: 0.0541 - accuracy: 0.2648 - val_loss: 0.0548 - val_accuracy: 0.2625\nEpoch 176/265\n5/5 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.2637\n (7.719164393302542, 1e-05)-DP guarantees for epoch 176 \n\n5/5 [==============================] - 2s 358ms/step - loss: 0.0542 - accuracy: 0.2637 - val_loss: 0.0547 - val_accuracy: 0.2621\nEpoch 177/265\n5/5 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.2661\n (7.745478613553454, 1e-05)-DP guarantees for epoch 177 \n\n5/5 [==============================] - 2s 351ms/step - loss: 0.0540 - accuracy: 0.2661 - val_loss: 0.0546 - val_accuracy: 0.2665\nEpoch 178/265\n5/5 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.2668\n (7.771792822684058, 1e-05)-DP guarantees for epoch 178 \n\n5/5 [==============================] - 2s 353ms/step - loss: 0.0541 - accuracy: 0.2668 - val_loss: 0.0546 - val_accuracy: 0.2659\nEpoch 179/265\n5/5 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.2685\n (7.7981070469012, 1e-05)-DP guarantees for epoch 179 \n\n5/5 [==============================] - 2s 357ms/step - loss: 0.0539 - accuracy: 0.2685 - val_loss: 0.0545 - val_accuracy: 0.2646\nEpoch 180/265\n5/5 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.2682\n (7.824421268798268, 1e-05)-DP guarantees for epoch 180 \n\n5/5 [==============================] - 2s 352ms/step - loss: 0.0538 - accuracy: 0.2682 - val_loss: 0.0545 - val_accuracy: 0.2656\nEpoch 181/265\n5/5 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.2671\n (7.850735498247861, 1e-05)-DP guarantees for epoch 181 \n\n5/5 [==============================] - 2s 358ms/step - loss: 0.0538 - accuracy: 0.2671 - val_loss: 0.0545 - val_accuracy: 0.2639\nEpoch 182/265\n5/5 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.2661\n (7.877049711425853, 1e-05)-DP guarantees for epoch 182 \n\n5/5 [==============================] - 3s 368ms/step - loss: 0.0539 - accuracy: 0.2661 - val_loss: 0.0544 - val_accuracy: 0.2645\nEpoch 183/265\n5/5 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.2635\n (7.903363929529842, 1e-05)-DP guarantees for epoch 183 \n\n5/5 [==============================] - 3s 356ms/step - loss: 0.0537 - accuracy: 0.2635 - val_loss: 0.0544 - val_accuracy: 0.2645\nEpoch 184/265\n5/5 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.2641\n (7.929678153587394, 1e-05)-DP guarantees for epoch 184 \n\n5/5 [==============================] - 3s 361ms/step - loss: 0.0538 - accuracy: 0.2641 - val_loss: 0.0543 - val_accuracy: 0.2641\nEpoch 185/265\n5/5 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.2668\n (7.955992381685565, 1e-05)-DP guarantees for epoch 185 \n\n5/5 [==============================] - 2s 348ms/step - loss: 0.0535 - accuracy: 0.2668 - val_loss: 0.0543 - val_accuracy: 0.2638\nEpoch 186/265\n5/5 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.2641\n (7.982306589145621, 1e-05)-DP guarantees for epoch 186 \n\n5/5 [==============================] - 3s 357ms/step - loss: 0.0535 - accuracy: 0.2641 - val_loss: 0.0543 - val_accuracy: 0.2654\nEpoch 187/265\n5/5 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.2653\n (8.008620808026855, 1e-05)-DP guarantees for epoch 187 \n\n5/5 [==============================] - 3s 412ms/step - loss: 0.0537 - accuracy: 0.2653 - val_loss: 0.0542 - val_accuracy: 0.2651\nEpoch 188/265\n5/5 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.2656\n (8.034935029136395, 1e-05)-DP guarantees for epoch 188 \n\n5/5 [==============================] - 3s 488ms/step - loss: 0.0535 - accuracy: 0.2656 - val_loss: 0.0542 - val_accuracy: 0.2662\nEpoch 189/265\n5/5 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.2653\n (8.061249248434443, 1e-05)-DP guarantees for epoch 189 \n\n5/5 [==============================] - 3s 444ms/step - loss: 0.0536 - accuracy: 0.2653 - val_loss: 0.0541 - val_accuracy: 0.2659\nEpoch 190/265\n5/5 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.2676\n (8.087563469816706, 1e-05)-DP guarantees for epoch 190 \n\n5/5 [==============================] - 3s 405ms/step - loss: 0.0533 - accuracy: 0.2676 - val_loss: 0.0541 - val_accuracy: 0.2663\nEpoch 191/265\n5/5 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.2669\n (8.113877688170744, 1e-05)-DP guarantees for epoch 191 \n\n5/5 [==============================] - 3s 385ms/step - loss: 0.0534 - accuracy: 0.2669 - val_loss: 0.0541 - val_accuracy: 0.2675\nEpoch 192/265\n5/5 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.2648\n (8.140191906358039, 1e-05)-DP guarantees for epoch 192 \n\n5/5 [==============================] - 3s 392ms/step - loss: 0.0535 - accuracy: 0.2648 - val_loss: 0.0540 - val_accuracy: 0.2676\nEpoch 193/265\n5/5 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.2680\n (8.166506132866681, 1e-05)-DP guarantees for epoch 193 \n\n5/5 [==============================] - 3s 379ms/step - loss: 0.0534 - accuracy: 0.2680 - val_loss: 0.0540 - val_accuracy: 0.2676\nEpoch 194/265\n5/5 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.2654\n (8.192820350846777, 1e-05)-DP guarantees for epoch 194 \n\n5/5 [==============================] - 2s 356ms/step - loss: 0.0533 - accuracy: 0.2654 - val_loss: 0.0540 - val_accuracy: 0.2679\nEpoch 195/265\n5/5 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.2681\n (8.219134573417037, 1e-05)-DP guarantees for epoch 195 \n\n5/5 [==============================] - 3s 381ms/step - loss: 0.0531 - accuracy: 0.2681 - val_loss: 0.0541 - val_accuracy: 0.2654\nEpoch 196/265\n5/5 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.2671\n (8.24544879099129, 1e-05)-DP guarantees for epoch 196 \n\n5/5 [==============================] - 3s 381ms/step - loss: 0.0532 - accuracy: 0.2671 - val_loss: 0.0540 - val_accuracy: 0.2658\nEpoch 197/265\n5/5 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.2666\n (8.271763016196239, 1e-05)-DP guarantees for epoch 197 \n\n5/5 [==============================] - 3s 389ms/step - loss: 0.0535 - accuracy: 0.2666 - val_loss: 0.0540 - val_accuracy: 0.2656\nEpoch 198/265\n5/5 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.2676\n (8.298077232897459, 1e-05)-DP guarantees for epoch 198 \n\n5/5 [==============================] - 3s 415ms/step - loss: 0.0534 - accuracy: 0.2676 - val_loss: 0.0539 - val_accuracy: 0.2656\nEpoch 199/265\n5/5 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.2672\n (8.324391446543665, 1e-05)-DP guarantees for epoch 199 \n\n5/5 [==============================] - 3s 380ms/step - loss: 0.0531 - accuracy: 0.2672 - val_loss: 0.0538 - val_accuracy: 0.2658\nEpoch 200/265\n5/5 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.2638\n (8.350705669706155, 1e-05)-DP guarantees for epoch 200 \n\n5/5 [==============================] - 2s 358ms/step - loss: 0.0534 - accuracy: 0.2638 - val_loss: 0.0538 - val_accuracy: 0.2659\nEpoch 201/265\n5/5 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.2672\n (8.377019893272927, 1e-05)-DP guarantees for epoch 201 \n\n5/5 [==============================] - 2s 369ms/step - loss: 0.0533 - accuracy: 0.2672 - val_loss: 0.0538 - val_accuracy: 0.2678\nEpoch 202/265\n5/5 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.2685\n (8.403334112768452, 1e-05)-DP guarantees for epoch 202 \n\n5/5 [==============================] - 3s 362ms/step - loss: 0.0533 - accuracy: 0.2685 - val_loss: 0.0537 - val_accuracy: 0.2677\nEpoch 203/265\n5/5 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.2701\n (8.429648329088547, 1e-05)-DP guarantees for epoch 203 \n\n5/5 [==============================] - 2s 351ms/step - loss: 0.0528 - accuracy: 0.2701 - val_loss: 0.0537 - val_accuracy: 0.2669\nEpoch 204/265\n5/5 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.2696\n (8.455962556505566, 1e-05)-DP guarantees for epoch 204 \n\n5/5 [==============================] - 2s 344ms/step - loss: 0.0528 - accuracy: 0.2696 - val_loss: 0.0537 - val_accuracy: 0.2681\nEpoch 205/265\n5/5 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.2691\n (8.4822767745793, 1e-05)-DP guarantees for epoch 205 \n\n5/5 [==============================] - 2s 348ms/step - loss: 0.0532 - accuracy: 0.2691 - val_loss: 0.0536 - val_accuracy: 0.2692\nEpoch 206/265\n5/5 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.2703\n (8.508590990133396, 1e-05)-DP guarantees for epoch 206 \n\n5/5 [==============================] - 3s 354ms/step - loss: 0.0531 - accuracy: 0.2703 - val_loss: 0.0535 - val_accuracy: 0.2683\nEpoch 207/265\n5/5 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.2705\n (8.534905221654196, 1e-05)-DP guarantees for epoch 207 \n\n5/5 [==============================] - 3s 348ms/step - loss: 0.0529 - accuracy: 0.2705 - val_loss: 0.0535 - val_accuracy: 0.2661\nEpoch 208/265\n5/5 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.2726\n (8.56121943210842, 1e-05)-DP guarantees for epoch 208 \n\n5/5 [==============================] - 2s 351ms/step - loss: 0.0526 - accuracy: 0.2726 - val_loss: 0.0535 - val_accuracy: 0.2671\nEpoch 209/265\n5/5 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.2703\n (8.58753364829852, 1e-05)-DP guarantees for epoch 209 \n\n5/5 [==============================] - 2s 350ms/step - loss: 0.0530 - accuracy: 0.2703 - val_loss: 0.0534 - val_accuracy: 0.2691\nEpoch 210/265\n5/5 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.2701\n (8.613847875406321, 1e-05)-DP guarantees for epoch 210 \n\n5/5 [==============================] - 2s 344ms/step - loss: 0.0527 - accuracy: 0.2701 - val_loss: 0.0534 - val_accuracy: 0.2676\nEpoch 211/265\n5/5 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.2713\n (8.640162093797892, 1e-05)-DP guarantees for epoch 211 \n\n5/5 [==============================] - 3s 350ms/step - loss: 0.0525 - accuracy: 0.2713 - val_loss: 0.0534 - val_accuracy: 0.2689\nEpoch 212/265\n5/5 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.2711\n (8.666476313556027, 1e-05)-DP guarantees for epoch 212 \n\n5/5 [==============================] - 2s 346ms/step - loss: 0.0527 - accuracy: 0.2711 - val_loss: 0.0533 - val_accuracy: 0.2679\nEpoch 213/265\n5/5 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.2722\n (8.692790541337777, 1e-05)-DP guarantees for epoch 213 \n\n5/5 [==============================] - 3s 356ms/step - loss: 0.0524 - accuracy: 0.2722 - val_loss: 0.0532 - val_accuracy: 0.2673\nEpoch 214/265\n5/5 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.2705\n (8.719104752659717, 1e-05)-DP guarantees for epoch 214 \n\n5/5 [==============================] - 3s 355ms/step - loss: 0.0526 - accuracy: 0.2705 - val_loss: 0.0532 - val_accuracy: 0.2675\nEpoch 215/265\n5/5 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.2729\n (8.745418971706883, 1e-05)-DP guarantees for epoch 215 \n\n5/5 [==============================] - 3s 359ms/step - loss: 0.0523 - accuracy: 0.2729 - val_loss: 0.0532 - val_accuracy: 0.2674\nEpoch 216/265\n5/5 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.2733\n (8.77173318977154, 1e-05)-DP guarantees for epoch 216 \n\n5/5 [==============================] - 3s 362ms/step - loss: 0.0525 - accuracy: 0.2733 - val_loss: 0.0532 - val_accuracy: 0.2662\nEpoch 217/265\n5/5 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.2719\n (8.798047413801395, 1e-05)-DP guarantees for epoch 217 \n\n5/5 [==============================] - 3s 353ms/step - loss: 0.0525 - accuracy: 0.2719 - val_loss: 0.0531 - val_accuracy: 0.2676\nEpoch 218/265\n5/5 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.2741\n (8.82436163499698, 1e-05)-DP guarantees for epoch 218 \n\n5/5 [==============================] - 2s 348ms/step - loss: 0.0524 - accuracy: 0.2741 - val_loss: 0.0531 - val_accuracy: 0.2671\nEpoch 219/265\n5/5 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.2702\n (8.850675857122916, 1e-05)-DP guarantees for epoch 219 \n\n5/5 [==============================] - 3s 386ms/step - loss: 0.0525 - accuracy: 0.2702 - val_loss: 0.0531 - val_accuracy: 0.2672\nEpoch 220/265\n5/5 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.2709\n (8.876990076626331, 1e-05)-DP guarantees for epoch 220 \n\n5/5 [==============================] - 3s 376ms/step - loss: 0.0527 - accuracy: 0.2709 - val_loss: 0.0531 - val_accuracy: 0.2668\nEpoch 221/265\n5/5 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.2715\n (8.903304291167267, 1e-05)-DP guarantees for epoch 221 \n\n5/5 [==============================] - 3s 363ms/step - loss: 0.0525 - accuracy: 0.2715 - val_loss: 0.0531 - val_accuracy: 0.2661\nEpoch 222/265\n5/5 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.2721\n (8.929618511328595, 1e-05)-DP guarantees for epoch 222 \n\n5/5 [==============================] - 3s 378ms/step - loss: 0.0524 - accuracy: 0.2721 - val_loss: 0.0530 - val_accuracy: 0.2677\nEpoch 223/265\n5/5 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.2726\n (8.955932731489924, 1e-05)-DP guarantees for epoch 223 \n\n5/5 [==============================] - 2s 353ms/step - loss: 0.0521 - accuracy: 0.2726 - val_loss: 0.0530 - val_accuracy: 0.2686\nEpoch 224/265\n5/5 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.2727\n (8.982246951651252, 1e-05)-DP guarantees for epoch 224 \n\n5/5 [==============================] - 2s 350ms/step - loss: 0.0521 - accuracy: 0.2727 - val_loss: 0.0530 - val_accuracy: 0.2690\nEpoch 225/265\n5/5 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.2706\n (9.00856117181258, 1e-05)-DP guarantees for epoch 225 \n\n5/5 [==============================] - 3s 353ms/step - loss: 0.0522 - accuracy: 0.2706 - val_loss: 0.0529 - val_accuracy: 0.2701\nEpoch 226/265\n5/5 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.2715\n (9.034875391973909, 1e-05)-DP guarantees for epoch 226 \n\n5/5 [==============================] - 3s 396ms/step - loss: 0.0521 - accuracy: 0.2715 - val_loss: 0.0529 - val_accuracy: 0.2690\nEpoch 227/265\n5/5 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.2713\n (9.061189612135239, 1e-05)-DP guarantees for epoch 227 \n\n5/5 [==============================] - 3s 365ms/step - loss: 0.0521 - accuracy: 0.2713 - val_loss: 0.0529 - val_accuracy: 0.2702\nEpoch 228/265\n5/5 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.2729\n (9.087503832296568, 1e-05)-DP guarantees for epoch 228 \n\n5/5 [==============================] - 3s 366ms/step - loss: 0.0520 - accuracy: 0.2729 - val_loss: 0.0529 - val_accuracy: 0.2698\nEpoch 229/265\n5/5 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.2717\n (9.113818052457898, 1e-05)-DP guarantees for epoch 229 \n\n5/5 [==============================] - 3s 364ms/step - loss: 0.0518 - accuracy: 0.2717 - val_loss: 0.0528 - val_accuracy: 0.2704\nEpoch 230/265\n5/5 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.2710\n (9.140132272619226, 1e-05)-DP guarantees for epoch 230 \n\n5/5 [==============================] - 3s 371ms/step - loss: 0.0518 - accuracy: 0.2710 - val_loss: 0.0528 - val_accuracy: 0.2693\nEpoch 231/265\n5/5 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.2725\n (9.166446492780555, 1e-05)-DP guarantees for epoch 231 \n\n5/5 [==============================] - 2s 357ms/step - loss: 0.0519 - accuracy: 0.2725 - val_loss: 0.0528 - val_accuracy: 0.2674\nEpoch 232/265\n5/5 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.2743\n (9.192760712941883, 1e-05)-DP guarantees for epoch 232 \n\n5/5 [==============================] - 3s 361ms/step - loss: 0.0517 - accuracy: 0.2743 - val_loss: 0.0528 - val_accuracy: 0.2685\nEpoch 233/265\n5/5 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.2712\n (9.219074933103212, 1e-05)-DP guarantees for epoch 233 \n\n5/5 [==============================] - 3s 353ms/step - loss: 0.0519 - accuracy: 0.2712 - val_loss: 0.0527 - val_accuracy: 0.2687\nEpoch 234/265\n5/5 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.2731\n (9.24538915326454, 1e-05)-DP guarantees for epoch 234 \n\n5/5 [==============================] - 3s 370ms/step - loss: 0.0517 - accuracy: 0.2731 - val_loss: 0.0527 - val_accuracy: 0.2663\nEpoch 235/265\n5/5 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.2720\n (9.27170337342587, 1e-05)-DP guarantees for epoch 235 \n\n5/5 [==============================] - 3s 350ms/step - loss: 0.0518 - accuracy: 0.2720 - val_loss: 0.0527 - val_accuracy: 0.2663\nEpoch 236/265\n5/5 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.2729\n (9.298017593587199, 1e-05)-DP guarantees for epoch 236 \n\n5/5 [==============================] - 2s 352ms/step - loss: 0.0515 - accuracy: 0.2729 - val_loss: 0.0526 - val_accuracy: 0.2658\nEpoch 237/265\n5/5 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.2726\n (9.324331813748529, 1e-05)-DP guarantees for epoch 237 \n\n5/5 [==============================] - 2s 353ms/step - loss: 0.0517 - accuracy: 0.2726 - val_loss: 0.0526 - val_accuracy: 0.2650\nEpoch 238/265\n5/5 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.2752\n (9.350646033909857, 1e-05)-DP guarantees for epoch 238 \n\n5/5 [==============================] - 3s 357ms/step - loss: 0.0515 - accuracy: 0.2752 - val_loss: 0.0525 - val_accuracy: 0.2655\nEpoch 239/265\n5/5 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.2739\n (9.376960254071186, 1e-05)-DP guarantees for epoch 239 \n\n5/5 [==============================] - 3s 368ms/step - loss: 0.0517 - accuracy: 0.2739 - val_loss: 0.0525 - val_accuracy: 0.2665\nEpoch 240/265\n5/5 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.2736\n (9.403274474232514, 1e-05)-DP guarantees for epoch 240 \n\n5/5 [==============================] - 3s 356ms/step - loss: 0.0515 - accuracy: 0.2736 - val_loss: 0.0525 - val_accuracy: 0.2673\nEpoch 241/265\n5/5 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.2729\n (9.429588694393843, 1e-05)-DP guarantees for epoch 241 \n\n5/5 [==============================] - 3s 364ms/step - loss: 0.0517 - accuracy: 0.2729 - val_loss: 0.0524 - val_accuracy: 0.2674\nEpoch 242/265\n5/5 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.2746\n (9.455902914555171, 1e-05)-DP guarantees for epoch 242 \n\n5/5 [==============================] - 3s 359ms/step - loss: 0.0518 - accuracy: 0.2746 - val_loss: 0.0525 - val_accuracy: 0.2694\nEpoch 243/265\n5/5 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.2756\n (9.482217134716501, 1e-05)-DP guarantees for epoch 243 \n\n5/5 [==============================] - 2s 357ms/step - loss: 0.0515 - accuracy: 0.2756 - val_loss: 0.0524 - val_accuracy: 0.2699\nEpoch 244/265\n5/5 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.2760\n (9.50853135487783, 1e-05)-DP guarantees for epoch 244 \n\n5/5 [==============================] - 3s 367ms/step - loss: 0.0512 - accuracy: 0.2760 - val_loss: 0.0524 - val_accuracy: 0.2712\nEpoch 245/265\n5/5 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.2756\n (9.534845575173634, 1e-05)-DP guarantees for epoch 245 \n\n5/5 [==============================] - 3s 360ms/step - loss: 0.0514 - accuracy: 0.2756 - val_loss: 0.0523 - val_accuracy: 0.2700\nEpoch 246/265\n5/5 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.2758\n (9.561159795911662, 1e-05)-DP guarantees for epoch 246 \n\n5/5 [==============================] - 2s 344ms/step - loss: 0.0512 - accuracy: 0.2758 - val_loss: 0.0523 - val_accuracy: 0.2716\nEpoch 247/265\n5/5 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.2783\n (9.587474015660208, 1e-05)-DP guarantees for epoch 247 \n\n5/5 [==============================] - 2s 352ms/step - loss: 0.0512 - accuracy: 0.2783 - val_loss: 0.0523 - val_accuracy: 0.2719\nEpoch 248/265\n5/5 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.2768\n (9.613788235533915, 1e-05)-DP guarantees for epoch 248 \n\n5/5 [==============================] - 3s 366ms/step - loss: 0.0514 - accuracy: 0.2768 - val_loss: 0.0522 - val_accuracy: 0.2711\nEpoch 249/265\n5/5 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.2780\n (9.64006012187098, 1e-05)-DP guarantees for epoch 249 \n\n5/5 [==============================] - 2s 352ms/step - loss: 0.0511 - accuracy: 0.2780 - val_loss: 0.0522 - val_accuracy: 0.2720\nEpoch 250/265\n5/5 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.2746\n (9.665736679745127, 1e-05)-DP guarantees for epoch 250 \n\n5/5 [==============================] - 3s 352ms/step - loss: 0.0511 - accuracy: 0.2746 - val_loss: 0.0522 - val_accuracy: 0.2731\nEpoch 251/265\n5/5 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.2777\n (9.690604545814235, 1e-05)-DP guarantees for epoch 251 \n\n5/5 [==============================] - 3s 357ms/step - loss: 0.0509 - accuracy: 0.2777 - val_loss: 0.0522 - val_accuracy: 0.2727\nEpoch 252/265\n5/5 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.2793\n (9.714604392289775, 1e-05)-DP guarantees for epoch 252 \n\n5/5 [==============================] - 3s 354ms/step - loss: 0.0511 - accuracy: 0.2793 - val_loss: 0.0522 - val_accuracy: 0.2701\nEpoch 253/265\n5/5 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.2751\n (9.737670917278972, 1e-05)-DP guarantees for epoch 253 \n\n5/5 [==============================] - 3s 354ms/step - loss: 0.0511 - accuracy: 0.2751 - val_loss: 0.0522 - val_accuracy: 0.2719\nEpoch 254/265\n5/5 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.2764\n (9.759732027763015, 1e-05)-DP guarantees for epoch 254 \n\n5/5 [==============================] - 3s 365ms/step - loss: 0.0510 - accuracy: 0.2764 - val_loss: 0.0522 - val_accuracy: 0.2718\nEpoch 255/265\n5/5 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.2765\n (9.780707877727917, 1e-05)-DP guarantees for epoch 255 \n\n5/5 [==============================] - 3s 341ms/step - loss: 0.0510 - accuracy: 0.2765 - val_loss: 0.0522 - val_accuracy: 0.2708\nEpoch 256/265\n5/5 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.2758\n (9.80055660088896, 1e-05)-DP guarantees for epoch 256 \n\n5/5 [==============================] - 3s 364ms/step - loss: 0.0511 - accuracy: 0.2758 - val_loss: 0.0521 - val_accuracy: 0.2726\nEpoch 257/265\n5/5 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.2767\n (9.820083418023108, 1e-05)-DP guarantees for epoch 257 \n\n5/5 [==============================] - 2s 360ms/step - loss: 0.0508 - accuracy: 0.2767 - val_loss: 0.0520 - val_accuracy: 0.2722\nEpoch 258/265\n5/5 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.2740\n (9.839610235157256, 1e-05)-DP guarantees for epoch 258 \n\n5/5 [==============================] - 3s 348ms/step - loss: 0.0511 - accuracy: 0.2740 - val_loss: 0.0520 - val_accuracy: 0.2707\nEpoch 259/265\n5/5 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.2782\n (9.859137052291402, 1e-05)-DP guarantees for epoch 259 \n\n5/5 [==============================] - 3s 367ms/step - loss: 0.0507 - accuracy: 0.2782 - val_loss: 0.0520 - val_accuracy: 0.2731\nEpoch 260/265\n5/5 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.2761\n (9.87866386942555, 1e-05)-DP guarantees for epoch 260 \n\n5/5 [==============================] - 3s 353ms/step - loss: 0.0510 - accuracy: 0.2761 - val_loss: 0.0519 - val_accuracy: 0.2707\nEpoch 261/265\n5/5 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.2751\n (9.898190686559698, 1e-05)-DP guarantees for epoch 261 \n\n5/5 [==============================] - 3s 379ms/step - loss: 0.0509 - accuracy: 0.2751 - val_loss: 0.0519 - val_accuracy: 0.2724\nEpoch 262/265\n5/5 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.2766\n (9.917717503693844, 1e-05)-DP guarantees for epoch 262 \n\n5/5 [==============================] - 2s 352ms/step - loss: 0.0511 - accuracy: 0.2766 - val_loss: 0.0520 - val_accuracy: 0.2729\nEpoch 263/265\n5/5 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.2773\n (9.937244320827991, 1e-05)-DP guarantees for epoch 263 \n\n5/5 [==============================] - 2s 342ms/step - loss: 0.0508 - accuracy: 0.2773 - val_loss: 0.0520 - val_accuracy: 0.2708\nEpoch 264/265\n5/5 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.2777\n (9.95677113796214, 1e-05)-DP guarantees for epoch 264 \n\n5/5 [==============================] - 3s 366ms/step - loss: 0.0507 - accuracy: 0.2777 - val_loss: 0.0519 - val_accuracy: 0.2733\nEpoch 265/265\n5/5 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.2784\n (9.976297955096285, 1e-05)-DP guarantees for epoch 265 \n\n5/5 [==============================] - 3s 378ms/step - loss: 0.0505 - accuracy: 0.2784 - val_loss: 0.0519 - val_accuracy: 0.2738\n</code>\n</pre> <p>This final val_accuracy is compliant with results reported in other framework. For comparison, in Opacus tutorials, the Resnet 18 reaches 60% val_accuracy at \\(\\epsilon=47\\), but 15% at \\(\\epsilon=13\\). </p> <pre><code>\n</code></pre>"},{"location":"notebooks/advanced_cifar10/#cifar-10-tutorial","title":"Cifar-10 tutorial","text":"<p>This notebook introduces advanced tools like MLP mixer, which involves residual connections with Lipschitz guarantees, other input space (HSB) and loss gradient clipping.</p>"},{"location":"notebooks/advanced_cifar10/#imports","title":"Imports","text":""},{"location":"notebooks/advanced_cifar10/#lip-dp-dependencies","title":"lip-dp dependencies","text":"<p>The need a model <code>DP_Model</code> that handles the noisification of gradients. It is trained with a <code>loss</code>. The model is initialized with the convenience function <code>DPParameters</code>. </p>"},{"location":"notebooks/advanced_cifar10/#setup-dp-lipschitz-model","title":"Setup DP Lipschitz model","text":"<p>Here we apply the \"global\" strategy, with a noise multiplier \\(2.5\\). Note that for Cifar-10 the dataset size is \\(N=50,000\\), and it is recommended that \\(\\delta&lt;\\frac{1}{N}\\). So we propose a value of \\(\\delta=10^{-5}\\).  </p>"},{"location":"notebooks/advanced_cifar10/#loading-the-data","title":"Loading the data","text":"<p>We clip the elementwise input upper-bound to \\(40.0\\). The operates in <code>HSV</code> space. The train set is augmented with random left/right flips.</p>"},{"location":"notebooks/advanced_cifar10/#build-the-mlp-mixer-model","title":"Build the MLP Mixer model","text":"<p>We imitate the interface of Keras. We use common layers found in deel-lip, which a wrapper that handles the bound propagation. </p>"},{"location":"notebooks/advanced_cifar10/#train-the-model","title":"Train the model","text":"<p>The model can be trained, and the DP Accountant will automatically track the privacy loss.</p>"},{"location":"notebooks/basics_mnist/","title":"Basics mnist","text":"<p>The library is based on tensorflow.</p> <pre><code>import tensorflow as tf\n</code></pre> <pre><code>from deel.lipdp import layers\nfrom deel.lipdp import losses\nfrom deel.lipdp.model import DP_Sequential\nfrom deel.lipdp.model import DPParameters\n</code></pre> <p>The <code>DP_Accountant</code> callback keeps track of \\((\\epsilon,\\delta)\\)-DP values epoch after epoch. In practice we may be interested in reaching the maximum val_accuracy under privacy constraint \\(\\epsilon\\): the convenience function <code>get_max_epochs</code> exactly does that by performing a dichotomy search over the number of epochs.</p> <pre><code>from deel.lipdp.model import DP_Accountant\nfrom deel.lipdp.sensitivity import get_max_epochs\n</code></pre> <p>The framework requires a control of the maximum norm of inputs. This can be ensured with input clipping for example: <code>bound_clip_value</code>.</p> <pre><code>from deel.lipdp.pipeline import bound_clip_value\nfrom deel.lipdp.pipeline import load_and_prepare_data\n</code></pre> <pre><code>dp_parameters = DPParameters(\n    noisify_strategy=\"global\",\n    noise_multiplier=2.0,\n    delta=1e-5,\n)\n\nepsilon_max = 3.0\n</code></pre> <pre><code>import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# data loader return dataset_metadata which allows to\n# know the informations required for privacy accounting\n# (dataset size, number of samples, max input bound...)\ninput_upper_bound = 20.0\nds_train, ds_test, dataset_metadata = load_and_prepare_data(\n    \"mnist\",\n    batch_size=1000,\n    drop_remainder=True,  # accounting assumes fixed batch size\n    bound_fct=bound_clip_value(  # other strategies are possible, like normalization.\n        input_upper_bound\n    ),  # clipping preprocessing allows to control input bound\n)\n</code></pre> <pre>\n<code>2023-05-24 16:00:31.206597: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-24 16:00:31.742417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 47066 MB memory:  -&gt; device: 0, name: Quadro RTX 8000, pci bus id: 0000:03:00.0, compute capability: 7.5\n</code>\n</pre> <pre><code># construct DP_Sequential\nmodel = DP_Sequential(\n    # works like usual sequential but requires DP layers\n    layers=[\n        # BoundedInput works like Input, but performs input clipping to guarantee input bound\n        layers.DP_BoundedInput(\n            input_shape=dataset_metadata.input_shape, upper_bound=input_upper_bound\n        ),\n        layers.DP_QuickSpectralConv2D( # Reshaped Kernel Orthogonalization (RKO) convolution.\n            filters=32,\n            kernel_size=3,\n            kernel_initializer=\"orthogonal\",\n            strides=1,\n            use_bias=False,  # No biases since the framework handles a single tf.Variable per layer.\n        ),\n        layers.DP_GroupSort(2),  # GNP activation function.\n        layers.DP_ScaledL2NormPooling2D(pool_size=2, strides=2),  # GNP pooling.\n        layers.DP_QuickSpectralConv2D( # Reshaped Kernel Orthogonalization (RKO) convolution.\n            filters=64,\n            kernel_size=3,\n            kernel_initializer=\"orthogonal\",\n            strides=1,\n            use_bias=False,  # No biases since the framework handles a single tf.Variable per layer.\n        ),\n        layers.DP_GroupSort(2),  # GNP activation function.\n        layers.DP_ScaledL2NormPooling2D(pool_size=2, strides=2),  # GNP pooling.\n\n        layers.DP_Flatten(),   # Convert features maps to flat vector.\n\n        layers.DP_QuickSpectralDense(512),  # GNP layer with orthogonal weight matrix.\n        layers.DP_GroupSort(2),\n        layers.DP_QuickSpectralDense(dataset_metadata.nb_classes),\n    ],\n    dp_parameters=dp_parameters,\n    dataset_metadata=dataset_metadata,\n)\n</code></pre> <p>We compile the model with: * any first order optimizer (e.g SGD). No adaptation or special optimizer is needed. * a loss with known Lipschitz constant, e.g Categorical Cross-entropy with temperature.</p> <pre><code>model.compile(\n    # Compile model using DP loss\n    loss=losses.DP_TauCategoricalCrossentropy(18.0),\n    # this method is compatible with any first order optimizer\n    optimizer=tf.keras.optimizers.SGD(learning_rate=2e-4, momentum=0.9),\n    metrics=[\"accuracy\"],\n)\nmodel.summary()\n</code></pre> <pre>\n<code>Model: \"dp__sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dp__bounded_input (DP_Bound  (None, 28, 28, 1)        0         \n edInput)                                                        \n\n dp__quick_spectral_conv2d (  (None, 26, 26, 32)       288       \n DP_QuickSpectralConv2D)                                         \n\n dp__group_sort (DP_GroupSor  (None, 26, 26, 32)       0         \n t)                                                              \n\n dp__scaled_l2_norm_pooling2  (None, 13, 13, 32)       0         \n d (DP_ScaledL2NormPooling2D                                     \n )                                                               \n\n dp__quick_spectral_conv2d_1  (None, 11, 11, 64)       18432     \n  (DP_QuickSpectralConv2D)                                       \n\n dp__group_sort_1 (DP_GroupS  (None, 11, 11, 64)       0         \n ort)                                                            \n\n dp__scaled_l2_norm_pooling2  (None, 5, 5, 64)         0         \n d_1 (DP_ScaledL2NormPooling                                     \n 2D)                                                             \n\n dp__flatten (DP_Flatten)    (None, 1600)              0         \n\n dp__quick_spectral_dense (D  (None, 512)              819200    \n P_QuickSpectralDense)                                           \n\n dp__group_sort_2 (DP_GroupS  (None, 512)              0         \n ort)                                                            \n\n dp__quick_spectral_dense_1   (None, 10)               5120      \n (DP_QuickSpectralDense)                                         \n\n=================================================================\nTotal params: 843,040\nTrainable params: 843,040\nNon-trainable params: 0\n_________________________________________________________________\n</code>\n</pre> <p>Note that the model contains \\(843\\)K parameters. Without gradient clipping these architectures can be trained with batch sizes as big as \\(1000\\) on a standard GPU.</p> <p>Then, we compute the number of epochs. The maximum value of epsilon will depends on dp_parameters and the number of epochs. In order to control epsilon, we compute the adequate number of epochs</p> <pre><code>num_epochs = get_max_epochs(epsilon_max, model)\n</code></pre> <pre>\n<code>epoch bounds = (0, 512.0) and epsilon = 7.994426666195571 at epoch 512.0\nepoch bounds = (0, 256.0) and epsilon = 5.34128917907949 at epoch 256.0\nepoch bounds = (0, 128.0) and epsilon = 3.631964622805248 at epoch 128.0\nepoch bounds = (64.0, 128.0) and epsilon = 2.4829841192119444 at epoch 64.0\nepoch bounds = (64.0, 96.0) and epsilon = 3.089635897639078 at epoch 96.0\nepoch bounds = (80.0, 96.0) and epsilon = 2.796528753679695 at epoch 80.0\nepoch bounds = (88.0, 96.0) and epsilon = 2.952713799856404 at epoch 88.0\nepoch bounds = (88.0, 92.0) and epsilon = 3.0216241846349847 at epoch 92.0\nepoch bounds = (90.0, 92.0) and epsilon = 2.987618328313939 at epoch 90.0\nepoch bounds = (90.0, 91.0) and epsilon = 3.0046212568846444 at epoch 91.0\n</code>\n</pre> <pre><code>hist = model.fit(\n    ds_train,\n    epochs=num_epochs,\n    validation_data=ds_test,\n    callbacks=[\n        # accounting is done thanks to a callback\n        DP_Accountant(log_fn=\"logging\"),  # wandb.log also available.\n    ],\n)\n</code></pre> <pre>\n<code>Epoch 1/91\n</code>\n</pre> <pre>\n<code>2023-05-24 16:00:36.621954: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8300\n2023-05-24 16:00:37.363789: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n</code>\n</pre> <pre>\n<code>60/60 [==============================] - ETA: 0s - loss: 0.2020 - accuracy: 0.2324\n (0.3227333785403041, 1e-05)-DP guarantees for epoch 1 \n\n60/60 [==============================] - 5s 38ms/step - loss: 0.2020 - accuracy: 0.2324 - val_loss: 0.1712 - val_accuracy: 0.3147\nEpoch 2/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.1607 - accuracy: 0.3958\n (0.41135036253440604, 1e-05)-DP guarantees for epoch 2 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.1604 - accuracy: 0.3992 - val_loss: 0.1486 - val_accuracy: 0.5122\nEpoch 3/91\n60/60 [==============================] - ETA: 0s - loss: 0.1426 - accuracy: 0.5510\n (0.4972854400421322, 1e-05)-DP guarantees for epoch 3 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.1426 - accuracy: 0.5510 - val_loss: 0.1334 - val_accuracy: 0.6108\nEpoch 4/91\n60/60 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.6333\n (0.5737399623472044, 1e-05)-DP guarantees for epoch 4 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.1291 - accuracy: 0.6333 - val_loss: 0.1213 - val_accuracy: 0.6784\nEpoch 5/91\n60/60 [==============================] - ETA: 0s - loss: 0.1182 - accuracy: 0.6883\n (0.6418194146435952, 1e-05)-DP guarantees for epoch 5 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.1182 - accuracy: 0.6883 - val_loss: 0.1109 - val_accuracy: 0.7180\nEpoch 6/91\n59/60 [============================&gt;.] - ETA: 0s - loss: 0.1088 - accuracy: 0.7247\n (0.7042008802236781, 1e-05)-DP guarantees for epoch 6 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.1087 - accuracy: 0.7247 - val_loss: 0.1024 - val_accuracy: 0.7527\nEpoch 7/91\n60/60 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.7488\n (0.7616059152520757, 1e-05)-DP guarantees for epoch 7 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.1012 - accuracy: 0.7488 - val_loss: 0.0955 - val_accuracy: 0.7698\nEpoch 8/91\n60/60 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.7644\n (0.8155744676428971, 1e-05)-DP guarantees for epoch 8 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0948 - accuracy: 0.7644 - val_loss: 0.0899 - val_accuracy: 0.7815\nEpoch 9/91\n60/60 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.7785\n (0.8666021691681208, 1e-05)-DP guarantees for epoch 9 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0896 - accuracy: 0.7785 - val_loss: 0.0848 - val_accuracy: 0.7936\nEpoch 10/91\n60/60 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.7868\n (0.9152742048884784, 1e-05)-DP guarantees for epoch 10 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0849 - accuracy: 0.7868 - val_loss: 0.0804 - val_accuracy: 0.8003\nEpoch 11/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0810 - accuracy: 0.7967\n (0.9617965624530973, 1e-05)-DP guarantees for epoch 11 \n\n60/60 [==============================] - 2s 30ms/step - loss: 0.0809 - accuracy: 0.7975 - val_loss: 0.0769 - val_accuracy: 0.8109\nEpoch 12/91\n60/60 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.8060\n (1.0059716506359193, 1e-05)-DP guarantees for epoch 12 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0774 - accuracy: 0.8060 - val_loss: 0.0733 - val_accuracy: 0.8179\nEpoch 13/91\n60/60 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.8131\n (1.049398006635733, 1e-05)-DP guarantees for epoch 13 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0740 - accuracy: 0.8131 - val_loss: 0.0704 - val_accuracy: 0.8269\nEpoch 14/91\n60/60 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.8216\n (1.090263192229449, 1e-05)-DP guarantees for epoch 14 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0713 - accuracy: 0.8216 - val_loss: 0.0677 - val_accuracy: 0.8309\nEpoch 15/91\n60/60 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.8240\n (1.131126828240101, 1e-05)-DP guarantees for epoch 15 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0689 - accuracy: 0.8240 - val_loss: 0.0656 - val_accuracy: 0.8355\nEpoch 16/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0669 - accuracy: 0.8293\n (1.169340908770284, 1e-05)-DP guarantees for epoch 16 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0668 - accuracy: 0.8296 - val_loss: 0.0635 - val_accuracy: 0.8398\nEpoch 17/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0647 - accuracy: 0.8333\n (1.2074292910030167, 1e-05)-DP guarantees for epoch 17 \n\n60/60 [==============================] - 2s 29ms/step - loss: 0.0646 - accuracy: 0.8335 - val_loss: 0.0615 - val_accuracy: 0.8437\nEpoch 18/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0630 - accuracy: 0.8366\n (1.2447047350704166, 1e-05)-DP guarantees for epoch 18 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0629 - accuracy: 0.8367 - val_loss: 0.0598 - val_accuracy: 0.8468\nEpoch 19/91\n60/60 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.8399\n (1.2800495944157277, 1e-05)-DP guarantees for epoch 19 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0612 - accuracy: 0.8399 - val_loss: 0.0582 - val_accuracy: 0.8508\nEpoch 20/91\n60/60 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.8428\n (1.3153944538284068, 1e-05)-DP guarantees for epoch 20 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0598 - accuracy: 0.8428 - val_loss: 0.0569 - val_accuracy: 0.8563\nEpoch 21/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0584 - accuracy: 0.8468\n (1.3507368078845663, 1e-05)-DP guarantees for epoch 21 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0584 - accuracy: 0.8466 - val_loss: 0.0557 - val_accuracy: 0.8572\nEpoch 22/91\n60/60 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.8509\n (1.383564204783113, 1e-05)-DP guarantees for epoch 22 \n\n60/60 [==============================] - 2s 30ms/step - loss: 0.0572 - accuracy: 0.8509 - val_loss: 0.0546 - val_accuracy: 0.8610\nEpoch 23/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0561 - accuracy: 0.8519\n (1.4161979427317832, 1e-05)-DP guarantees for epoch 23 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0562 - accuracy: 0.8518 - val_loss: 0.0537 - val_accuracy: 0.8619\nEpoch 24/91\n60/60 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.8547\n (1.448831680775656, 1e-05)-DP guarantees for epoch 24 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0552 - accuracy: 0.8547 - val_loss: 0.0525 - val_accuracy: 0.8657\nEpoch 25/91\n59/60 [============================&gt;.] - ETA: 0s - loss: 0.0541 - accuracy: 0.8575\n (1.4814654188092617, 1e-05)-DP guarantees for epoch 25 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0541 - accuracy: 0.8576 - val_loss: 0.0516 - val_accuracy: 0.8675\nEpoch 26/91\n60/60 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.8578\n (1.512526290723161, 1e-05)-DP guarantees for epoch 26 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0531 - accuracy: 0.8578 - val_loss: 0.0506 - val_accuracy: 0.8691\nEpoch 27/91\n60/60 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.8605\n (1.5424804710143858, 1e-05)-DP guarantees for epoch 27 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0522 - accuracy: 0.8605 - val_loss: 0.0497 - val_accuracy: 0.8709\nEpoch 28/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0512 - accuracy: 0.8624\n (1.5724346510360574, 1e-05)-DP guarantees for epoch 28 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0512 - accuracy: 0.8626 - val_loss: 0.0488 - val_accuracy: 0.8730\nEpoch 29/91\n59/60 [============================&gt;.] - ETA: 0s - loss: 0.0503 - accuracy: 0.8650\n (1.6023888317992228, 1e-05)-DP guarantees for epoch 29 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0503 - accuracy: 0.8653 - val_loss: 0.0479 - val_accuracy: 0.8752\nEpoch 30/91\n59/60 [============================&gt;.] - ETA: 0s - loss: 0.0495 - accuracy: 0.8665\n (1.632343011263517, 1e-05)-DP guarantees for epoch 30 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0495 - accuracy: 0.8667 - val_loss: 0.0471 - val_accuracy: 0.8749\nEpoch 31/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0488 - accuracy: 0.8684\n (1.6622962394525178, 1e-05)-DP guarantees for epoch 31 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0487 - accuracy: 0.8686 - val_loss: 0.0463 - val_accuracy: 0.8779\nEpoch 32/91\n60/60 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.8697\n (1.689965116494089, 1e-05)-DP guarantees for epoch 32 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0480 - accuracy: 0.8697 - val_loss: 0.0457 - val_accuracy: 0.8777\nEpoch 33/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0475 - accuracy: 0.8700\n (1.7172705001520499, 1e-05)-DP guarantees for epoch 33 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0475 - accuracy: 0.8704 - val_loss: 0.0452 - val_accuracy: 0.8790\nEpoch 34/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0469 - accuracy: 0.8736\n (1.7445758842338837, 1e-05)-DP guarantees for epoch 34 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0468 - accuracy: 0.8738 - val_loss: 0.0446 - val_accuracy: 0.8806\nEpoch 35/91\n59/60 [============================&gt;.] - ETA: 0s - loss: 0.0463 - accuracy: 0.8754\n (1.7718812676250233, 1e-05)-DP guarantees for epoch 35 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0462 - accuracy: 0.8756 - val_loss: 0.0441 - val_accuracy: 0.8825\nEpoch 36/91\n60/60 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.8763\n (1.799186650959813, 1e-05)-DP guarantees for epoch 36 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0456 - accuracy: 0.8763 - val_loss: 0.0434 - val_accuracy: 0.8831\nEpoch 37/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0450 - accuracy: 0.8771\n (1.8264920346090618, 1e-05)-DP guarantees for epoch 37 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0450 - accuracy: 0.8773 - val_loss: 0.0429 - val_accuracy: 0.8846\nEpoch 38/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0444 - accuracy: 0.8786\n (1.8537974184156425, 1e-05)-DP guarantees for epoch 38 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0444 - accuracy: 0.8786 - val_loss: 0.0423 - val_accuracy: 0.8855\nEpoch 39/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0439 - accuracy: 0.8800\n (1.8807666749981604, 1e-05)-DP guarantees for epoch 39 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0439 - accuracy: 0.8802 - val_loss: 0.0419 - val_accuracy: 0.8863\nEpoch 40/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0435 - accuracy: 0.8803\n (1.9054738700393052, 1e-05)-DP guarantees for epoch 40 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0435 - accuracy: 0.8804 - val_loss: 0.0415 - val_accuracy: 0.8858\nEpoch 41/91\n60/60 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.8816\n (1.9301604511513608, 1e-05)-DP guarantees for epoch 41 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0430 - accuracy: 0.8816 - val_loss: 0.0410 - val_accuracy: 0.8884\nEpoch 42/91\n60/60 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.8824\n (1.9548470320035656, 1e-05)-DP guarantees for epoch 42 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0425 - accuracy: 0.8824 - val_loss: 0.0405 - val_accuracy: 0.8890\nEpoch 43/91\n60/60 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.8837\n (1.979533612594768, 1e-05)-DP guarantees for epoch 43 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0421 - accuracy: 0.8837 - val_loss: 0.0403 - val_accuracy: 0.8890\nEpoch 44/91\n60/60 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.8856\n (2.0042201936126345, 1e-05)-DP guarantees for epoch 44 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0418 - accuracy: 0.8856 - val_loss: 0.0399 - val_accuracy: 0.8908\nEpoch 45/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0414 - accuracy: 0.8858\n (2.0289067746857206, 1e-05)-DP guarantees for epoch 45 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0414 - accuracy: 0.8856 - val_loss: 0.0393 - val_accuracy: 0.8926\nEpoch 46/91\n60/60 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.8872\n (2.053593355232055, 1e-05)-DP guarantees for epoch 46 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0408 - accuracy: 0.8872 - val_loss: 0.0388 - val_accuracy: 0.8951\nEpoch 47/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0405 - accuracy: 0.8882\n (2.078279935996221, 1e-05)-DP guarantees for epoch 47 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0404 - accuracy: 0.8887 - val_loss: 0.0385 - val_accuracy: 0.8959\nEpoch 48/91\n60/60 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.8882\n (2.1029665168498504, 1e-05)-DP guarantees for epoch 48 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0400 - accuracy: 0.8882 - val_loss: 0.0381 - val_accuracy: 0.8952\nEpoch 49/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0397 - accuracy: 0.8890\n (2.127653097450219, 1e-05)-DP guarantees for epoch 49 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0398 - accuracy: 0.8888 - val_loss: 0.0379 - val_accuracy: 0.8943\nEpoch 50/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0396 - accuracy: 0.8887\n (2.151531383398666, 1e-05)-DP guarantees for epoch 50 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0395 - accuracy: 0.8889 - val_loss: 0.0375 - val_accuracy: 0.8946\nEpoch 51/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0391 - accuracy: 0.8893\n (2.1736284198821467, 1e-05)-DP guarantees for epoch 51 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0391 - accuracy: 0.8895 - val_loss: 0.0372 - val_accuracy: 0.8968\nEpoch 52/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0387 - accuracy: 0.8908\n (2.195725456202997, 1e-05)-DP guarantees for epoch 52 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0387 - accuracy: 0.8908 - val_loss: 0.0368 - val_accuracy: 0.8967\nEpoch 53/91\n60/60 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.8905\n (2.217822492103547, 1e-05)-DP guarantees for epoch 53 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0385 - accuracy: 0.8905 - val_loss: 0.0366 - val_accuracy: 0.8991\nEpoch 54/91\n60/60 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.8913\n (2.2399195284840734, 1e-05)-DP guarantees for epoch 54 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0382 - accuracy: 0.8913 - val_loss: 0.0365 - val_accuracy: 0.8992\nEpoch 55/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0380 - accuracy: 0.8924\n (2.2620165646623547, 1e-05)-DP guarantees for epoch 55 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0380 - accuracy: 0.8921 - val_loss: 0.0362 - val_accuracy: 0.8994\nEpoch 56/91\n60/60 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.8925\n (2.2841136015562187, 1e-05)-DP guarantees for epoch 56 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0377 - accuracy: 0.8925 - val_loss: 0.0358 - val_accuracy: 0.8999\nEpoch 57/91\n60/60 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.8930\n (2.3062106367493893, 1e-05)-DP guarantees for epoch 57 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0374 - accuracy: 0.8930 - val_loss: 0.0356 - val_accuracy: 0.9004\nEpoch 58/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0371 - accuracy: 0.8938\n (2.3283076739544244, 1e-05)-DP guarantees for epoch 58 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0372 - accuracy: 0.8939 - val_loss: 0.0354 - val_accuracy: 0.9010\nEpoch 59/91\n60/60 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.8951\n (2.3504047095381226, 1e-05)-DP guarantees for epoch 59 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0369 - accuracy: 0.8951 - val_loss: 0.0351 - val_accuracy: 0.9010\nEpoch 60/91\n60/60 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.8963\n (2.3725017457248683, 1e-05)-DP guarantees for epoch 60 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0365 - accuracy: 0.8963 - val_loss: 0.0347 - val_accuracy: 0.9037\nEpoch 61/91\n60/60 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.8968\n (2.3945987822094885, 1e-05)-DP guarantees for epoch 61 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0363 - accuracy: 0.8968 - val_loss: 0.0346 - val_accuracy: 0.9024\nEpoch 62/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0360 - accuracy: 0.8979\n (2.4166958179233653, 1e-05)-DP guarantees for epoch 62 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0360 - accuracy: 0.8981 - val_loss: 0.0343 - val_accuracy: 0.9041\nEpoch 63/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0358 - accuracy: 0.8986\n (2.438792853624178, 1e-05)-DP guarantees for epoch 63 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0358 - accuracy: 0.8987 - val_loss: 0.0340 - val_accuracy: 0.9068\nEpoch 64/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0355 - accuracy: 0.8995\n (2.4608898896847116, 1e-05)-DP guarantees for epoch 64 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0356 - accuracy: 0.8992 - val_loss: 0.0338 - val_accuracy: 0.9072\nEpoch 65/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0352 - accuracy: 0.9005\n (2.4829841192119444, 1e-05)-DP guarantees for epoch 65 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0353 - accuracy: 0.9000 - val_loss: 0.0336 - val_accuracy: 0.9059\nEpoch 66/91\n60/60 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.8996\n (2.5034880893370737, 1e-05)-DP guarantees for epoch 66 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0351 - accuracy: 0.8996 - val_loss: 0.0334 - val_accuracy: 0.9070\nEpoch 67/91\n59/60 [============================&gt;.] - ETA: 0s - loss: 0.0350 - accuracy: 0.9003\n (2.523024133549594, 1e-05)-DP guarantees for epoch 67 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0349 - accuracy: 0.9003 - val_loss: 0.0333 - val_accuracy: 0.9069\nEpoch 68/91\n60/60 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9005\n (2.542560178527111, 1e-05)-DP guarantees for epoch 68 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0348 - accuracy: 0.9005 - val_loss: 0.0332 - val_accuracy: 0.9071\nEpoch 69/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0346 - accuracy: 0.9006\n (2.5620962223364145, 1e-05)-DP guarantees for epoch 69 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0347 - accuracy: 0.9007 - val_loss: 0.0329 - val_accuracy: 0.9081\nEpoch 70/91\n59/60 [============================&gt;.] - ETA: 0s - loss: 0.0345 - accuracy: 0.9015\n (2.5816322672410785, 1e-05)-DP guarantees for epoch 70 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0345 - accuracy: 0.9014 - val_loss: 0.0327 - val_accuracy: 0.9069\nEpoch 71/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0343 - accuracy: 0.9017\n (2.601168310806795, 1e-05)-DP guarantees for epoch 71 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0343 - accuracy: 0.9019 - val_loss: 0.0326 - val_accuracy: 0.9090\nEpoch 72/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0342 - accuracy: 0.9021\n (2.620704354996593, 1e-05)-DP guarantees for epoch 72 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0342 - accuracy: 0.9022 - val_loss: 0.0324 - val_accuracy: 0.9089\nEpoch 73/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0340 - accuracy: 0.9018\n (2.640240400625916, 1e-05)-DP guarantees for epoch 73 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0339 - accuracy: 0.9020 - val_loss: 0.0322 - val_accuracy: 0.9096\nEpoch 74/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0339 - accuracy: 0.9018\n (2.659776444789028, 1e-05)-DP guarantees for epoch 74 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0338 - accuracy: 0.9022 - val_loss: 0.0320 - val_accuracy: 0.9103\nEpoch 75/91\n59/60 [============================&gt;.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9024\n (2.679312488654814, 1e-05)-DP guarantees for epoch 75 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0335 - accuracy: 0.9024 - val_loss: 0.0318 - val_accuracy: 0.9088\nEpoch 76/91\n59/60 [============================&gt;.] - ETA: 0s - loss: 0.0333 - accuracy: 0.9025\n (2.69884853278786, 1e-05)-DP guarantees for epoch 76 \n\n60/60 [==============================] - 2s 29ms/step - loss: 0.0333 - accuracy: 0.9023 - val_loss: 0.0315 - val_accuracy: 0.9098\nEpoch 77/91\n60/60 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9033\n (2.7183845763895516, 1e-05)-DP guarantees for epoch 77 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0332 - accuracy: 0.9033 - val_loss: 0.0314 - val_accuracy: 0.9125\nEpoch 78/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0330 - accuracy: 0.9046\n (2.737920620600221, 1e-05)-DP guarantees for epoch 78 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0330 - accuracy: 0.9048 - val_loss: 0.0313 - val_accuracy: 0.9119\nEpoch 79/91\n60/60 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9053\n (2.7574566653298858, 1e-05)-DP guarantees for epoch 79 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0328 - accuracy: 0.9053 - val_loss: 0.0311 - val_accuracy: 0.9115\nEpoch 80/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0328 - accuracy: 0.9052\n (2.7769927101097007, 1e-05)-DP guarantees for epoch 80 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0327 - accuracy: 0.9056 - val_loss: 0.0310 - val_accuracy: 0.9118\nEpoch 81/91\n60/60 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9056\n (2.796528753679695, 1e-05)-DP guarantees for epoch 81 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0325 - accuracy: 0.9056 - val_loss: 0.0308 - val_accuracy: 0.9114\nEpoch 82/91\n60/60 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9057\n (2.816064798903292, 1e-05)-DP guarantees for epoch 82 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0324 - accuracy: 0.9057 - val_loss: 0.0307 - val_accuracy: 0.9114\nEpoch 83/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0323 - accuracy: 0.9053\n (2.8356008431856474, 1e-05)-DP guarantees for epoch 83 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0322 - accuracy: 0.9057 - val_loss: 0.0305 - val_accuracy: 0.9117\nEpoch 84/91\n60/60 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9063\n (2.8551368864333964, 1e-05)-DP guarantees for epoch 84 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0320 - accuracy: 0.9063 - val_loss: 0.0303 - val_accuracy: 0.9117\nEpoch 85/91\n60/60 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9064\n (2.8746729305801413, 1e-05)-DP guarantees for epoch 85 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0318 - accuracy: 0.9064 - val_loss: 0.0302 - val_accuracy: 0.9121\nEpoch 86/91\n59/60 [============================&gt;.] - ETA: 0s - loss: 0.0317 - accuracy: 0.9074\n (2.894208975473722, 1e-05)-DP guarantees for epoch 86 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0316 - accuracy: 0.9076 - val_loss: 0.0299 - val_accuracy: 0.9132\nEpoch 87/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0314 - accuracy: 0.9078\n (2.9137450193835823, 1e-05)-DP guarantees for epoch 87 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0314 - accuracy: 0.9076 - val_loss: 0.0298 - val_accuracy: 0.9123\nEpoch 88/91\n60/60 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9086\n (2.9332810632263646, 1e-05)-DP guarantees for epoch 88 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0313 - accuracy: 0.9086 - val_loss: 0.0299 - val_accuracy: 0.9133\nEpoch 89/91\n59/60 [============================&gt;.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9087\n (2.952713799856404, 1e-05)-DP guarantees for epoch 89 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0313 - accuracy: 0.9087 - val_loss: 0.0298 - val_accuracy: 0.9140\nEpoch 90/91\n60/60 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9097\n (2.970615400210975, 1e-05)-DP guarantees for epoch 90 \n\n60/60 [==============================] - 2s 28ms/step - loss: 0.0312 - accuracy: 0.9097 - val_loss: 0.0298 - val_accuracy: 0.9127\nEpoch 91/91\n58/60 [============================&gt;.] - ETA: 0s - loss: 0.0312 - accuracy: 0.9091\n (2.987618328313939, 1e-05)-DP guarantees for epoch 91 \n\n60/60 [==============================] - 2s 27ms/step - loss: 0.0312 - accuracy: 0.9093 - val_loss: 0.0297 - val_accuracy: 0.9132\n</code>\n</pre> <p>The model can be further improved by tuning various hyper-parameters, by adding layers (see <code>advanced_cifar10.ipynb</code> tutorial). </p> <pre><code>\n</code></pre>"},{"location":"notebooks/basics_mnist/#mnist-tutorial","title":"Mnist tutorial","text":"<p>This notebook introduces the basics of usage of our library.</p>"},{"location":"notebooks/basics_mnist/#imports","title":"Imports","text":""},{"location":"notebooks/basics_mnist/#lip-dp-dependencies","title":"lip-dp dependencies","text":"<p>The need a model <code>DP_Sequential</code> that handles the noisification of gradients. It is composed <code>layers</code> and trained with a loss found in <code>loss</code>. The model is initialized with the convenience function <code>DPParameters</code>. </p>"},{"location":"notebooks/basics_mnist/#setup-dp-lipschitz-model","title":"Setup DP Lipschitz model","text":"<p>Here we apply the \"global\" strategy, with a noise multiplier \\(2.5\\). Note that for Mnist the dataset size is \\(N=60,000\\), and it is recommended that \\(\\delta&lt;\\frac{1}{N}\\). So we propose a value of \\(\\delta=10^{-5}\\).</p>"},{"location":"notebooks/basics_mnist/#loading-the-data","title":"Loading the data","text":"<p>We clip the elementwise input upper-bound to \\(20.0\\).</p>"},{"location":"notebooks/basics_mnist/#build-the-dp-model","title":"Build the DP model","text":"<p>We imitate the interface of Keras. We use common layers found in deel-lip, which a wrapper that handles the bound propagation. </p>"},{"location":"notebooks/basics_mnist/#train-the-model","title":"Train the model","text":"<p>The model can be trained, and the DP Accountant will automatically track the privacy loss.</p>"}]}